"""
arXivç§å­è®ºæ–‡æ£€ç´¢æ¨¡å—
åŸºäºarXiv APIå’Œåˆ†ç±»ç³»ç»Ÿè·å–é«˜è´¨é‡ç§å­è®ºæ–‡

æ ¸å¿ƒç­–ç•¥ï¼š
1. ä½¿ç”¨arXiv Categoriesè¿›è¡Œç²¾å‡†æ£€ç´¢
2. ç»“åˆå…³é”®è¯ï¼ˆtitle, abstractï¼‰è¿‡æ»¤
3. é™å®šæ—¶é—´èŒƒå›´ï¼ˆè¿‘3-5å¹´ï¼‰
4. æŒ‰ç›¸å…³æ€§å’Œå¼•ç”¨é‡æ’åº
"""

import arxiv
import logging
import json
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta, timezone
from collections import defaultdict
from llm_config import LLMConfig, LLMClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# arXivåˆ†ç±»æ˜ å°„è¡¨ï¼ˆCSå­é¢†åŸŸï¼‰
ARXIV_CATEGORY_MAP = {
    "NLP": ["cs.CL"],  # Computation and Language
    "Machine Learning": ["cs.LG", "stat.ML"],  # Machine Learning
    "Computer Vision": ["cs.CV"],  # Computer Vision
    "Artificial Intelligence": ["cs.AI"],  # Artificial Intelligence
    "Robotics": ["cs.RO"],  # Robotics
    "Information Retrieval": ["cs.IR"],  # Information Retrieval
    "Neural Networks": ["cs.NE"],  # Neural and Evolutionary Computing
    "Cryptography": ["cs.CR"],  # Cryptography and Security
    "Software Engineering": ["cs.SE"],  # Software Engineering
    "Databases": ["cs.DB"],  # Databases
    "Distributed Computing": ["cs.DC"],  # Distributed Computing
    "Human-Computer Interaction": ["cs.HC"],  # Human-Computer Interaction
}


class ArxivSeedRetriever:
    """
    arXivç§å­è®ºæ–‡æ£€ç´¢å™¨
    ä¸“æ³¨äºè·å–é«˜è´¨é‡çš„é¢†åŸŸç§å­è®ºæ–‡
    """

    def __init__(
        self,
        max_results_per_query: int = 50,
        years_back: int = 5,
        min_relevance_score: float = 0.4,  # é™ä½é˜ˆå€¼,æé«˜å¬å›ç‡
        llm_client: Optional[LLMClient] = None,
        use_llm_query_generation: bool = True,
        enable_semantic_expansion: bool = True,
        expansion_max_topics: int = 4,
        expansion_max_keywords: int = 8
    ):
        """
        åˆå§‹åŒ–arXivç§å­æ£€ç´¢å™¨

        Args:
            max_results_per_query: æ¯æ¬¡æŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
            years_back: å›æº¯å¹´æ•°ï¼ˆä»å½“å‰å¹´ä»½å¾€å‰ï¼‰
            min_relevance_score: æœ€å°ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-1ï¼‰ï¼Œé»˜è®¤0.4ä»¥æé«˜å¬å›ç‡
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œç”¨äºæ™ºèƒ½ç”ŸæˆæŸ¥è¯¢ï¼‰
            use_llm_query_generation: æ˜¯å¦ä½¿ç”¨LLMç”ŸæˆæŸ¥è¯¢ï¼ˆé»˜è®¤Trueï¼‰
            enable_semantic_expansion: æ˜¯å¦å¯ç”¨è¯­ä¹‰æ‰©å±•ï¼ˆé»˜è®¤Trueï¼‰
            expansion_max_topics: æœ€å¤šæ‰©å±•ä¸»é¢˜æ•°ï¼ˆé»˜è®¤4ï¼‰
            expansion_max_keywords: æœ€å¤šæ‰©å±•å…³é”®è¯æ•°ï¼ˆé»˜è®¤8ï¼‰
        """
        self.client = arxiv.Client()
        self.max_results_per_query = max_results_per_query
        self.years_back = years_back
        self.min_relevance_score = min_relevance_score
        self.llm_client = llm_client
        self.use_llm_query_generation = use_llm_query_generation
        self.enable_semantic_expansion = enable_semantic_expansion
        self.expansion_max_topics = expansion_max_topics
        self.expansion_max_keywords = expansion_max_keywords

        logger.info("arXivç§å­æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  max_results_per_query={max_results_per_query}")
        logger.info(f"  years_back={years_back}")
        logger.info(f"  min_relevance_score={min_relevance_score}")
        logger.info(f"  use_llm_query_generation={use_llm_query_generation}")
        if use_llm_query_generation and llm_client:
            logger.info(f"  enable_semantic_expansion={enable_semantic_expansion}")
            if enable_semantic_expansion:
                logger.info(f"  expansion_max_topics={expansion_max_topics}")
                logger.info(f"  expansion_max_keywords={expansion_max_keywords}")

    def retrieve_seed_papers(
        self,
        topic: str,
        keywords: Optional[List[str]] = None,
        categories: Optional[List[str]] = None,
        max_seeds: int = 100,
        sort_by: arxiv.SortCriterion = arxiv.SortCriterion.Relevance
    ) -> List[Dict]:
        """
        æ£€ç´¢ç§å­è®ºæ–‡ï¼ˆé«˜è´¨é‡æ ¸å¿ƒè®ºæ–‡ï¼‰

        Args:
            topic: ä¸»é¢˜åç§°ï¼ˆå¦‚ "Natural Language Processing"ï¼‰
            keywords: å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºæ ‡é¢˜/æ‘˜è¦åŒ¹é…ï¼‰
            categories: arXivåˆ†ç±»åˆ—è¡¨ï¼ˆå¦‚ ["cs.CL", "cs.AI"]ï¼‰
            max_seeds: æœ€å¤§ç§å­è®ºæ–‡æ•°é‡
            sort_by: æ’åºæ–¹å¼

        Returns:
            ç§å­è®ºæ–‡åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æ£€ç´¢ç§å­è®ºæ–‡: topic='{topic}'")

        # 1. è‡ªåŠ¨æ¨æ–­categories
        if not categories:
            categories = self._infer_categories(topic)
            logger.info(f"è‡ªåŠ¨æ¨æ–­arXivåˆ†ç±»: {categories}")

        # 2. æ„å»ºæŸ¥è¯¢
        query = self._build_arxiv_query(
            topic=topic,
            keywords=keywords,
            categories=categories
        )
        logger.info(f"arXivæŸ¥è¯¢: {query}")

        # 3. è®¾ç½®æ—¶é—´èŒƒå›´
        # è®¾å®šæ—¶é—´èŒƒå›´ä¸º1995å¹´åˆ°2022å¹´
        start_date = datetime(1995, 1, 1, tzinfo=timezone.utc)
        end_date = datetime.now(timezone.utc) # ç›´åˆ°ç°åœ¨
        logger.info(f"æ—¶é—´èŒƒå›´: >= {start_date.strftime('%Y-%m-%d')} åˆ° <= {end_date.strftime('%Y-%m-%d')}")

        # 4. æ‰§è¡Œæ£€ç´¢
        search = arxiv.Search(
            query=query,
            max_results=self.max_results_per_query,
            sort_by=sort_by,
            sort_order=arxiv.SortOrder.Descending
        )

        papers = []
        try:
            # æ˜¾å¼è½¬æ¢ä¸ºåˆ—è¡¨,æ•è·ç½‘ç»œå¼‚å¸¸
            results = list(self.client.results(search))
            logger.info(f"  æˆåŠŸè·å– {len(results)} æ¡åŸå§‹ç»“æœ")
        except Exception as e:
            logger.error(f"  âŒ arXiv APIè¯·æ±‚å¤±è´¥: {e}")
            logger.warning("  æç¤º: å›½å†…è®¿é—®arXivå¯èƒ½éœ€è¦ä»£ç†,æˆ–ç¨åé‡è¯•")
            return []

        for result in results:
            # æ—¶é—´è¿‡æ»¤
            if result.published < start_date or result.published > end_date:
                continue

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            paper = self._parse_arxiv_result(result)

            # ç›¸å…³æ€§è¿‡æ»¤
            relevance_score = self._compute_relevance(paper, topic, keywords)
            paper['relevance_score'] = relevance_score

            if relevance_score >= self.min_relevance_score:
                papers.append(paper)
                logger.info(
                    f"  âœ“ [{paper['year']}] {paper['title'][:60]}... "
                    f"(ç›¸å…³æ€§: {relevance_score:.2f})"
                )

            if len(papers) >= max_seeds:
                break

        # 5. æŒ‰ç›¸å…³æ€§æ’åº
        papers.sort(key=lambda x: x['relevance_score'], reverse=True)

        logger.info(f"âœ… æ£€ç´¢åˆ° {len(papers)} ç¯‡é«˜è´¨é‡ç§å­è®ºæ–‡")
        return papers[:max_seeds]

    def _infer_categories(self, topic: str) -> List[str]:
        """
        æ ¹æ®ä¸»é¢˜æ¨æ–­arXivåˆ†ç±»

        Args:
            topic: ä¸»é¢˜åç§°

        Returns:
            æ¨æ–­çš„åˆ†ç±»åˆ—è¡¨
        """
        topic_lower = topic.lower()

        # å°è¯•åŒ¹é…é¢„å®šä¹‰æ˜ å°„
        for key, cats in ARXIV_CATEGORY_MAP.items():
            if key.lower() in topic_lower:
                return cats

        # å…³é”®è¯åŒ¹é…
        if any(kw in topic_lower for kw in ["nlp", "language", "text", "translation"]):
            return ["cs.CL"]
        elif any(kw in topic_lower for kw in ["vision", "image", "video"]):
            return ["cs.CV"]
        elif any(kw in topic_lower for kw in ["learning", "neural", "deep"]):
            return ["cs.LG"]
        elif any(kw in topic_lower for kw in ["ai", "intelligence", "agent"]):
            return ["cs.AI"]

        # é»˜è®¤è¿”å›é€šç”¨CSåˆ†ç±»
        return ["cs.AI", "cs.LG"]

    def _build_arxiv_query(
        self,
        topic: str,
        keywords: Optional[List[str]],
        categories: List[str]
    ) -> str:
        """
        æ„å»ºarXivæŸ¥è¯¢å­—ç¬¦ä¸²

        å¦‚æœå¯ç”¨LLMä¸”å®¢æˆ·ç«¯å¯ç”¨,åˆ™ä½¿ç”¨LLMç”ŸæˆæŸ¥è¯¢
        å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿè§„åˆ™æ–¹æ³•

        Args:
            topic: ä¸»é¢˜
            keywords: å…³é”®è¯åˆ—è¡¨
            categories: arXivåˆ†ç±»åˆ—è¡¨

        Returns:
            æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # å°è¯•ä½¿ç”¨LLMç”ŸæˆæŸ¥è¯¢
        if self.use_llm_query_generation and self.llm_client:
            try:
                llm_query = self._generate_query_with_llm(topic, keywords, categories)
                if llm_query:
                    logger.info(f"âœ¨ ä½¿ç”¨LLMç”Ÿæˆçš„æŸ¥è¯¢: {llm_query}")
                    return llm_query
            except Exception as e:
                logger.warning(f"LLMæŸ¥è¯¢ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {e}")

        # ä¼ ç»Ÿè§„åˆ™æ–¹æ³•
        return self._build_arxiv_query_traditional(topic, keywords, categories)

    def _expand_semantic_concepts(
        self,
        topic: str,
        keywords: Optional[List[str]]
    ) -> Dict:
        """
        é˜¶æ®µ1: è¯­ä¹‰æ‰©å±•
        ä½¿ç”¨LLMä½œä¸ºé¢†åŸŸä¸“å®¶ï¼Œæ‰©å±•ç›¸å…³æ¦‚å¿µã€åŒä¹‰è¯å’Œå­é¢†åŸŸ

        Args:
            topic: ç ”ç©¶ä¸»é¢˜
            keywords: å…³é”®è¯åˆ—è¡¨(å¯é€‰)

        Returns:
            æ‰©å±•åçš„æ¦‚å¿µå­—å…¸ï¼Œæ ¼å¼:
            {
                'expanded_topics': [...],     # ç›¸å…³ä¸»é¢˜
                'expanded_keywords': [...],   # æ‰©å±•å…³é”®è¯
                'synonyms': [...],            # åŒä¹‰è¯
                'subfields': [...]            # å­é¢†åŸŸ
            }
        """
        logger.info(f"ğŸ” [é˜¶æ®µ1] è¯­ä¹‰æ‰©å±•: topic='{topic}'")

        system_prompt = """You are a domain expert in computer science research.
Your task is to expand research topics and keywords by providing semantically
related concepts, synonyms, subfields, and alternative terminology.

Focus on:
- Computer Science, AI, and Machine Learning domains
- Academic and technical terminology
- Both broad and specific related concepts"""

        user_prompt = f"""Research Topic: {topic}
Current Keywords: {', '.join(keywords) if keywords else 'None'}

Please expand this research area by providing:
1. Related Topics: 2-{self.expansion_max_topics} semantically similar or overlapping research topics
2. Expanded Keywords: 5-{self.expansion_max_keywords} additional relevant technical terms, methods, or concepts
3. Synonyms: 2-4 alternative terms or abbreviations for the main topic
4. Subfields: 2-3 more specific subfields or applications within this area

Important:
- Focus on computer science and AI-related terms
- Use technical/academic terminology
- Keep each item concise (1-5 words)
- Avoid generic terms like "research", "study", "analysis"

Output ONLY valid JSON in this exact format (no markdown, no code blocks):
{{
  "expanded_topics": ["topic1", "topic2", ...],
  "expanded_keywords": ["keyword1", "keyword2", ...],
  "synonyms": ["synonym1", "synonym2", ...],
  "subfields": ["subfield1", "subfield2", ...]
}}"""

        try:
            response = self.llm_client.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.3,
                max_tokens=500
            )

            # æ¸…ç†å“åº”ï¼ˆç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°ï¼‰
            response = response.strip()
            if response.startswith('```'):
                # ç§»é™¤ ```json å’Œ ```
                lines = response.split('\n')
                response = '\n'.join(lines[1:-1]) if len(lines) > 2 else response
            response = response.strip()

            # è§£æJSON
            expanded = json.loads(response)

            # éªŒè¯å¹¶é™åˆ¶æ•°é‡
            expanded_topics = expanded.get('expanded_topics', [])[:self.expansion_max_topics]
            expanded_keywords = expanded.get('expanded_keywords', [])[:self.expansion_max_keywords]
            synonyms = expanded.get('synonyms', [])[:4]
            subfields = expanded.get('subfields', [])[:3]

            result = {
                'expanded_topics': expanded_topics,
                'expanded_keywords': expanded_keywords,
                'synonyms': synonyms,
                'subfields': subfields
            }

            # è¾“å‡ºæ‰©å±•ç»“æœ
            logger.info(f"  âœ… è¯­ä¹‰æ‰©å±•æˆåŠŸ:")
            logger.info(f"    - ç›¸å…³ä¸»é¢˜({len(expanded_topics)}): {', '.join(expanded_topics[:3])}{'...' if len(expanded_topics) > 3 else ''}")
            logger.info(f"    - æ‰©å±•å…³é”®è¯({len(expanded_keywords)}): {', '.join(expanded_keywords[:5])}{'...' if len(expanded_keywords) > 5 else ''}")
            logger.info(f"    - åŒä¹‰è¯({len(synonyms)}): {', '.join(synonyms)}")
            logger.info(f"    - å­é¢†åŸŸ({len(subfields)}): {', '.join(subfields)}")

            return result

        except json.JSONDecodeError as e:
            logger.warning(f"  âš ï¸ JSONè§£æå¤±è´¥: {e}")
            logger.warning(f"  LLMåŸå§‹å“åº”: {response[:200]}...")
            return {}
        except Exception as e:
            logger.warning(f"  âš ï¸ è¯­ä¹‰æ‰©å±•å¤±è´¥: {e}")
            return {}

    def _generate_query_with_llm(
        self,
        topic: str,
        keywords: Optional[List[str]],
        categories: List[str]
    ) -> Optional[str]:
        """
        ä½¿ç”¨LLMæ™ºèƒ½ç”ŸæˆarXivæŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆä¸¤é˜¶æ®µæ–¹æ³•ï¼‰

        å¦‚æœå¯ç”¨è¯­ä¹‰æ‰©å±•ï¼Œåˆ™æ‰§è¡Œï¼š
          é˜¶æ®µ1: è¯­ä¹‰æ‰©å±• - ä½œä¸ºé¢†åŸŸä¸“å®¶æ‰©å±•ç›¸å…³æ¦‚å¿µ
          é˜¶æ®µ2: æŸ¥è¯¢æ„å»º - ä½œä¸ºå›¾ä¹¦ç®¡ç†å‘˜æ„å»ºç²¾ç¡®æŸ¥è¯¢
        å¦åˆ™ï¼Œç›´æ¥ç”ŸæˆæŸ¥è¯¢ï¼ˆä¼ ç»Ÿå•é˜¶æ®µæ–¹æ³•ï¼‰

        Args:
            topic: ä¸»é¢˜
            keywords: å…³é”®è¯åˆ—è¡¨
            categories: arXivåˆ†ç±»åˆ—è¡¨

        Returns:
            LLMç”Ÿæˆçš„æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå¤±è´¥åˆ™è¿”å›None
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è¯­ä¹‰æ‰©å±•
        if self.enable_semantic_expansion:
            logger.info("\n" + "="*70)
            logger.info("ğŸš€ ä½¿ç”¨ä¸¤é˜¶æ®µLLMæŸ¥è¯¢ç”Ÿæˆï¼ˆè¯­ä¹‰æ‰©å±• + æŸ¥è¯¢æ„å»ºï¼‰")
            logger.info("="*70)

            # é˜¶æ®µ1: è¯­ä¹‰æ‰©å±•
            try:
                expanded = self._expand_semantic_concepts(topic, keywords)
            except Exception as e:
                logger.warning(f"âš ï¸ è¯­ä¹‰æ‰©å±•å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹è¾“å…¥")
                expanded = {}

            # åˆå¹¶åŸå§‹è¾“å…¥å’Œæ‰©å±•ç»“æœ
            all_topics = [topic]
            if expanded:
                all_topics.extend(expanded.get('expanded_topics', []))
                all_topics.extend(expanded.get('synonyms', []))

            all_keywords = list(keywords) if keywords else []
            if expanded:
                all_keywords.extend(expanded.get('expanded_keywords', []))

            logger.info(f"\nğŸ“¦ åˆå¹¶ç»“æœ: {len(all_topics)} ä¸ªä¸»é¢˜, {len(all_keywords)} ä¸ªå…³é”®è¯")

            # é˜¶æ®µ2: æŸ¥è¯¢æ„å»º
            try:
                query = self._construct_arxiv_query_with_llm(
                    original_topic=topic,
                    all_topics=all_topics,
                    all_keywords=all_keywords,
                    categories=categories
                )
                if query:
                    logger.info("="*70 + "\n")
                    return query
            except Exception as e:
                logger.warning(f"âš ï¸ æŸ¥è¯¢æ„å»ºå¤±è´¥: {e}")

            logger.info("="*70 + "\n")
            return None

        else:
            # ä¼ ç»Ÿå•é˜¶æ®µæ–¹æ³•ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            logger.info("ğŸ’¡ ä½¿ç”¨å•é˜¶æ®µLLMæŸ¥è¯¢ç”Ÿæˆï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰")

            # æ„å»ºprompt
            system_prompt = """You are an expert at constructing arXiv API queries.
Your task is to generate effective search queries that will find relevant academic papers.

arXiv Query Syntax Rules:
1. Categories: Use "cat:cs.AI" or "cat:cs.LG" format
2. Title search: Use "ti:keyword" (without quotes for flexible matching)
3. Abstract search: Use "abs:keyword" (without quotes for flexible matching)
4. Boolean operators: AND, OR, ANDNOT
5. Parentheses for grouping: (cat:cs.AI OR cat:cs.LG)
6. For multi-word phrases: Break into key terms or use without quotes for flexible matching

Important Tips:
- Avoid overly strict exact phrase matching (don't use quotes for long phrases)
- Extract core keywords from long phrases
- Use OR to connect related terms
- Balance between precision and recall"""

            user_prompt = f"""Generate an arXiv search query for the following research topic:

Topic: {topic}
Additional Keywords: {', '.join(keywords) if keywords else 'None'}
Target Categories: {', '.join(categories)}

Requirements:
1. Include category filters: {' OR '.join([f'cat:{cat}' for cat in categories])}
2. Extract 3-5 core keywords from the topic (ignore stopwords like 'for', 'the', 'with')
3. Use flexible matching (no quotes for multi-word terms)
4. Connect keywords with OR for broader coverage
5. Use AND to combine category filters with keyword filters

Output ONLY the final query string, no explanation."""

            try:
                response = self.llm_client.generate(
                    prompt=user_prompt,
                    system_prompt=system_prompt,
                    temperature=0.3,
                    max_tokens=200
                )

                # æ¸…ç†å“åº”ï¼ˆç§»é™¤å¯èƒ½çš„å¤šä½™æ–‡æœ¬ï¼‰
                query = response.strip()
                # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                if query.startswith('```'):
                    query = '\n'.join(query.split('\n')[1:-1])
                query = query.strip()

                return query

            except Exception as e:
                logger.error(f"LLMæŸ¥è¯¢ç”Ÿæˆå‡ºé”™: {e}")
                return None

    def _construct_arxiv_query_with_llm(
        self,
        original_topic: str,
        all_topics: List[str],
        all_keywords: List[str],
        categories: List[str]
    ) -> Optional[str]:
        """
        [ä¼˜åŒ–ç‰ˆ] é˜¶æ®µ2: 
        1. LLM æŒ‘é€‰ 3-5 ä¸ªæœ€å…³é”®çš„æ£€ç´¢è¯ï¼ˆçŸ­è¯­ï¼‰
        2. Python ä»£ç è‡ªåŠ¨å°†å…¶åŒ…è£…ä¸º (ti:"..." OR abs:"...") æ ¼å¼
        è¿™é¿å…äº† LLM ç”Ÿæˆè¯­æ³•é”™è¯¯æˆ–é—æ¼ abs æ ‡ç­¾
        """
        
        # 1. æ„å»º Promptï¼Œåªè¦æ±‚è¿”å›å…³é”®è¯åˆ—è¡¨
        system_prompt = """You are an expert arXiv search optimizer.
Your task is to select the 3-5 MOST CRITICAL search terms from a list of candidates.
Select terms that will maximize the retrieval of high-quality papers.

Rules:
1. Include the full topic name (e.g., "Natural Language Processing").
2. Include the most common acronym (e.g., "NLP").
3. Include 1-2 core technical synonyms (e.g., "Computational Linguistics").
4. Output specific phrases, not generic words.
5. Return ONLY a Python-style list of strings."""

        user_prompt = f"""Task: Select search terms for arXiv.

Original Topic: {original_topic}
Candidates: {', '.join((all_topics + all_keywords)[:20])}

Output Format: ["term1", "term2", "term3"]
Output ONLY the list."""

        try:
            # 2. è°ƒç”¨ LLM
            response = self.llm_client.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.1, # é™ä½éšæœºæ€§ï¼Œå°±è¦æœ€å‡†çš„
                max_tokens=100
            )
            
            # 3. è§£æ LLM è¿”å›çš„åˆ—è¡¨å­—ç¬¦ä¸²
            import ast
            cleaned_response = response.strip()
            # å¤„ç†å¯èƒ½çš„ markdown æ ‡è®°
            if "```" in cleaned_response:
                cleaned_response = cleaned_response.split("```")[1].replace("json", "").replace("python", "").strip()
            
            try:
                # å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬ä¸ºåˆ—è¡¨
                selected_terms = ast.literal_eval(cleaned_response)
                if not isinstance(selected_terms, list):
                    raise ValueError("Output is not a list")
            except:
                # å…œåº•ï¼šå¦‚æœè§£æå¤±è´¥ï¼Œç®€å•çš„æŒ‰é€—å·åˆ†å‰²ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨åŸå§‹ Topic
                logger.warning(f"è§£æLLMåˆ—è¡¨å¤±è´¥: {cleaned_response}, å›é€€åˆ°åŸå§‹Topic")
                selected_terms = [original_topic]

            logger.info(f"ğŸ§  LLMé€‰å®šçš„æ ¸å¿ƒè¯: {selected_terms}")

            # 4. [å…³é”®æ­¥éª¤] Python è´Ÿè´£ä¸¥æ ¼æ„å»ºè¯­æ³•
            # æ ¼å¼: (cat:...) AND ((ti:"A" OR abs:"A") OR (ti:"B" OR abs:"B")...)
            
            # 4.1 æ„å»ºåˆ†ç±»éƒ¨åˆ†
            cat_part = " OR ".join([f"cat:{cat}" for cat in categories])
            
            # 4.2 æ„å»ºå†…å®¹éƒ¨åˆ† (è‡ªåŠ¨ä¸ºæ¯ä¸ªè¯åŠ ä¸Šå¼•å·å’ŒåŒå­—æ®µæ£€ç´¢)
            content_parts = []
            for term in selected_terms:
                term = term.strip()
                if not term: continue
                # å¼ºåˆ¶åŠ ä¸Šå¼•å·ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦
                safe_term = f'"{term}"' 
                # ç”Ÿæˆ (ti:"term" OR abs:"term")
                part = f'(ti:{safe_term} OR abs:{safe_term})'
                content_parts.append(part)
            
            if not content_parts:
                content_parts = [f'(ti:"{original_topic}" OR abs:"{original_topic}")']

            content_query = " OR ".join(content_parts)
            
            # 5. ç»„åˆæœ€ç»ˆæŸ¥è¯¢
            final_query = f"({cat_part}) AND ({content_query})"
            
            logger.info(f"âœ… Pythonç»„è£…æŸ¥è¯¢æˆåŠŸ: {final_query}")
            return final_query

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æ„å»ºè¿‡ç¨‹å‡ºé”™: {e}")
            return None

    def _build_arxiv_query_traditional(
        self,
        topic: str,
        keywords: Optional[List[str]],
        categories: List[str]
    ) -> str:
        """
        ä¼ ç»Ÿè§„åˆ™æ–¹æ³•æ„å»ºarXivæŸ¥è¯¢å­—ç¬¦ä¸²

        ä¼˜åŒ–ç­–ç•¥:
        1. å»æ‰å¼•å·,ä½¿ç”¨å®½æ¾åŒ¹é… (ti:keyword è€Œé ti:"keyword")
        2. ä½¿ç”¨ OR è¿æ¥ topic å’Œ keywords (æé«˜å¬å›ç‡)
        3. ä¾èµ–åç»­çš„ _compute_relevance è¿›è¡Œç²¾ç»†è¿‡æ»¤

        Args:
            topic: ä¸»é¢˜
            keywords: å…³é”®è¯åˆ—è¡¨
            categories: arXivåˆ†ç±»åˆ—è¡¨

        Returns:
            æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        query_parts = []

        # æ·»åŠ åˆ†ç±»çº¦æŸ
        if categories:
            cat_query = " OR ".join([f"cat:{cat}" for cat in categories])
            query_parts.append(f"({cat_query})")

        # æ„å»ºå†…å®¹æŸ¥è¯¢: topic OR keywords (å®½æ¾åŒ¹é…)
        content_parts = []

        # æ·»åŠ ä¸»é¢˜ (ä¸åŠ å¼•å·,å®½æ¾åŒ¹é…)
        if topic:
            content_parts.append(f"ti:{topic}")
            content_parts.append(f"abs:{topic}")

        # æ·»åŠ å…³é”®è¯ (ä¸åŠ å¼•å·,å®½æ¾åŒ¹é…)
        if keywords:
            for kw in keywords:
                content_parts.append(f"ti:{kw}")
                content_parts.append(f"abs:{kw}")

        # ä½¿ç”¨ OR è¿æ¥æ‰€æœ‰å†…å®¹éƒ¨åˆ†
        if content_parts:
            content_query = " OR ".join(content_parts)
            query_parts.append(f"({content_query})")

        # ä½¿ç”¨ANDè¿æ¥åˆ†ç±»å’Œå…³é”®è¯
        return " AND ".join(query_parts)

    def _parse_arxiv_result(self, result: arxiv.Result) -> Dict:
        """
        è§£æarXivç»“æœä¸ºæ ‡å‡†æ ¼å¼

        Args:
            result: arxiv.Resultå¯¹è±¡

        Returns:
            æ ‡å‡†è®ºæ–‡å­—å…¸
        """
        return {
            'arxiv_id': result.get_short_id(),
            'title': result.title,
            'authors': [author.name for author in result.authors],
            'abstract': result.summary,
            'year': result.published.year,
            'published_date': result.published,
            'updated_date': result.updated,
            'categories': result.categories,
            'primary_category': result.primary_category,
            'pdf_url': result.pdf_url,
            'doi': result.doi,
            'comment': result.comment,
            'journal_ref': result.journal_ref,
            # ç”¨äºåç»­æ˜ å°„
            'source': 'arxiv',
            'openalex_id': None  # å¾…æ˜ å°„
        }

    def _compute_relevance(
        self,
        paper: Dict,
        topic: str,
        keywords: Optional[List[str]]
    ) -> float:
        """
        è®¡ç®—è®ºæ–‡ä¸ä¸»é¢˜çš„ç›¸å…³æ€§åˆ†æ•°

        Args:
            paper: è®ºæ–‡æ•°æ®
            topic: ä¸»é¢˜
            keywords: å…³é”®è¯åˆ—è¡¨

        Returns:
            ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        text = (paper['title'] + ' ' + paper['abstract']).lower()
        topic_lower = topic.lower()

        score = 0.0

        # 1. ä¸»é¢˜åŒ¹é…ï¼ˆæƒé‡: 0.4ï¼‰
        # æ”¹è¿›ï¼šæ‹†åˆ†ä¸»é¢˜ä¸ºå•è¯ï¼Œè®¡ç®—è¯æ±‡è¦†ç›–ç‡
        topic_words = [w for w in topic_lower.split() if len(w) > 2]  # è¿‡æ»¤çŸ­è¯
        if topic_words:
            matched_topic_words = sum(1 for word in topic_words if word in text)
            topic_coverage = matched_topic_words / len(topic_words)
            score += 0.4 * topic_coverage
        else:
            # å¦‚æœä¸»é¢˜ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œæ£€æŸ¥å®Œæ•´åŒ¹é…
            if topic_lower in text:
                score += 0.4

        # 2. å…³é”®è¯åŒ¹é…ï¼ˆæƒé‡: 0.4ï¼‰
        if keywords:
            matched_keywords = sum(1 for kw in keywords if kw.lower() in text)
            score += 0.4 * (matched_keywords / len(keywords))

        # 3. åˆ†ç±»åŒ¹é…ï¼ˆæƒé‡: 0.2ï¼‰
        primary_cat = paper.get('primary_category', '')
        if primary_cat.startswith('cs.'):
            score += 0.2

        return min(score, 1.0)

    def retrieve_recent_papers(
        self,
        topic: str,
        keywords: Optional[List[str]] = None,
        categories: Optional[List[str]] = None,
        max_results: int = 20,
        months_back: int = 12
    ) -> List[Dict]:
        """
        æ£€ç´¢æœ€æ–°å‰æ²¿è®ºæ–‡ï¼ˆç”¨äºæ­¥éª¤4ï¼šè¡¥å……SOTAï¼‰

        Args:
            topic: ä¸»é¢˜
            keywords: å…³é”®è¯åˆ—è¡¨
            categories: arXivåˆ†ç±»åˆ—è¡¨
            max_results: æœ€å¤§ç»“æœæ•°
            months_back: å›æº¯æœˆæ•°

        Returns:
            æœ€æ–°è®ºæ–‡åˆ—è¡¨
        """
        logger.info(f"æ£€ç´¢æœ€æ–°è®ºæ–‡: topic='{topic}', å›æº¯{months_back}ä¸ªæœˆ")

        # è‡ªåŠ¨æ¨æ–­categories
        if not categories:
            categories = self._infer_categories(topic)

        # æ„å»ºæŸ¥è¯¢
        query = self._build_arxiv_query(topic, keywords, categories)

        # è®¾ç½®æ—¶é—´èŒƒå›´
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=30 * months_back)

        # æ‰§è¡Œæ£€ç´¢ï¼ˆæŒ‰æäº¤æ—¥æœŸæ’åºï¼‰
        search = arxiv.Search(
            query=query,
            max_results=max_results * 2,  # å¤šå–ä¸€äº›ï¼Œåç»­è¿‡æ»¤
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )

        papers = []
        try:
            # æ˜¾å¼è½¬æ¢ä¸ºåˆ—è¡¨,æ•è·ç½‘ç»œå¼‚å¸¸
            results = list(self.client.results(search))
            logger.info(f"  æˆåŠŸè·å– {len(results)} æ¡æœ€æ–°è®ºæ–‡åŸå§‹ç»“æœ")
        except Exception as e:
            logger.error(f"  âŒ arXiv APIè¯·æ±‚å¤±è´¥: {e}")
            logger.warning("  æç¤º: å›½å†…è®¿é—®arXivå¯èƒ½éœ€è¦ä»£ç†,æˆ–ç¨åé‡è¯•")
            return []

        for result in results:
            # åªè¦æœ€æ–°çš„
            if result.published < cutoff_date:
                continue

            paper = self._parse_arxiv_result(result)
            relevance_score = self._compute_relevance(paper, topic, keywords)
            paper['relevance_score'] = relevance_score

            if relevance_score >= self.min_relevance_score:
                papers.append(paper)
                logger.info(
                    f"  âœ“ [{paper['published_date'].strftime('%Y-%m')}] "
                    f"{paper['title'][:60]}... (ç›¸å…³æ€§: {relevance_score:.2f})"
                )

            if len(papers) >= max_results:
                break

        logger.info(f"âœ… æ£€ç´¢åˆ° {len(papers)} ç¯‡æœ€æ–°è®ºæ–‡")
        return papers


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    retriever = ArxivSeedRetriever(
        max_results_per_query=50,
        years_back=5,
        min_relevance_score=0.6
    )

    # ç¤ºä¾‹1: æ£€ç´¢NLPç§å­è®ºæ–‡
    print("=" * 80)
    print("ç¤ºä¾‹1: æ£€ç´¢NLPç§å­è®ºæ–‡")
    print("=" * 80)
    seeds = retriever.retrieve_seed_papers(
        topic="Natural Language Processing",
        keywords=["transformer", "attention", "language model"],
        max_seeds=10
    )

    for i, paper in enumerate(seeds[:5], 1):
        print(f"\n[{i}] {paper['title']}")
        print(f"    arXiv ID: {paper['arxiv_id']}")
        print(f"    å¹´ä»½: {paper['year']}")
        print(f"    åˆ†ç±»: {paper['primary_category']}")
        print(f"    ç›¸å…³æ€§: {paper['relevance_score']:.2f}")

    # ç¤ºä¾‹2: æ£€ç´¢æœ€æ–°è®ºæ–‡
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹2: æ£€ç´¢æœ€æ–°è®ºæ–‡ï¼ˆè¿‘6ä¸ªæœˆï¼‰")
    print("=" * 80)
    recent = retriever.retrieve_recent_papers(
        topic="Large Language Models",
        keywords=["reasoning", "chain of thought"],
        max_results=10,
        months_back=6
    )

    for i, paper in enumerate(recent[:5], 1):
        print(f"\n[{i}] {paper['title']}")
        print(f"    arXiv ID: {paper['arxiv_id']}")
        print(f"    å‘å¸ƒæ—¥æœŸ: {paper['published_date'].strftime('%Y-%m-%d')}")
        print(f"    ç›¸å…³æ€§: {paper['relevance_score']:.2f}")
