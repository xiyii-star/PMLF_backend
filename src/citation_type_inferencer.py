"""
Socket Matching å¼•ç”¨å…³ç³»ç±»å‹æ¨æ–­å™¨
åŸºäºæ·±åº¦è®ºæ–‡ä¿¡æ¯çš„"æ¥å£å¯¹æ¥"æ–¹æ³•æ¨æ–­å¼•ç”¨å…³ç³»çš„è¯­ä¹‰ç±»å‹

æ ¸å¿ƒæ€æƒ³ï¼š
å°†æå–çš„è®ºæ–‡æ·±åº¦ä¿¡æ¯ï¼ˆProblem, Method, Limitation, Future_Workï¼‰ä½œä¸º"æ¥å£ï¼ˆSocketï¼‰"ï¼Œ
é€šè¿‡ LLM Agent åˆ¤æ–­è¿™äº›æ¥å£æ˜¯å¦èƒ½å¤Ÿå¯¹æ¥ï¼Œä»è€Œæ¨æ–­å¼•ç”¨å…³ç³»çš„è¯­ä¹‰ç±»å‹ã€‚

æ”¯æŒçš„å…³ç³»ç±»å‹ï¼ˆSocket Matching - 6ç§ï¼‰ï¼šï¼ˆOvercomesã€Realizesã€Extendsã€Alternativeã€Adapts_toã€Baselinesï¼‰
1. Overcomes - æ”»å…‹/ä¼˜åŒ–ï¼ˆçºµå‘æ·±åŒ–ï¼‰
   æ¥æºï¼šMatch 1 (Limitationâ†’Problem)
2. Realizes - å®ç°æ„¿æ™¯ï¼ˆç§‘ç ”ä¼ æ‰¿ï¼‰
   æ¥æºï¼šMatch 2 (Future_Workâ†’Problem)
3. Extends - æ–¹æ³•æ‰©å±•ï¼ˆå¾®åˆ›æ–°ï¼‰
   æ¥æºï¼šMatch 3 Extension
4. Alternative - å¦è¾Ÿè¹Šå¾„ï¼ˆé¢ è¦†åˆ›æ–°ï¼‰
   æ¥æºï¼šMatch 3 Alternative
5. Adapts_to - æŠ€æœ¯è¿ç§»ï¼ˆæ¨ªå‘æ‰©æ•£ï¼‰
   æ¥æºï¼šMatch 4 (Problemâ†’Problem è·¨åŸŸ)
6. Baselines - åŸºçº¿å¯¹æ¯”ï¼ˆèƒŒæ™¯å™ªéŸ³ï¼‰
   æ¥æºï¼šæ— åŒ¹é…

é€»è¾‘å¯¹æ¥çŸ©é˜µï¼ˆ4ä¸ªMatch â†’ 6ç§ç±»å‹ï¼‰ï¼š
- Match 1: A.Limitation â†” B.Problem â†’ Overcomes
- Match 2: A.Future_Work â†” B.Problem â†’ Realizes
- Match 3: (Problemä¸€è‡´)A.Method â†” B.Method â†’ Extends(Extension) / Alternative
- Match 4: A.Problem â†” B.Problem(è·¨åŸŸ) â†’ Adapts_to
- æ— åŒ¹é… â†’ Baselines
"""

import json
import logging
import os
import re
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path

# å¯¼å…¥LLMé…ç½®æ¨¡å—
try:
    from llm_config import create_llm_client
except ImportError:
    create_llm_client = None

logger = logging.getLogger(__name__)


@dataclass
class SocketMatchResult:
    """Socket åŒ¹é…ç»“æœ"""
    match_type: str  # "limitation_problem", "future_work_problem", "method_extension", "problem_adaptation"
    is_match: bool
    confidence: float
    reasoning: str
    evidence: str
    additional_info: Dict = None  # é¢å¤–ä¿¡æ¯ï¼ˆå¦‚ relationship_type, source_domain, target_domainï¼‰


@dataclass
class CitationRelationship:
    """å¼•ç”¨å…³ç³»"""
    citing_id: str
    cited_id: str
    relationship_type: str  # Overcomes, Realizes, Extends, Alternative, Adapts_to, Baselines (6ç§ç±»å‹)
    confidence: float
    reasoning: str
    evidence: str
    match_results: List[SocketMatchResult]


class CitationTypeInferencer:
    """
    Socket Matching å¼•ç”¨å…³ç³»ç±»å‹æ¨æ–­å™¨

    ä½¿ç”¨ LLM Agent è¿›è¡Œæ·±åº¦è¯­ä¹‰åˆ†æï¼Œé€šè¿‡"æ¥å£å¯¹æ¥"çš„æ–¹å¼åˆ¤æ–­å¼•ç”¨å…³ç³»ç±»å‹
    """

    def __init__(self, llm_client=None, config_path: str = None, prompts_dir: str = "./prompts"):
        """
        åˆå§‹åŒ–æ¨æ–­å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼‰
            config_path: LLMé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæä¾›ä¸”llm_clientä¸ºNoneï¼Œåˆ™ä»æ­¤æ–‡ä»¶åŠ è½½ï¼‰
            prompts_dir: æç¤ºè¯ç›®å½•
        """
        # å¦‚æœæä¾›äº†config_pathä½†æ²¡æœ‰æä¾›llm_clientï¼Œå°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½
        if llm_client is None and config_path:
            if create_llm_client is None:
                logger.warning("æ— æ³•å¯¼å…¥create_llm_clientï¼Œå°†ä½¿ç”¨è§„åˆ™æ–¹æ³•")
                self.llm_client = None
            else:
                try:
                    config_file = Path(config_path)
                    if config_file.exists():
                        self.llm_client = create_llm_client(str(config_file))
                        logger.info(f"âœ… ä»é…ç½®æ–‡ä»¶åŠ è½½LLMå®¢æˆ·ç«¯: {config_path}")
                    else:
                        logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œå°†ä½¿ç”¨è§„åˆ™æ–¹æ³•")
                        self.llm_client = None
                except Exception as e:
                    logger.warning(f"åŠ è½½LLMå®¢æˆ·ç«¯å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨è§„åˆ™æ–¹æ³•")
                    self.llm_client = None
        else:
            self.llm_client = llm_client

        self.prompts_dir = Path(prompts_dir)
        self.prompts_cache = {}

        # åŠ è½½æç¤ºè¯
        self._load_prompts()

        # å…³ç³»ç±»å‹ä¼˜å…ˆçº§ï¼ˆç”¨äºè§„åˆ™æ–¹æ³•å’Œå†²çªè§£å†³ï¼‰
        self.relationship_priority = {
            "Overcomes": 6,     # æœ€é«˜ä¼˜å…ˆçº§ - ç›´æ¥è§£å†³é—®é¢˜
            "Realizes": 5,      # æ¬¡é«˜ä¼˜å…ˆçº§ - å®ç°æ„¿æ™¯
            "Adapts_to": 4,     # é«˜ä¼˜å…ˆçº§ - æŠ€æœ¯è¿ç§»
            "Extends": 3,       # ä¸­é«˜ä¼˜å…ˆçº§ - æ–¹æ³•æ‰©å±•
            "Alternative": 2,   # ä¸­ä¼˜å…ˆçº§ - å¦è¾Ÿè¹Šå¾„
            "Baselines": 1      # æœ€ä½ä¼˜å…ˆçº§ - åŸºçº¿å¯¹æ¯”
        }

        logger.info("CitationTypeInferencer åˆå§‹åŒ–å®Œæˆ")
        if self.llm_client:
            logger.info("  æ¨¡å¼: LLM Socket Matching")
        else:
            logger.info("  æ¨¡å¼: åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼ˆé™çº§æ¨¡å¼ï¼‰")

    def _load_prompts(self):
        """åŠ è½½æ‰€æœ‰æç¤ºè¯"""
        prompt_files = {
            'match_limitation_problem': 'match_limitation_problem.txt',
            'match_future_work_problem': 'match_future_work_problem.txt',
            'match_method_extension': 'match_method_extension.txt',
            'match_problem_adaptation': 'match_problem_adaptation.txt',
        }

        for key, filename in prompt_files.items():
            file_path = self.prompts_dir / filename
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        self.prompts_cache[key] = f.read().strip()
                    logger.debug(f"  åŠ è½½æç¤ºè¯: {key}")
                except Exception as e:
                    logger.warning(f"  åŠ è½½æç¤ºè¯å¤±è´¥ ({key}): {e}")
            else:
                logger.warning(f"  æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {filename}")

        logger.info(f"åŠ è½½ {len(self.prompts_cache)} ä¸ªæç¤ºè¯æ¨¡æ¿")

    def infer_edge_types(
        self,
        papers: List[Dict],
        citation_edges: List[Tuple[str, str]]
    ) -> Tuple[List[Tuple[str, str, str]], Dict[str, int]]:
        """
        æ‰¹é‡æ¨æ–­å¼•ç”¨å…³ç³»ç±»å‹

        Args:
            papers: è®ºæ–‡åˆ—è¡¨ï¼ˆå¿…é¡»åŒ…å« rag_analysis æˆ– deep_analysisï¼‰
            citation_edges: å¼•ç”¨å…³ç³»åˆ—è¡¨ [(citing_id, cited_id), ...]

        Returns:
            (typed_edges, statistics)
            - typed_edges: å¸¦ç±»å‹çš„å¼•ç”¨å…³ç³» [(citing_id, cited_id, edge_type), ...]
            - statistics: ç±»å‹ç»Ÿè®¡ {edge_type: count}
        """
        logger.info(f"å¼€å§‹æ¨æ–­ {len(citation_edges)} æ¡å¼•ç”¨å…³ç³»çš„ç±»å‹...")

        # æ„å»ºè®ºæ–‡å­—å…¸
        papers_dict = {paper['id']: paper for paper in papers}

        # æ¨æ–­æ¯æ¡è¾¹çš„ç±»å‹
        typed_edges = []
        statistics = {}
        relationships = []

        for i, (citing_id, cited_id) in enumerate(citation_edges):
            logger.info(f"å¤„ç†å¼•ç”¨å…³ç³» {i+1}/{len(citation_edges)}: {citing_id} -> {cited_id}")

            if citing_id in papers_dict and cited_id in papers_dict:
                relationship = self.infer_single_edge_type(
                    papers_dict[citing_id],
                    papers_dict[cited_id]
                )
                relationships.append(relationship)
                edge_type = relationship.relationship_type
            else:
                # è®ºæ–‡ä¸åœ¨å­—å…¸ä¸­ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹
                edge_type = "Baselines"
                logger.warning(f"  è®ºæ–‡ä¸åœ¨å­—å…¸ä¸­ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹: {edge_type}")

            typed_edges.append((citing_id, cited_id, edge_type))
            statistics[edge_type] = statistics.get(edge_type, 0) + 1

        logger.info(f"âœ… å¼•ç”¨ç±»å‹æ¨æ–­å®Œæˆ")
        logger.info(f"  æ€»å¼•ç”¨å…³ç³»: {len(typed_edges)} æ¡")
        logger.info(f"\nğŸ“Š å¼•ç”¨ç±»å‹åˆ†å¸ƒ:")
        for edge_type, count in sorted(statistics.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(typed_edges)) * 100 if typed_edges else 0
            logger.info(f"  â€¢ {edge_type}: {count} æ¡ ({percentage:.1f}%)")

        return typed_edges, statistics

    def infer_single_edge_type(
        self,
        citing_paper: Dict,
        cited_paper: Dict
    ) -> CitationRelationship:
        """
        æ¨æ–­å•æ¡å¼•ç”¨å…³ç³»çš„ç±»å‹ï¼ˆSocket Matchingï¼‰

        Args:
            citing_paper: å¼•ç”¨è®ºæ–‡ï¼ˆPaper Bï¼‰
            cited_paper: è¢«å¼•ç”¨è®ºæ–‡ï¼ˆPaper Aï¼‰

        Returns:
            CitationRelationship å¯¹è±¡
        """
        # æå–æ·±åº¦åˆ†æä¿¡æ¯
        citing_analysis = self._extract_deep_analysis(citing_paper)
        cited_analysis = self._extract_deep_analysis(cited_paper)

        # æå–å¼•ç”¨ä¸Šä¸‹æ–‡
        citation_context = self._extract_citation_context(citing_paper, cited_paper)

        # å¦‚æœæ²¡æœ‰LLMå®¢æˆ·ç«¯ï¼Œä½¿ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•
        if not self.llm_client:
            return self._rule_based_inference(
                citing_paper, cited_paper, citing_analysis, cited_analysis
            )

        # Socket Matching: æ‰§è¡Œ4ä¸ªåŒ¹é…æ£€æµ‹
        match_results = []

        # Match 1: A.Limitation â†” B.Problem
        if cited_analysis.get('limitation') and citing_analysis.get('problem'):
            match = self._check_limitation_problem_match(
                cited_paper, citing_paper, cited_analysis, citing_analysis, citation_context
            )
            if match:
                match_results.append(match)

        # Match 2: A.Future_Work â†” B.Problem
        if cited_analysis.get('future_work') and citing_analysis.get('problem'):
            match = self._check_future_work_problem_match(
                cited_paper, citing_paper, cited_analysis, citing_analysis, citation_context
            )
            if match:
                match_results.append(match)

        # Match 3: A.Method â†” B.Method
        if cited_analysis.get('method') and citing_analysis.get('method'):
            match = self._check_method_extension_match(
                cited_paper, citing_paper, cited_analysis, citing_analysis, citation_context
            )
            if match:
                match_results.append(match)

        # Match 4: A.Problem â†” B.Problem (ä½†åœºæ™¯ä¸åŒ)
        if cited_analysis.get('problem') and citing_analysis.get('problem'):
            match = self._check_problem_adaptation_match(
                cited_paper, citing_paper, cited_analysis, citing_analysis, citation_context
            )
            if match:
                match_results.append(match)

        # ç»¼åˆæ‰€æœ‰åŒ¹é…ç»“æœï¼Œæœ€ç»ˆåˆ†ç±»
        relationship = self._classify_relationship(
            citing_paper, cited_paper, match_results, citation_context
        )

        return relationship

    def _check_limitation_problem_match(
        self,
        cited_paper: Dict,
        citing_paper: Dict,
        cited_analysis: Dict,
        citing_analysis: Dict,
        citation_context: str
    ) -> Optional[SocketMatchResult]:
        """
        Match 1: æ£€æŸ¥ A.Limitation â†” B.Problem
        åˆ¤æ–­ B æ˜¯å¦è§£å†³äº† A çš„å±€é™æ€§
        """
        prompt_template = self.prompts_cache.get('match_limitation_problem')
        if not prompt_template:
            logger.warning("ç¼ºå°‘ match_limitation_problem æç¤ºè¯ï¼Œè·³è¿‡")
            return None

        # å¡«å……æç¤ºè¯
        prompt = prompt_template.format(
            cited_title=cited_paper.get('title', 'Unknown'),
            cited_limitation=cited_analysis.get('limitation', 'N/A'),
            citing_title=citing_paper.get('title', 'Unknown'),
            citing_problem=citing_analysis.get('problem', 'N/A'),
            citation_context=citation_context
        )

        # è°ƒç”¨LLM
        try:
            response = self.llm_client.generate(prompt)

            # æå–JSONå†…å®¹
            json_str = self._extract_json_from_response(response)
            result = json.loads(json_str)

            return SocketMatchResult(
                match_type="limitation_problem",
                is_match=result.get('is_match', False),
                confidence=result.get('confidence', 0.0),
                reasoning=result.get('reasoning', ''),
                evidence=result.get('evidence', '')
            )
        except Exception as e:
            logger.error(f"Match 1 (Limitation-Problem) å¤±è´¥: {e}")
            return None

    def _check_future_work_problem_match(
        self,
        cited_paper: Dict,
        citing_paper: Dict,
        cited_analysis: Dict,
        citing_analysis: Dict,
        citation_context: str
    ) -> Optional[SocketMatchResult]:
        """
        Match 2: æ£€æŸ¥ A.Future_Work â†” B.Problem
        åˆ¤æ–­ B æ˜¯å¦å®ç°äº† A çš„æœªæ¥å·¥ä½œå»ºè®®
        """
        # å¦‚æœ A çš„ Future Work æå–ä¸ºç©ºï¼Œæˆ–è€…å¤ªçŸ­ï¼Œç›´æ¥è·³è¿‡ Match 2
        future_work = cited_analysis.get('future_work', '')
        if not future_work or len(future_work) < 5 or future_work == "N/A":
            logger.info("    â†’ Match 2 è·³è¿‡: Açš„Future Workä¸ºç©ºæˆ–è¿‡çŸ­")
            return None

        prompt_template = self.prompts_cache.get('match_future_work_problem')
        if not prompt_template:
            logger.warning("ç¼ºå°‘ match_future_work_problem æç¤ºè¯ï¼Œè·³è¿‡")
            return None

        prompt = prompt_template.format(
            cited_title=cited_paper.get('title', 'Unknown'),
            cited_year=cited_paper.get('year', 'N/A'),
            cited_future_work=future_work,
            citing_title=citing_paper.get('title', 'Unknown'),
            citing_year=citing_paper.get('year', 'N/A'),
            citing_problem=citing_analysis.get('problem', 'N/A'),
            citation_context=citation_context
        )

        try:
            response = self.llm_client.generate(prompt)
            json_str = self._extract_json_from_response(response)
            result = json.loads(json_str)

            # åŒé‡è¿‡æ»¤ï¼šåŒºåˆ†çœŸä¼ æ‰¿(Realizes) vs å‡å®¢å¥—(Extends/Baselines)
            is_match = result.get('is_match', False)
            specificity = result.get('specificity', 'low')
            confidence = result.get('confidence', 0.0)

            # åœºæ™¯1: LLMè®¤ä¸ºåŒ¹é… + å»ºè®®å¾ˆå…·ä½“(high specificity) â†’ çœŸæ­£çš„Realizes
            if is_match and specificity == "high" and confidence > 0.6:
                logger.info(f"    â†’ Match 2 å…·ä½“æ€§æ£€æŸ¥: âœ“ é«˜å…·ä½“æ€§ (specificity=high, conf={confidence:.2f})")
                return SocketMatchResult(
                    match_type="future_work_problem",
                    is_match=True,
                    confidence=confidence,
                    reasoning=result.get('reasoning', ''),
                    evidence=result.get('evidence', ''),
                    additional_info={'specificity': 'high'}
                )

            # åœºæ™¯2: LLMè®¤ä¸ºåŒ¹é… + ä½†å»ºè®®å¾ˆå®½æ³›(low specificity) â†’ å‡å®¢å¥—,é™çº§
            elif is_match and specificity == "low":
                logger.info(f"    â†’ Match 2 å…·ä½“æ€§æ£€æŸ¥: âœ— ä½å…·ä½“æ€§ (specificity=low, conf={confidence:.2f}) - ç–‘ä¼¼å®¢å¥—è¯,ä¸è®¡å…¥Realizes")
                return SocketMatchResult(
                    match_type="future_work_problem",
                    is_match=False,  # å¼ºåˆ¶æ ‡è®°ä¸ºä¸åŒ¹é…
                    confidence=0.0,
                    reasoning=f"[è¿‡æ»¤] Açš„Future Workè¿‡äºå®½æ³›,ä¸ç¬¦åˆRealizesæ ‡å‡†ã€‚{result.get('reasoning', '')}",
                    evidence=result.get('evidence', ''),
                    additional_info={'specificity': 'low', 'filtered': True}
                )

            # åœºæ™¯3: LLMè®¤ä¸ºä¸åŒ¹é…
            else:
                logger.info(f"    â†’ Match 2: ä¸åŒ¹é… (is_match=False)")
                return SocketMatchResult(
                    match_type="future_work_problem",
                    is_match=False,
                    confidence=0.0,
                    reasoning=result.get('reasoning', ''),
                    evidence=result.get('evidence', '')
                )
        except Exception as e:
            logger.error(f"Match 2 (FutureWork-Problem) å¤±è´¥: {e}")
            return None

    def _check_method_extension_match(
        self,
        cited_paper: Dict,
        citing_paper: Dict,
        cited_analysis: Dict,
        citing_analysis: Dict,
        citation_context: str
    ) -> Optional[SocketMatchResult]:
        """
        Match 3: æ£€æŸ¥ A.Method â†” B.Method
        åˆ¤æ–­æ˜¯ Extensionï¼ˆæ‰©å±•ï¼‰è¿˜æ˜¯ Alternativeï¼ˆå¦è¾Ÿè¹Šå¾„ï¼‰
        """
        prompt_template = self.prompts_cache.get('match_method_extension')
        if not prompt_template:
            logger.warning("ç¼ºå°‘ match_method_extension æç¤ºè¯ï¼Œè·³è¿‡")
            return None

        prompt = prompt_template.format(
            cited_title=cited_paper.get('title', 'Unknown'),
            cited_year=cited_paper.get('year', 'N/A'),
            cited_problem=cited_analysis.get('problem', 'N/A'),
            cited_method=cited_analysis.get('method', 'N/A'),
            citing_title=citing_paper.get('title', 'Unknown'),
            citing_year=citing_paper.get('year', 'N/A'),
            citing_problem=citing_analysis.get('problem', 'N/A'),
            citing_method=citing_analysis.get('method', 'N/A'),
            citation_context=citation_context
        )

        try:
            response = self.llm_client.generate(prompt)
            json_str = self._extract_json_from_response(response)
            result = json.loads(json_str)

            return SocketMatchResult(
                match_type="method_extension",
                is_match=(result.get('relationship_type') != 'none'),
                confidence=result.get('confidence', 0.0),
                reasoning=result.get('reasoning', ''),
                evidence=result.get('evidence', ''),
                additional_info={'relationship_type': result.get('relationship_type', 'none')}
            )
        except Exception as e:
            logger.error(f"Match 3 (Method Extension) å¤±è´¥: {e}")
            return None

    def _check_problem_adaptation_match(
        self,
        cited_paper: Dict,
        citing_paper: Dict,
        cited_analysis: Dict,
        citing_analysis: Dict,
        citation_context: str
    ) -> Optional[SocketMatchResult]:
        """
        Match 4: æ£€æŸ¥ A.Problem â†” B.Problem (ä½†åœºæ™¯ä¸åŒ)
        åˆ¤æ–­æ˜¯å¦æ˜¯æŠ€æœ¯è¿ç§»/æ³›åŒ–
        """
        prompt_template = self.prompts_cache.get('match_problem_adaptation')
        if not prompt_template:
            logger.warning("ç¼ºå°‘ match_problem_adaptation æç¤ºè¯ï¼Œè·³è¿‡")
            return None

        prompt = prompt_template.format(
            cited_title=cited_paper.get('title', 'Unknown'),
            cited_year=cited_paper.get('year', 'N/A'),
            cited_problem=cited_analysis.get('problem', 'N/A'),
            cited_method=cited_analysis.get('method', 'N/A'),
            citing_title=citing_paper.get('title', 'Unknown'),
            citing_year=citing_paper.get('year', 'N/A'),
            citing_problem=citing_analysis.get('problem', 'N/A'),
            citing_method=citing_analysis.get('method', 'N/A'),
            citation_context=citation_context
        )

        try:
            response = self.llm_client.generate(prompt)
            json_str = self._extract_json_from_response(response)
            result = json.loads(json_str)

            # åŒé‡è¿‡æ»¤ï¼šåŒºåˆ†çœŸè·¨åŸŸè¿ç§»(Adapts_to) vs æ¢æ•°æ®é›†(Extends)
            is_adaptation = result.get('is_adaptation', False)
            domain_shift_type = result.get('domain_shift_type', 'none')
            confidence = result.get('confidence', 0.0)

            # åœºæ™¯1: çœŸæ­£çš„è·¨åŸŸè¿ç§» (cross-task/cross-modality) â†’ Adapts_to
            if is_adaptation and domain_shift_type in ['cross-task', 'cross-modality']:
                logger.info(f"    â†’ Match 4 é¢†åŸŸè·¨åº¦æ£€æŸ¥: âœ“ çœŸè·¨åŸŸè¿ç§» (type={domain_shift_type}, conf={confidence:.2f})")
                return SocketMatchResult(
                    match_type="problem_adaptation",
                    is_match=True,
                    confidence=confidence,
                    reasoning=result.get('reasoning', ''),
                    evidence=result.get('evidence', ''),
                    additional_info={
                        'source_domain': result.get('source_domain', ''),
                        'target_domain': result.get('target_domain', ''),
                        'domain_shift_type': domain_shift_type
                    }
                )

            # åœºæ™¯2: åªæ˜¯æ¢æ•°æ®é›† (same-task-new-data) â†’ ä¸ç®—Adapts_to,é™çº§
            elif is_adaptation and domain_shift_type == 'same-task-new-data':
                logger.info(f"    â†’ Match 4 é¢†åŸŸè·¨åº¦æ£€æŸ¥: âœ— ä»…æ¢æ•°æ®é›† (type={domain_shift_type}, conf={confidence:.2f}) - ä¸ç®—çœŸæ­£çš„Adapts_to")
                return SocketMatchResult(
                    match_type="problem_adaptation",
                    is_match=False,  # å¼ºåˆ¶æ ‡è®°ä¸ºä¸åŒ¹é…
                    confidence=0.0,
                    reasoning=f"[è¿‡æ»¤] ä»…åœ¨åŒç±»ä»»åŠ¡ä¸Šæ›´æ¢æ•°æ®é›†,ä¸ç¬¦åˆçœŸæ­£çš„é¢†åŸŸè¿ç§»ã€‚{result.get('reasoning', '')}",
                    evidence=result.get('evidence', ''),
                    additional_info={
                        'domain_shift_type': domain_shift_type,
                        'filtered': True
                    }
                )

            # åœºæ™¯3: ä¸æ˜¯adaptationæˆ–domain_shift_typeä¸ºnone
            else:
                logger.info(f"    â†’ Match 4: ä¸åŒ¹é… (is_adaptation={is_adaptation}, type={domain_shift_type})")
                return SocketMatchResult(
                    match_type="problem_adaptation",
                    is_match=False,
                    confidence=0.0,
                    reasoning=result.get('reasoning', ''),
                    evidence=result.get('evidence', '')
                )
        except Exception as e:
            logger.error(f"Match 4 (Problem Adaptation) å¤±è´¥: {e}")
            return None

    def _classify_relationship(
        self,
        citing_paper: Dict,
        cited_paper: Dict,
        match_results: List[SocketMatchResult],
        citation_context: str  # ä¿ç•™ä»¥å¤‡æœªæ¥ä½¿ç”¨
    ) -> CitationRelationship:
        """
        ç»¼åˆæ‰€æœ‰åŒ¹é…ç»“æœï¼Œæœ€ç»ˆåˆ†ç±»å…³ç³»ç±»å‹
        ä½¿ç”¨åŸºäºä¼˜å…ˆçº§çš„å†³ç­–æ ‘é€»è¾‘ï¼ˆä¸å†ä¾èµ–LLMï¼‰

        å†³ç­–æ ‘é€»è¾‘ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
        1. Match 1 (Limitationâ†’Problem) æˆåŠŸ â†’ Overcomes
        2. Match 2 (Future_Workâ†’Problem) æˆåŠŸ â†’ Realizes
        3. Match 4 (Problemâ†’Problem è·¨åŸŸ) æˆåŠŸ â†’ Adapts_to
        4. Match 3 (Methodâ†’Method) æˆåŠŸ:
           - extension â†’ Extends
           - alternative â†’ Alternative
           - none â†’ Baselines
        5. æ— ä»»ä½•åŒ¹é… â†’ Baselines

        ä¼˜å…ˆçº§æ’åºï¼šOvercomes > Realizes > Adapts_to > Extends > Alternative > Baselines
        """
        # å¦‚æœæ²¡æœ‰åŒ¹é…ç»“æœï¼Œé»˜è®¤ä¸º Baselines
        if not match_results:
            logger.info("  æ— åŒ¹é…ç»“æœ -> Baselines")
            return CitationRelationship(
                citing_id=citing_paper['id'],
                cited_id=cited_paper['id'],
                relationship_type="Baselines",
                confidence=0.3,
                reasoning="æ— æ˜ç¡®çš„æ·±åº¦å…³ç³»ï¼Œä»…ä½œä¸ºåŸºçº¿å¯¹æ¯”",
                evidence="",
                match_results=[]
            )

        # å°†åŒ¹é…ç»“æœæŒ‰ç±»å‹ç»„ç»‡
        matches_by_type = {}
        for match in match_results:
            if match.is_match:
                matches_by_type[match.match_type] = match

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥åŒ¹é…ç»“æœ
        relationship_type = "Baselines"
        confidence = 0.3
        reasoning = "æ— æ˜ç¡®çš„æ·±åº¦å…³ç³»ï¼Œä»…ä½œä¸ºåŸºçº¿å¯¹æ¯”"
        evidence = ""
        relationship_decided = False  # æ ‡è®°æ˜¯å¦å·²ç¡®å®šå…³ç³»ç±»å‹

        # ä¼˜å…ˆçº§1: Match 1 (Limitationâ†’Problem) â†’ Overcomes
        if not relationship_decided and "limitation_problem" in matches_by_type:
            match = matches_by_type["limitation_problem"]
            relationship_type = "Overcomes"
            confidence = match.confidence
            reasoning = f"Bè§£å†³äº†Açš„å±€é™æ€§ã€‚{match.reasoning}"
            evidence = match.evidence
            relationship_decided = True
            logger.info(f"  âœ“ Match 1 (Limitationâ†’Problem) åŒ¹é…æˆåŠŸ -> Overcomes (ç½®ä¿¡åº¦: {confidence:.2f})")

        # ä¼˜å…ˆçº§2: Match 2 (Future_Workâ†’Problem) â†’ Realizes
        # ç‰¹åˆ«æ³¨æ„ï¼šå¿…é¡»æ˜¯é«˜å…·ä½“æ€§çš„future workï¼Œä¸èƒ½æ˜¯å®¢å¥—è¯
        if not relationship_decided and "future_work_problem" in matches_by_type:
            match = matches_by_type["future_work_problem"]

            # åŒé‡éªŒè¯ï¼šæ£€æŸ¥specificity
            specificity = match.additional_info.get('specificity', 'low') if match.additional_info else 'low'

            if specificity == "high" and match.confidence > 0.6:
                # çœŸæ­£çš„Realizesï¼šAæŒ–å‘ Bå¡«å‘
                relationship_type = "Realizes"
                confidence = match.confidence
                reasoning = f"Bå®ç°äº†Aè®¾æƒ³çš„å…·ä½“æœªæ¥å·¥ä½œæ–¹å‘ã€‚{match.reasoning}"
                evidence = match.evidence
                relationship_decided = True
                logger.info(f"  âœ“ Match 2 (Future_Workâ†’Problem) åŒ¹é…æˆåŠŸ -> Realizes (ç½®ä¿¡åº¦: {confidence:.2f}) [é«˜å…·ä½“æ€§]")
            else:
                # ä½å…·ä½“æ€§æˆ–ä½ç½®ä¿¡åº¦ï¼šä¸ç®—Realizesï¼Œç»§ç»­æ£€æŸ¥å…¶ä»–Match
                logger.info(f"  âš  Match 2 æ£€æµ‹åˆ°ä½†å…·ä½“æ€§ä¸è¶³ (specificity={specificity}, conf={match.confidence:.2f}) - è·³è¿‡Realizesï¼Œæ£€æŸ¥å…¶ä»–Match")

        # ä¼˜å…ˆçº§3: Match 4 (Problemâ†’Problem è·¨åŸŸ) â†’ Adapts_to
        # ç‰¹åˆ«æ³¨æ„ï¼šå¿…é¡»æ˜¯çœŸæ­£çš„è·¨ä»»åŠ¡/è·¨æ¨¡æ€ï¼Œä¸èƒ½åªæ˜¯æ¢æ•°æ®é›†
        if not relationship_decided and "problem_adaptation" in matches_by_type:
            match = matches_by_type["problem_adaptation"]

            # åŒé‡éªŒè¯ï¼šæ£€æŸ¥domain_shift_type
            domain_shift_type = match.additional_info.get('domain_shift_type', 'none') if match.additional_info else 'none'

            if domain_shift_type in ['cross-task', 'cross-modality']:
                # çœŸæ­£çš„è·¨åŸŸè¿ç§»ï¼šæŠ€æœ¯æ¨ªå‘æ‰©æ•£
                relationship_type = "Adapts_to"
                confidence = match.confidence
                source_domain = match.additional_info.get('source_domain', '') if match.additional_info else ''
                target_domain = match.additional_info.get('target_domain', '') if match.additional_info else ''
                reasoning = f"Bå°†Açš„æ–¹æ³•è¿ç§»åˆ°ä¸åŒé¢†åŸŸï¼ˆ{source_domain} â†’ {target_domain}ï¼Œ{domain_shift_type}ï¼‰ã€‚{match.reasoning}"
                evidence = match.evidence
                relationship_decided = True
                logger.info(f"  âœ“ Match 4 (Problemâ†’Problem è·¨åŸŸ) åŒ¹é…æˆåŠŸ -> Adapts_to (ç½®ä¿¡åº¦: {confidence:.2f}) [{domain_shift_type}]")
            else:
                # ä»…æ¢æ•°æ®é›†æˆ–æ— æ˜æ˜¾è·¨åº¦ï¼šä¸ç®—Adapts_to
                logger.info(f"  âš  Match 4 æ£€æµ‹åˆ°ä½†é¢†åŸŸè·¨åº¦ä¸è¶³ (type={domain_shift_type}, conf={match.confidence:.2f}) - è·³è¿‡Adapts_to")

        # ä¼˜å…ˆçº§4-5: Match 3 (Methodâ†’Method) â†’ Extends / Alternative
        if not relationship_decided and "method_extension" in matches_by_type:
            match = matches_by_type["method_extension"]
            rel_type = match.additional_info.get('relationship_type', 'none') if match.additional_info else 'none'

            if rel_type == "extension":
                relationship_type = "Extends"
                confidence = match.confidence
                reasoning = f"Båœ¨Açš„æ–¹æ³•åŸºç¡€ä¸Šåšäº†å¢é‡æ”¹è¿›ã€‚{match.reasoning}"
                evidence = match.evidence
                relationship_decided = True
                logger.info(f"  âœ“ Match 3 (Method Extension) åŒ¹é…æˆåŠŸ -> Extends (ç½®ä¿¡åº¦: {confidence:.2f})")

            elif rel_type == "alternative":
                relationship_type = "Alternative"
                confidence = match.confidence
                reasoning = f"Bä½¿ç”¨ä¸åŒèŒƒå¼è§£å†³ç±»ä¼¼é—®é¢˜ã€‚{match.reasoning}"
                evidence = match.evidence
                relationship_decided = True
                logger.info(f"  âœ“ Match 3 (Method Alternative) åŒ¹é…æˆåŠŸ -> Alternative (ç½®ä¿¡åº¦: {confidence:.2f})")

            else:  # rel_type == "none"
                relationship_type = "Baselines"
                confidence = 0.4
                reasoning = "æ–¹æ³•ä¹‹é—´æ— æ˜ç¡®ç»§æ‰¿æˆ–æ”¹è¿›å…³ç³»ï¼Œä»…ä½œä¸ºåŸºçº¿å¯¹æ¯”"
                evidence = match.evidence
                relationship_decided = True
                logger.info(f"  âœ“ Match 3 (Method None) -> Baselines (ç½®ä¿¡åº¦: {confidence:.2f})")

        # ä¼˜å…ˆçº§6: æ— æœ‰æ•ˆåŒ¹é… â†’ Baselines
        if not relationship_decided:
            logger.info("  æ‰€æœ‰åŒ¹é…å‡æœªæˆåŠŸ -> Baselines")

        return CitationRelationship(
            citing_id=citing_paper['id'],
            cited_id=cited_paper['id'],
            relationship_type=relationship_type,
            confidence=confidence,
            reasoning=reasoning,
            evidence=evidence,
            match_results=match_results
        )

    def _extract_deep_analysis(self, paper: Dict) -> Dict:
        """
        æå–è®ºæ–‡çš„æ·±åº¦åˆ†æä¿¡æ¯
        ä¼˜å…ˆçº§ï¼šdeep_analysis > rag_analysis > ç©ºå­—å…¸
        """
        if 'deep_analysis' in paper:
            return paper['deep_analysis']
        elif 'rag_analysis' in paper:
            return paper['rag_analysis']
        else:
            return {}

    def _extract_citation_context(self, citing_paper: Dict, cited_paper: Dict) -> str:
        """
        æå–å¼•ç”¨ä¸Šä¸‹æ–‡ - ä»PDFæ–‡ä»¶ä¸­æå–å¼•ç”¨Açš„å…·ä½“å¥å­

        ä¼˜å…ˆçº§:
        1. ä»PDFæ–‡ä»¶ä¸­æå–å¼•ç”¨ä¸Šä¸‹æ–‡(åŸºäºä½œè€…åå’Œå¹´ä»½åŒ¹é…)
        2. å¦‚æœPDFä¸å¯ç”¨æˆ–æå–å¤±è´¥ï¼Œè¿”å›ç®€å•æè¿°

        Args:
            citing_paper: å¼•ç”¨è®ºæ–‡(Paper B)
            cited_paper: è¢«å¼•è®ºæ–‡(Paper A)

        Returns:
            å¼•ç”¨ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼Œå¯èƒ½åŒ…å«å¤šä¸ªå¼•ç”¨ç‚¹
        """
        # å°è¯•ä»PDFæå–å¼•ç”¨ä¸Šä¸‹æ–‡
        try:
            contexts = self._extract_citation_from_pdf(citing_paper, cited_paper)
            if contexts:
                # è¿”å›å‰3ä¸ªå¼•ç”¨ä¸Šä¸‹æ–‡
                context_str = " | ".join([
                    f"[p.{ctx['page']}] {ctx['context']}"
                    for ctx in contexts[:3]
                ])
                logger.debug(f"ä»PDFæå–åˆ° {len(contexts)} ä¸ªå¼•ç”¨ä¸Šä¸‹æ–‡")
                return context_str
        except Exception as e:
            logger.warning(f"ä»PDFæå–å¼•ç”¨ä¸Šä¸‹æ–‡å¤±è´¥: {e}")

        # é™çº§æ–¹æ¡ˆï¼šè¿”å›ç®€å•æè¿°
        return f"{citing_paper.get('title', 'Paper B')} å¼•ç”¨äº† {cited_paper.get('title', 'Paper A')}"

    def _extract_citation_from_pdf(self, citing_paper: Dict, cited_paper: Dict) -> List[Dict]:
        """
        ä»PDFæ–‡ä»¶ä¸­æå–å¼•ç”¨ä¸Šä¸‹æ–‡

        ç­–ç•¥:
        1. å®šä½PDFæ–‡ä»¶è·¯å¾„
        2. æå–PDFå…¨æ–‡
        3. ä½¿ç”¨å¼•ç”¨æ¨¡å¼åŒ¹é…(åŸºäºä½œè€…åå’Œå¹´ä»½)
        4. æå–å¼•ç”¨å‰åçš„ä¸Šä¸‹æ–‡

        Args:
            citing_paper: å¼•ç”¨è®ºæ–‡
            cited_paper: è¢«å¼•è®ºæ–‡

        Returns:
            å¼•ç”¨ä¸Šä¸‹æ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'page': int, 'context': str}
        """
        # 1. å®šä½PDFæ–‡ä»¶
        pdf_path = self._get_pdf_path(citing_paper)
        if not pdf_path or not os.path.exists(pdf_path):
            logger.debug(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            return []

        # 2. è·å–è¢«å¼•è®ºæ–‡çš„è¯†åˆ«ä¿¡æ¯
        cited_info = self._extract_citation_identifiers(cited_paper)
        if not cited_info:
            logger.debug(f"æ— æ³•æå–è¢«å¼•è®ºæ–‡çš„è¯†åˆ«ä¿¡æ¯: {cited_paper.get('id')}")
            return []

        # 3. å°è¯•ä½¿ç”¨PyMuPDFæå–
        try:
            import fitz  # PyMuPDF
            contexts = []

            with fitz.open(pdf_path) as doc:
                for page_num, page in enumerate(doc):
                    text = page.get_text()

                    # æŸ¥æ‰¾å¼•ç”¨æ¨¡å¼
                    matches = self._find_citation_patterns(text, cited_info)

                    for match in matches:
                        # æå–ä¸Šä¸‹æ–‡(å¼•ç”¨å‰åå„100ä¸ªå­—ç¬¦)
                        start = max(0, match['start'] - 100)
                        end = min(len(text), match['end'] + 100)
                        context = text[start:end].strip()

                        # æ¸…ç†ä¸Šä¸‹æ–‡(ç§»é™¤å¤šä½™æ¢è¡Œå’Œç©ºæ ¼)
                        context = ' '.join(context.split())

                        contexts.append({
                            'page': page_num + 1,
                            'context': context,
                            'citation_text': match['citation']
                        })

            return contexts

        except ImportError:
            # PyMuPDFæœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨PyPDF2
            logger.debug("PyMuPDFæœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨PyPDF2")
            return self._extract_citation_from_pdf_pypdf2(pdf_path, cited_info)
        except Exception as e:
            logger.warning(f"ä½¿ç”¨PyMuPDFæå–å¤±è´¥: {e}")
            return []

    def _extract_citation_from_pdf_pypdf2(self, pdf_path: str, cited_info: Dict) -> List[Dict]:
        """ä½¿ç”¨PyPDF2æå–å¼•ç”¨ä¸Šä¸‹æ–‡(é™çº§æ–¹æ¡ˆ)"""
        try:
            import PyPDF2
            contexts = []

            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)

                for page_num, page in enumerate(reader.pages):
                    try:
                        text = page.extract_text()
                        if not text:
                            continue

                        # æŸ¥æ‰¾å¼•ç”¨æ¨¡å¼
                        matches = self._find_citation_patterns(text, cited_info)

                        for match in matches:
                            start = max(0, match['start'] - 100)
                            end = min(len(text), match['end'] + 100)
                            context = text[start:end].strip()
                            context = ' '.join(context.split())

                            contexts.append({
                                'page': page_num + 1,
                                'context': context,
                                'citation_text': match['citation']
                            })
                    except Exception as e:
                        logger.debug(f"å¤„ç†ç¬¬{page_num+1}é¡µå¤±è´¥: {e}")
                        continue

            return contexts

        except ImportError:
            logger.warning("PyPDF2æœªå®‰è£…ï¼Œæ— æ³•æå–PDFå†…å®¹")
            return []
        except Exception as e:
            logger.warning(f"ä½¿ç”¨PyPDF2æå–å¤±è´¥: {e}")
            return []

    def _get_pdf_path(self, paper: Dict) -> Optional[str]:
        """
        è·å–è®ºæ–‡PDFæ–‡ä»¶è·¯å¾„

        ç­–ç•¥:
        1. æ£€æŸ¥paperå¯¹è±¡ä¸­çš„pdf_pathå­—æ®µ
        2. åŸºäºpaper_idåœ¨é»˜è®¤ç›®å½•ä¸­æŸ¥æ‰¾
        3. åŸºäºtitleåœ¨é»˜è®¤ç›®å½•ä¸­æŸ¥æ‰¾
        """
        # ç­–ç•¥1: ä½¿ç”¨paperä¸­çš„pdf_path
        if paper.get('pdf_path') and os.path.exists(paper['pdf_path']):
            return paper['pdf_path']

        # ç­–ç•¥2: åŸºäºpaper_idæŸ¥æ‰¾
        paper_id = paper.get('id', '')
        if paper_id:
            # é»˜è®¤PDFç›®å½•
            pdf_dir = Path('./data/papers')
            if not pdf_dir.exists():
                pdf_dir = Path('/home/lexy/ä¸‹è½½/CLwithRAG/KGdemo/data/papers')

            if pdf_dir.exists():
                # æŸ¥æ‰¾ä»¥paper_idå¼€å¤´çš„PDFæ–‡ä»¶
                for pdf_file in pdf_dir.glob(f'{paper_id}*.pdf'):
                    return str(pdf_file)

        # ç­–ç•¥3: åŸºäºtitleæŸ¥æ‰¾(å¦‚æœpaper_idæŸ¥æ‰¾å¤±è´¥)
        title = paper.get('title', '')
        if title and pdf_dir.exists():
            # å°†æ ‡é¢˜è½¬æ¢ä¸ºå®‰å…¨æ–‡ä»¶åæ ¼å¼
            safe_title = re.sub(r'[^\w\s-]', '', title).strip()
            safe_title = re.sub(r'[\s]+', '_', safe_title)[:50]

            for pdf_file in pdf_dir.glob(f'*{safe_title}*.pdf'):
                return str(pdf_file)

        return None

    def _extract_citation_identifiers(self, paper: Dict) -> Optional[Dict]:
        """
        æå–è®ºæ–‡çš„å¼•ç”¨è¯†åˆ«ä¿¡æ¯

        Returns:
            åŒ…å«è¯†åˆ«ä¿¡æ¯çš„å­—å…¸:
            {
                'authors': ['Smith', 'Jones'],  # ä¸»è¦ä½œè€…å§“æ°
                'year': '2020',
                'first_author': 'Smith',
                'title_keywords': ['deep', 'learning']
            }
        """
        info = {}

        # æå–å¹´ä»½
        year = paper.get('year') or paper.get('publication_year')
        if year:
            info['year'] = str(year)

        # æå–ä½œè€…ä¿¡æ¯
        authors = paper.get('authors', [])
        if authors:
            # æ”¯æŒå¤šç§ä½œè€…æ ¼å¼
            if isinstance(authors, list):
                if authors and isinstance(authors[0], dict):
                    # æ ¼å¼: [{'name': 'John Smith'}, ...]
                    author_names = [a.get('name', '') or a.get('author', '') for a in authors]
                else:
                    # æ ¼å¼: ['John Smith', ...]
                    author_names = authors

                # æå–å§“æ°
                surnames = []
                for name in author_names[:3]:  # åªå–å‰3ä¸ªä½œè€…
                    if name:
                        # æå–å§“æ°(å‡è®¾å§“æ°æ˜¯æœ€åä¸€ä¸ªå•è¯)
                        parts = name.strip().split()
                        if parts:
                            surnames.append(parts[-1])

                if surnames:
                    info['authors'] = surnames
                    info['first_author'] = surnames[0]

        # æå–æ ‡é¢˜å…³é”®è¯
        title = paper.get('title', '')
        if title:
            # æå–æœ‰æ„ä¹‰çš„å•è¯(é•¿åº¦>3)
            words = re.findall(r'\b\w{4,}\b', title.lower())
            info['title_keywords'] = words[:5]  # å–å‰5ä¸ªå…³é”®è¯

        return info if info else None

    def _find_citation_patterns(self, text: str, cited_info: Dict) -> List[Dict]:
        """
        åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾å¼•ç”¨æ¨¡å¼

        æ”¯æŒçš„å¼•ç”¨æ ¼å¼:
        1. [Author, Year] - [Smith, 2020]
        2. (Author, Year) - (Smith, 2020)
        3. Author (Year) - Smith (2020)
        4. [1], [2], etc. - æ•°å­—å¼•ç”¨(ä»…å½“ä¸Šä¸‹æ–‡ä¸­æåˆ°ä½œè€…æ—¶)
        5. Author et al., Year - Smith et al., 2020

        Args:
            text: PDFæ–‡æœ¬å†…å®¹
            cited_info: è¢«å¼•è®ºæ–‡çš„è¯†åˆ«ä¿¡æ¯

        Returns:
            åŒ¹é…åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'start': int, 'end': int, 'citation': str}
        """
        matches = []

        first_author = cited_info.get('first_author', '')
        year = cited_info.get('year', '')

        if not first_author or not year:
            return matches

        # æ„å»ºå¼•ç”¨æ¨¡å¼(ä¸åŒºåˆ†å¤§å°å†™)
        patterns = [
            # [Author, Year] æˆ– [Author et al., Year]
            rf'\[{first_author}(?:\s+et\s+al\.?)?,?\s*{year}\]',
            # (Author, Year) æˆ– (Author et al., Year)
            rf'\({first_author}(?:\s+et\s+al\.?)?,?\s*{year}\)',
            # Author (Year) æˆ– Author et al. (Year)
            rf'{first_author}(?:\s+et\s+al\.)?\s*\({year}\)',
            # Author et al., Year
            rf'{first_author}\s+et\s+al\.,?\s*{year}',
        ]

        for pattern in patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                matches.append({
                    'start': match.start(),
                    'end': match.end(),
                    'citation': match.group(0)
                })

        # æŒ‰ä½ç½®æ’åºå¹¶å»é‡
        matches = sorted(matches, key=lambda x: x['start'])

        # å»é™¤é‡å çš„åŒ¹é…
        unique_matches = []
        last_end = -1
        for match in matches:
            if match['start'] >= last_end:
                unique_matches.append(match)
                last_end = match['end']

        return unique_matches

    def _rule_based_inference(
        self,
        citing_paper: Dict,
        cited_paper: Dict,
        citing_analysis: Dict,
        cited_analysis: Dict
    ) -> CitationRelationship:
        """
        åŸºäºè§„åˆ™çš„æ¨æ–­ï¼ˆå½“æ²¡æœ‰LLMæ—¶ä½¿ç”¨ï¼‰
        """
        # æå–åŸºæœ¬ä¿¡æ¯
        citing_year = citing_paper.get('year', 0)
        cited_year = cited_paper.get('year', 0)
        year_diff = citing_year - cited_year if citing_year > 0 and cited_year > 0 else 0

        # ç®€å•è§„åˆ™
        relationship_type = "Baselines"
        confidence = 0.3
        reasoning = "åŸºäºè§„åˆ™çš„ç®€å•æ¨æ–­"

        # è§„åˆ™1: å¦‚æœæœ‰limitationå’Œproblemï¼Œå¯èƒ½æ˜¯Overcomes
        if cited_analysis.get('limitation') and citing_analysis.get('problem'):
            if self._text_similarity(cited_analysis['limitation'], citing_analysis['problem']) > 0.3:
                relationship_type = "Overcomes"
                confidence = 0.6
                reasoning = "Bçš„é—®é¢˜ä¸Açš„å±€é™æ€§ç›¸å…³"

        # è§„åˆ™2: å¦‚æœæœ‰future_workå’Œproblemï¼Œå¯èƒ½æ˜¯Realizes
        if cited_analysis.get('future_work') and citing_analysis.get('problem'):
            if self._text_similarity(cited_analysis['future_work'], citing_analysis['problem']) > 0.3:
                relationship_type = "Realizes"
                confidence = 0.6
                reasoning = "Bå®ç°äº†Aå»ºè®®çš„æœªæ¥å·¥ä½œ"

        logger.info(f"  è§„åˆ™æ¨æ–­: {relationship_type} (ç½®ä¿¡åº¦: {confidence:.2f})")

        return CitationRelationship(
            citing_id=citing_paper['id'],
            cited_id=cited_paper['id'],
            relationship_type=relationship_type,
            confidence=confidence,
            reasoning=reasoning,
            evidence="",
            match_results=[]
        )

    def _text_similarity(self, text1: str, text2: str) -> float:
        """ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—"""
        if not text1 or not text2:
            return 0.0

        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return intersection / union if union > 0 else 0.0

    def _extract_json_from_response(self, response: str) -> str:
        """
        ä»LLMå“åº”ä¸­æå–JSONå†…å®¹
        å¤„ç†å¯èƒ½åŒ…å«markdownä»£ç å—çš„æƒ…å†µ
        """
        import re

        # å»é™¤é¦–å°¾ç©ºç™½
        response = response.strip()

        # å°è¯•æå–markdownä»£ç å—ä¸­çš„JSON
        # åŒ¹é… ```json ... ``` æˆ– ``` ... ```
        json_block_pattern = r'```(?:json)?\s*\n?(.*?)\n?```'
        match = re.search(json_block_pattern, response, re.DOTALL)

        if match:
            return match.group(1).strip()

        # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œç›´æ¥è¿”å›åŸå“åº”
        return response


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import logging
    import sys
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    use_rule_based = len(sys.argv) > 1 and sys.argv[1] == '--rule-based'

    # åˆ›å»ºæ¨æ–­å™¨ï¼ˆé»˜è®¤ä½¿ç”¨LLMï¼‰
    if use_rule_based:
        print("\nğŸ“ ä½¿ç”¨è§„åˆ™æ–¹æ³•æ¨¡å¼ï¼ˆæ— LLMï¼‰")
        print("æç¤º: ä¸ä½¿ç”¨å‚æ•°åˆ™é»˜è®¤å¯ç”¨LLMæ¨¡å¼\n")
        inferencer = CitationTypeInferencer(llm_client=None)
    else:
        print("\nğŸ”Œ ä½¿ç”¨ LLM Socket Matching æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰")
        print("ä» config/config.yaml åŠ è½½LLMé…ç½®...")
        print("æç¤º: ä½¿ç”¨ --rule-based å‚æ•°åˆ‡æ¢åˆ°è§„åˆ™æ–¹æ³•\n")
        inferencer = CitationTypeInferencer(config_path="config/config.yaml")

    # æµ‹è¯•è®ºæ–‡æ•°æ®
    test_papers = [
        {
            'id': 'W1',
            'title': 'Attention Is All You Need',
            'year': 2017,
            'cited_by_count': 50000,
            'deep_analysis': {
                'problem': 'Existing sequence models are difficult to parallelize',
                'method': 'Proposed Transformer model based entirely on attention mechanisms',
                'limitation': 'Limited to fixed-length sequences, requires large amounts of training data',
                'future_work': 'Explore Transformer applications in other domains like computer vision'
            }
        },
        {
            'id': 'W2',
            'title': 'BERT: Pre-training of Deep Bidirectional Transformers',
            'year': 2018,
            'cited_by_count': 40000,
            'deep_analysis': {
                'problem': 'Existing pre-training models can only model in one direction',
                'method': 'Proposed bidirectional Transformer pre-training method BERT',
                'limitation': 'BERT is computationally expensive for fine-tuning',
                'future_work': 'Investigate more efficient pre-training methods'
            }
        },
        {
            'id': 'W3',
            'title': 'Vision Transformer (ViT)',
            'year': 2020,
            'cited_by_count': 15000,
            'deep_analysis': {
                'problem': 'Applying Transformer to computer vision tasks',
                'method': 'Demonstrated that pure Transformer can work well on image classification',
                'limitation': 'Requires very large datasets to train effectively',
                'future_work': 'Apply to other vision tasks like detection and segmentation'
            }
        }
    ]

    # æµ‹è¯•å¼•ç”¨å…³ç³»
    test_edges = [
        ('W2', 'W1'),  # BERTå¼•ç”¨Transformer (åº”è¯¥æ˜¯ Overcomes æˆ– Baselines)
        ('W3', 'W1'),  # ViTå¼•ç”¨Transformer (åº”è¯¥æ˜¯ Realizes - å®ç°äº†æœªæ¥å·¥ä½œå»ºè®®)
    ]

    # æ¨æ–­å¼•ç”¨ç±»å‹
    print("\n" + "="*80)
    print("Socket Matching å¼•ç”¨å…³ç³»ç±»å‹æ¨æ–­æµ‹è¯•")
    print("="*80)

    typed_edges, statistics = inferencer.infer_edge_types(test_papers, test_edges)

    print("\nå¼•ç”¨å…³ç³»ç±»å‹æ¨æ–­ç»“æœ:")
    print("="*80)
    for citing_id, cited_id, edge_type in typed_edges:
        citing_paper = next(p for p in test_papers if p['id'] == citing_id)
        cited_paper = next(p for p in test_papers if p['id'] == cited_id)
        print(f"\n{citing_paper['title']}")
        print(f"  â†’ {cited_paper['title']}")
        print(f"  å…³ç³»ç±»å‹: {edge_type}")
    print("="*80)
