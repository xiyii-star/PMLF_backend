
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>äº¤äº’å¼è®ºæ–‡å¼•ç”¨çŸ¥è¯†å›¾è°±</title>
            <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
            <style>
                body {
                    margin: 0;
                    padding: 0;
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                    background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
                    height: 100vh;
                    overflow: hidden;
                }
                .container {
                    display: flex;
                    width: 100%;
                    height: 100vh;
                    background: white;
                    overflow: hidden;
                }
                .graph-section {
                    width: 70%;
                    display: flex;
                    flex-direction: column;
                }
                .graph-container {
                    flex: 1;
                    padding: 10px;
                }
                .legend-container {
                    padding: 15px;
                    background: #f8f9fa;
                    border-top: 1px solid #dee2e6;
                    min-height: 180px;
                    max-height: 200px;
                }
                .details-section {
                    width: 30%;
                    background: #f8f9fa;
                    border-left: 1px solid #dee2e6;
                    display: flex;
                    flex-direction: column;
                }
                .details-header {
                    padding: 15px 20px;
                    background: #6c757d;
                    color: white;
                    font-weight: bold;
                    font-size: 18px;
                }
                .details-content {
                    padding: 20px;
                    flex: 1;
                    overflow-y: auto;
                }
                .paper-info {
                    margin-bottom: 15px;
                }
                .paper-info h3 {
                    color: #2c3e50;
                    margin: 0 0 10px 0;
                    font-size: 16px;
                    line-height: 1.4;
                }
                .paper-info p {
                    margin: 5px 0;
                    color: #5a5a5a;
                    font-size: 14px;
                }
                .legend-grid {
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                    gap: 10px;
                }
                .legend-item {
                    display: flex;
                    align-items: center;
                    padding: 8px;
                    background: white;
                    border-radius: 5px;
                    font-size: 12px;
                }
                .legend-color {
                    width: 20px;
                    height: 3px;
                    margin-right: 8px;
                    border-radius: 2px;
                }
                .stats {
                    background: #e9ecef;
                    padding: 15px;
                    border-radius: 8px;
                    margin-bottom: 15px;
                }
                .stat-item {
                    display: flex;
                    justify-content: space-between;
                    margin: 5px 0;
                    font-size: 14px;
                }
                .placeholder {
                    text-align: center;
                    color: #6c757d;
                    font-style: italic;
                    margin-top: 50px;
                }
                .title {
                    text-align: center;
                    color: #2c3e50;
                    margin-bottom: 20px;
                    font-size: 24px;
                    font-weight: bold;
                }
                /* æ ‡ç­¾é¡µæ ·å¼ */
                .tabs {
                    display: flex;
                    background: #dee2e6;
                    border-bottom: 2px solid #6c757d;
                }
                .tab {
                    flex: 1;
                    padding: 12px 10px;
                    text-align: center;
                    cursor: pointer;
                    border: none;
                    background: #dee2e6;
                    color: #495057;
                    font-size: 14px;
                    font-weight: 500;
                    transition: all 0.3s;
                }
                .tab:hover {
                    background: #c4c8cc;
                }
                .tab.active {
                    background: #6c757d;
                    color: white;
                }
                .tab-content {
                    display: none;
                    padding: 20px;
                    overflow-y: auto;
                    height: calc(100vh - 130px);
                }
                .tab-content.active {
                    display: block;
                }
                .epoch-card {
                    background: white;
                    border-left: 4px solid #3498DB;
                    padding: 15px;
                    margin-bottom: 15px;
                    border-radius: 5px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }
                .epoch-card h4 {
                    margin: 0 0 10px 0;
                    color: #2c3e50;
                    font-size: 15px;
                }
                .epoch-card p {
                    margin: 5px 0;
                    font-size: 13px;
                    color: #555;
                }
                .idea-card {
                    background: white;
                    border-left: 4px solid #2ECC71;
                    padding: 15px;
                    margin-bottom: 15px;
                    border-radius: 5px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }
                .idea-card h4 {
                    margin: 0 0 10px 0;
                    color: #2c3e50;
                    font-size: 15px;
                }
                .idea-card .status-badge {
                    display: inline-block;
                    padding: 3px 8px;
                    border-radius: 3px;
                    font-size: 11px;
                    font-weight: bold;
                    margin-left: 8px;
                }
                .status-success {
                    background: #d4edda;
                    color: #155724;
                }
                .status-incompatible {
                    background: #f8d7da;
                    color: #721c24;
                }
                .pivot-paper {
                    background: #fff3cd;
                    padding: 10px;
                    margin: 8px 0;
                    border-radius: 4px;
                    font-size: 12px;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <!-- å·¦ä¾§å›¾è°±éƒ¨åˆ† (70%) -->
                <div class="graph-section">
                    <div class="title">äº¤äº’å¼è®ºæ–‡å¼•ç”¨çŸ¥è¯†å›¾è°±</div>
                    <div class="graph-container">
                        <div id="graph" style="width:100%; height:100%;"></div>
                    </div>
                    <div class="legend-container">
                        <h4 style="margin-top:0; color:#2c3e50;">ğŸ”Œ Socket Matching å¼•ç”¨å…³ç³»ç±»å‹ï¼ˆ6ç§æ ¸å¿ƒç±»å‹ï¼‰</h4>
                        <div class="legend-grid">
                            <!-- Socket Matching æ ¸å¿ƒç±»å‹ï¼ˆ6ç§ï¼‰-->
                            <div class="legend-item" style="border-left: 3px solid #E74C3C;">
                                <div class="legend-color" style="background-color:#E74C3C; height:3px;"></div>
                                <span><strong>Overcomes</strong> - æ”»å…‹/ä¼˜åŒ–ï¼ˆçºµå‘æ·±åŒ–ï¼‰</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #9B59B6;">
                                <div class="legend-color" style="background-color:#9B59B6; height:3px;"></div>
                                <span><strong>Realizes</strong> - å®ç°æ„¿æ™¯ï¼ˆç§‘ç ”ä¼ æ‰¿ï¼‰</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #2ECC71;">
                                <div class="legend-color" style="background-color:#2ECC71; height:2px;"></div>
                                <span><strong>Extends</strong> - æ–¹æ³•æ‰©å±•ï¼ˆå¾®åˆ›æ–°ï¼‰</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #E67E22;">
                                <div class="legend-color" style="background-color:#E67E22; border: 2px dotted #E67E22; height:1px;"></div>
                                <span><strong>Alternative</strong> - å¦è¾Ÿè¹Šå¾„ï¼ˆé¢ è¦†åˆ›æ–°ï¼‰</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #3498DB;">
                                <div class="legend-color" style="background-color:#3498DB; border: 2px dashed #3498DB; height:1px;"></div>
                                <span><strong>Adapts_to</strong> - è¿ç§»/åº”ç”¨ï¼ˆæ¨ªå‘æ‰©æ•£ï¼‰</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #95A5A6;">
                                <div class="legend-color" style="background-color:#95A5A6; height:1px;"></div>
                                <span><strong>Baselines</strong> - åŸºçº¿å¯¹æ¯”ï¼ˆèƒŒæ™¯å™ªéŸ³ï¼‰</span>
                            </div>
                        </div>
                        <p style="margin-top:10px; font-size:11px; color:#666;">
                            ğŸ’¡ <strong>é€»è¾‘å¯¹æ¥çŸ©é˜µ (4ä¸ªMatch â†’ 6ç§ç±»å‹)</strong>: Match1(Limitationâ†’Problem) â†’ Overcomes | Match2(FutureWorkâ†’Problem) â†’ Realizes | Match3(Methodâ†’Method) â†’ Extends/Alternative | Match4(Problemè·¨åŸŸ) â†’ Adapts_to | æ— åŒ¹é… â†’ Baselines
                        </p>
                    </div>
                </div>

                <!-- å³ä¾§è¯¦æƒ…éƒ¨åˆ† (30%) -->
                <div class="details-section">
                    <!-- æ ‡ç­¾é¡µå¯¼èˆª -->
                    <div class="tabs">
                        <div class="tab active" onclick="switchTab(event, 'paper-tab')">ğŸ“„ è®ºæ–‡è¯¦æƒ…</div>
                        <div class="tab" onclick="switchTab(event, 'survey-tab')">ğŸ“ æ·±åº¦ç»¼è¿°</div>
                        <div class="tab" onclick="switchTab(event, 'ideas-tab')">ğŸ’¡ ç§‘ç ”åˆ›æ„</div>
                    </div>

                    <!-- è®ºæ–‡è¯¦æƒ…æ ‡ç­¾é¡µ -->
                    <div id="paper-tab" class="tab-content active">
                        <div class="stats">
                            <h4 style="margin-top:0;">å›¾è°±ç»Ÿè®¡</h4>
                            <div class="stat-item">
                                <span>è®ºæ–‡æ€»æ•°:</span>
                                <span>86</span>
                            </div>
                            <div class="stat-item">
                                <span>å¼•ç”¨å…³ç³»:</span>
                                <span>99</span>
                            </div>
                            <div class="stat-item">
                                <span>æ—¶é—´è·¨åº¦:</span>
                                <span>1972 - 2026</span>
                            </div>
                        </div>
                        <div class="placeholder">
                            ğŸ‘† ç‚¹å‡»å›¾è°±ä¸­çš„èŠ‚ç‚¹æŸ¥çœ‹è®ºæ–‡è¯¦ç»†ä¿¡æ¯
                        </div>
                    </div>

                    <!-- æ·±åº¦ç»¼è¿°æ ‡ç­¾é¡µ -->
                    <div id="survey-tab" class="tab-content"></div>

                    <!-- ç§‘ç ”åˆ›æ„æ ‡ç­¾é¡µ -->
                    <div id="ideas-tab" class="tab-content"></div>
                </div>
            </div>

            <script>
                // ========== æ•°æ®åˆå§‹åŒ– ==========
                const nodesData = [
  {
    "id": "W4205802268",
    "x": 2022.0769381610855,
    "y": 1.2988407506896755,
    "title": "The Routledge Handbook of Translation and Methodology",
    "authors": [
      "Federico Zanettin",
      "Christopher Rundle"
    ],
    "first_author": "Federico Zanettin",
    "first_author_surname": "Zanettin",
    "year": 2022,
    "cited_by_count": 41,
    "venue": "",
    "size": 20.738478471940066,
    "color": "hsl(14, 70%, 60%)",
    "label": "Zanettin ,2022",
    "rag_problem": "Interpreter education often lacks a balance between practice-oriented skills and the acquisition of academic knowledge, leading to insufficient theoretical grounding for professional interpreters.",
    "rag_method": "The authors propose an educational model that integrates practice-oriented training with academic knowledge acquisition and the ability to theorize.\n\n**Explanation:** By combining practical training with academic knowledge and theoretical skills, the proposed model ensures that interpreters are not only proficient in their practical tasks but also understand the underlying theories and methodologies. This dual focus equips interpreters with a deeper understanding of their profession, enabling them to adapt to diverse challenges and contribute to the field's development.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate and address problematic aspects in interpreter education to develop more effective solutions, as these issues are highlighted to stimulate further research or action.\n- Conduct research on the integration of practice-oriented teaching with academic knowledge acquisition and theoretical skills in interpreter education to enhance training methodologies.\n- Explore the two particularly relevant issues in interpreter education mentioned in the study to provide deeper insights and improvements in these areas.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3122397014",
    "x": 2020.8811716808302,
    "y": 8.673818462168533,
    "title": "Spark NLP: Natural Language Understanding at Scale",
    "authors": [
      "Veysel Kocaman",
      "David Talby"
    ],
    "first_author": "Veysel Kocaman",
    "first_author_surname": "Kocaman",
    "year": 2021,
    "cited_by_count": 6,
    "venue": "",
    "size": 13.864916279404547,
    "color": "hsl(21, 70%, 60%)",
    "label": "Kocaman ,2021",
    "rag_problem": "Scaling natural language processing (NLP) tasks in distributed environments while maintaining performance and accuracy is challenging.",
    "rag_method": "Spark NLP is built on top of Apache Spark ML, providing performant and accurate NLP annotations that integrate seamlessly with machine learning pipelines in distributed environments.\n\n**Explanation:** By leveraging Apache Spark ML's distributed computing capabilities, Spark NLP enables efficient scaling of NLP tasks across clusters. Its design ensures that NLP annotations are both accurate and performant, addressing the challenge of maintaining quality while scaling computations.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4360845368",
    "x": 2022.5269021421905,
    "y": -2.34538035830912,
    "title": "Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Training",
    "authors": [
      "Laura RamÃ­rez-Polo",
      "Chelo Vargas-Sierra"
    ],
    "first_author": "Laura RamÃ­rez-Polo",
    "first_author_surname": "RamÃ­rez-Polo",
    "year": 2023,
    "cited_by_count": 24,
    "venue": "",
    "size": 18.7482775184176,
    "color": "hsl(7, 70%, 60%)",
    "label": "RamÃ­rez-Polo ,2023",
    "rag_problem": "Ethical concerns related to the use of translation technology and artificial intelligence systems are not adequately addressed in current translator training programs.",
    "rag_method": "Proposal for integrating ethical competence into translator training models and teaching programs, emphasizing the ethical implications of technology use.\n\n**Explanation:** By incorporating ethical competence into training programs, translators will be equipped to critically evaluate and responsibly use technology and AI systems. This ensures that ethical considerations are embedded in their professional practice, addressing the gap in current pedagogical models that overlook these concerns.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate how ethical concerns regarding translation technology and AI systems can be systematically integrated into pedagogical models for translator training. This could involve developing specific curricula or frameworks addressing ethical competence.\n- Explore the effectiveness of current teaching programs in addressing the intersection of technology and ethics in translation practice. Future studies could assess whether these programs adequately prepare translators for real-world ethical challenges.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4317823603",
    "x": 2022.4542514814068,
    "y": 4.316536731971434,
    "title": "Natural Language Processing for Policymaking",
    "authors": [
      "Zhijing Jin",
      "Rada Mihalcea"
    ],
    "first_author": "Zhijing Jin",
    "first_author_surname": "Jin",
    "year": 2022,
    "cited_by_count": 12,
    "venue": "",
    "size": 16.239679506095523,
    "color": "hsl(14, 70%, 60%)",
    "label": "Jin ,2022",
    "rag_problem": "Policymakers face challenges in analyzing and interpreting large volumes of textual data, such as policy documents, public feedback, and legislative texts, which are critical for informed decision-making.",
    "rag_method": "The application of Natural Language Processing (NLP) techniques to automate the analysis, extraction, and summarization of relevant information from textual data.\n\n**Explanation:** NLP techniques enable the processing of unstructured textual data by identifying key themes, extracting actionable insights, and summarizing complex documents efficiently. This reduces the cognitive and time burden on policymakers, allowing them to focus on decision-making rather than manual data analysis.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4362571217",
    "x": 2022.5263786394175,
    "y": 3.208334981946828,
    "title": "Question Classification in Albanian Through Deep Learning Approaches",
    "authors": [
      "Evis Trandafili",
      "Nelda Kote",
      "Gjergj Plepi"
    ],
    "first_author": "Evis Trandafili",
    "first_author_surname": "Trandafili",
    "year": 2023,
    "cited_by_count": 4,
    "venue": "",
    "size": 12.5741387592088,
    "color": "hsl(7, 70%, 60%)",
    "label": "Trandafili ,2023",
    "rag_problem": "There is a lack of research and effective methods for question classification in the Albanian language, which is a complex Indo-European language.",
    "rag_method": "The authors propose using deep learning approaches to classify questions in Albanian, leveraging advanced neural network models to handle the complexity of the language.\n\n**Explanation:** Deep learning approaches, such as neural networks, are capable of learning complex patterns and representations in data. By applying these methods to Albanian question classification, the system can effectively analyze linguistic structures and semantic nuances specific to Albanian, addressing the challenge posed by its complexity and lack of prior research. This enables accurate determination of question types, which is crucial for intelligent conversation systems.",
    "rag_limitation": "- The method focuses solely on the Albanian language, which limits its generalizability to other languages, especially those with different linguistic structures.\n- There is limited prior research on question classification for Albanian, which may restrict the availability of resources and benchmarks to evaluate the proposed approach comprehensively.",
    "rag_future_work": "- Expand research to other complex Indo-European languages: Since the paper focuses on Albanian, future work could involve applying similar deep learning approaches to other under-researched languages within the same linguistic family.\n- Enhance dataset size and diversity: Future work could focus on creating larger and more diverse datasets for Albanian question classification to improve model robustness and generalization.\n- Explore advanced deep learning architectures: Investigating more sophisticated or specialized neural network models could further improve classification accuracy for Albanian questions.\n- Integrate question classification into broader conversational AI systems: Future directions might include embedding the classification models into comprehensive Albanian-language Question Answering systems.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4386328785",
    "x": 2022.8600353929933,
    "y": 11.315376880043987,
    "title": "Operational performance improvement in manual assembly lines: a case study in Denmark andÂ conceptual model for quick andÂ long-term wins",
    "authors": [
      "Diego Augusto de JesÃºs Pacheco",
      "Thomas Schougaard"
    ],
    "first_author": "Diego Augusto de JesÃºs Pacheco",
    "first_author_surname": "Pacheco",
    "year": 2023,
    "cited_by_count": 3,
    "venue": "",
    "size": 11.718113659767154,
    "color": "hsl(7, 70%, 60%)",
    "label": "Pacheco ,2023",
    "rag_problem": "Manual assembly lines face production levelling issues when higher productivity is urgently required to meet market demands.",
    "rag_method": "Development of a conceptual model integrating quick wins and long-term strategies to address production levelling problems.\n\n**Explanation:** The conceptual model provides a structured approach to identify inefficiencies in manual assembly processes and implement targeted improvements. Quick wins focus on immediate, low-cost changes to boost productivity, while long-term strategies ensure sustained operational performance by addressing deeper systemic issues. This dual approach ensures both immediate relief to meet urgent demands and a foundation for sustained productivity growth.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate the scalability of the proposed conceptual model to other industries or assembly line settings with varying levels of automation. This would help determine the model's adaptability and broader applicability.\n- Explore advanced data analytics and machine learning techniques to enhance the identification and resolution of production leveling problems in manual assembly lines. This could lead to more precise and efficient operational improvements.\n- Conduct longitudinal studies to assess the long-term impact of implementing the proposed quick and long-term win strategies on operational performance. This would provide deeper insights into their sustainability and effectiveness over time.\n- Develop and test additional tools or frameworks to better integrate qualitative and quantitative data in addressing productivity challenges, ensuring a more comprehensive approach to operational performance improvement.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2041578073",
    "x": 2014.0531685476747,
    "y": -4.151516411884867,
    "title": "The design and evaluation of a Statistical Machine Translation syllabus for translation students",
    "authors": [
      "Stephen Doherty",
      "Dorothy Kenny"
    ],
    "first_author": "Stephen Doherty",
    "first_author_surname": "Doherty",
    "year": 2014,
    "cited_by_count": 117,
    "venue": "",
    "size": 24.701339008056326,
    "color": "hsl(71, 70%, 60%)",
    "label": "Doherty ,2014",
    "rag_problem": "There is a lack of reflection and structured integration of Statistical Machine Translation (SMT) into translation studies curricula, despite its growing importance in the field.",
    "rag_method": "The design and implementation of a holistic SMT syllabus specifically tailored for translation students.\n\n**Explanation:** By creating a dedicated SMT syllabus, the authors address the gap in translation studies curricula by providing students with structured knowledge and practical skills related to SMT. This ensures that students are better prepared to understand and utilize SMT technology in professional translation contexts, aligning their education with industry advancements.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate how Statistical Machine Translation (SMT) can be more effectively integrated into translation studies curricula, addressing the current lack of reflection on this integration.\n- Develop and evaluate a holistic SMT syllabus further, building on the priorities and aspirations outlined in the companion paper.\n- Explore practical methods for teaching SMT to translation students, focusing on aligning technological advancements with pedagogical approaches.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2165612380",
    "x": 1975.3015256035214,
    "y": -3.9727550973429624,
    "title": "A vector space model for automatic indexing",
    "authors": [
      "Gerard Salton",
      "Anita M.-Y. Wong",
      "Chulâ€Su Yang"
    ],
    "first_author": "Gerard Salton",
    "first_author_surname": "Salton",
    "year": 1975,
    "cited_by_count": 7329,
    "venue": "",
    "size": 60,
    "color": "hsl(240, 70%, 60%)",
    "label": "Salton ,1975",
    "rag_problem": "In document retrieval systems, stored entities (documents) are often compared with each other or with incoming search requests, but traditional indexing methods fail to maximize the separation between entities in the indexing space, leading to suboptimal retrieval performance.",
    "rag_method": "The authors propose a vector space model for automatic indexing, where entities are represented in a high-dimensional space designed to maximize the distance between them.\n\n**Explanation:** By representing documents and search requests as vectors in a high-dimensional space, the model ensures that entities are positioned as far apart as possible. This reduces the density of the object space and minimizes overlap between entities, improving the precision and efficiency of retrieval by making it easier to distinguish between documents and match them to relevant search requests.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Explore methods to optimize the density of the object space in the vector space model, as retrieval performance may correlate inversely with space density. This could involve developing new algorithms or techniques to better distribute entities in the indexing space.\n- Investigate the relationship between indexing system properties and retrieval performance in various environments. This includes testing the model in different document retrieval or pattern matching scenarios to validate its effectiveness and generalizability.\n- Develop techniques to ensure that entities in the indexing space are positioned as far apart as possible. This might involve studying alternative distance metrics or dimensionality reduction methods to improve the separation of entities.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3082051346",
    "x": 2019.511657997645,
    "y": 3.7999274265502216,
    "title": "Publishing volumes in major databases related to Covid-19",
    "authors": [
      "Jaime A. Teixeira da Silva",
      "Panagiotis Tsigaris",
      "Mohammadamin Erfanmanesh"
    ],
    "first_author": "Jaime A. Teixeira da Silva",
    "first_author_surname": "Silva",
    "year": 2020,
    "cited_by_count": 150,
    "venue": "",
    "size": 25.647329559563993,
    "color": "hsl(28, 70%, 60%)",
    "label": "Silva ,2020",
    "rag_problem": "The unprecedented volume of Covid-19-related research papers makes it challenging to track, analyze, and synthesize the vast amount of information effectively.",
    "rag_method": "The authors assess and analyze data from major scientific databases, specifically Clarivate Analytics' Web of Science and Elsevier's Scopus, to provide insights into the publishing volumes of Covid-19-related research.\n\n**Explanation:** By focusing on data from established and comprehensive databases that do not include preprints, the authors provide a structured and reliable analysis of the publishing trends. This helps researchers and policymakers understand the scope and scale of Covid-19-related research, enabling more effective navigation and prioritization of the vast body of literature.",
    "rag_limitation": "- The method relies solely on data from Clarivate Analytics' Web of Science and Elsevier's Scopus, which do not index preprints, potentially omitting a significant portion of Covid-19-related research published in preprint repositories.\n- The analysis does not account for the quality or impact of the published papers, focusing only on publication volumes, which may limit the depth of insights into the scientific contributions.",
    "rag_future_work": "- Investigate the inclusion of preprint databases: Future work could focus on analyzing publishing volumes in preprint repositories, as the current study only considers indexed databases like Web of Science and Scopus. This would provide a more comprehensive understanding of Covid-19 research dissemination.\n- Explore trends over a longer timeline: The study could be extended to examine publishing trends over a longer period to assess how research output evolves as the pandemic progresses or subsides.\n- Analyze the impact of publications on policy and practice: Future research could evaluate how the published Covid-19 studies have influenced public health policies, clinical practices, or vaccine/drug development efforts.\n- Compare publishing patterns across disciplines: Further work could explore how different scientific disciplines have contributed to Covid-19 research, identifying gaps or areas of overrepresentation.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3082307550",
    "x": 2020.308396494275,
    "y": 14.738214361098237,
    "title": "An overview of literature on COVID-19, MERS and SARS: Using text mining and latent Dirichlet allocation",
    "authors": [
      "Xian Cheng",
      "Qiang Cao",
      "Stephen Shaoyi Liao"
    ],
    "first_author": "Xian Cheng",
    "first_author_surname": "Cheng",
    "year": 2020,
    "cited_by_count": 68,
    "venue": "",
    "size": 22.64291367730685,
    "color": "hsl(28, 70%, 60%)",
    "label": "Cheng ,2020",
    "rag_problem": "The overwhelming volume of literature on COVID-19, MERS, and SARS makes it challenging for health and medical researchers to efficiently extract relevant insights and identify key trends.",
    "rag_method": "The authors propose using text mining techniques combined with latent Dirichlet allocation (LDA) to systematically analyze and summarize the vast body of literature.\n\n**Explanation:** Text mining techniques enable automated processing and extraction of information from large datasets, while latent Dirichlet allocation (LDA) is a topic modeling algorithm that identifies latent topics within text data. By applying these methods, the authors can categorize and summarize the literature, making it easier for researchers to identify patterns, trends, and relevant insights without manually reviewing each document. This reduces the cognitive burden and enhances the efficiency of literature review processes.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Explore the application of advanced text mining techniques beyond latent Dirichlet allocation to improve the analysis of large-scale health-related literature, enabling more precise topic modeling and insights.\n- Investigate interdisciplinary collaborations between information science specialists and health professionals to develop tailored computational tools for pandemic response and preparedness.\n- Expand the scope of analysis to include emerging infectious diseases and their literature, ensuring adaptability to future global health crises.\n- Conduct comparative studies on the effectiveness of different text mining methodologies in synthesizing information from diverse datasets related to COVID-19, MERS, and SARS.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2118020653",
    "x": 2001.7294856575668,
    "y": 0.397409662596665,
    "title": "Machine learning in automated text categorization",
    "authors": [
      "Fabrizio Sebastiani"
    ],
    "first_author": "Fabrizio Sebastiani",
    "first_author_surname": "Sebastiani",
    "year": 2002,
    "cited_by_count": 7820,
    "venue": "",
    "size": 60,
    "color": "hsl(155, 70%, 60%)",
    "label": "Sebastiani ,2002",
    "rag_problem": "The need to efficiently and accurately categorize large volumes of digital text into predefined categories due to the increasing availability of documents in digital form.",
    "rag_method": "Using machine learning techniques to automatically build classifiers through inductive learning from preclassified documents.\n\n**Explanation:** Machine learning provides a scalable and automated approach to text categorization by learning the characteristics of predefined categories from labeled training data. This eliminates the need for manual rule creation, which is labor-intensive and less adaptable to large datasets. The inductive process ensures that the classifier can generalize from examples, enabling accurate categorization of unseen texts.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W168564468",
    "x": 2010.03257200436,
    "y": -6.191913980734803,
    "title": "Software Framework for Topic Modelling with Large Corpora",
    "authors": [
      "Radim Å˜ehÅ¯Å™ek",
      "Petr Sojka"
    ],
    "first_author": "Radim Å˜ehÅ¯Å™ek",
    "first_author_surname": "Å˜ehÅ¯Å™ek",
    "year": 2010,
    "cited_by_count": 3795,
    "venue": "",
    "size": 52.273221598033814,
    "color": "hsl(99, 70%, 60%)",
    "label": "Å˜ehÅ¯Å™ek ,2010",
    "rag_problem": "Existing implementations of popular topic modeling algorithms struggle with scalability and ease of use when applied to large corpora due to memory limitations.",
    "rag_method": "A Natural Language Processing software framework based on document streaming, which processes corpora document by document in a memory-independent fashion.\n\n**Explanation:** The document streaming approach avoids loading the entire corpus into memory at once, instead processing one document at a time. This significantly reduces memory usage, enabling the framework to handle large corpora efficiently. Additionally, the framework is designed to be user-friendly, addressing the ease-of-use challenge.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Explore and implement more scalable algorithms to handle even larger corpora efficiently, addressing the current scalability limitations of popular topic modeling methods.\n- Investigate enhancements to the document streaming approach to improve memory independence and processing speed for extremely large datasets.\n- Develop user-friendly interfaces and tools to make the framework more accessible to non-expert users in Natural Language Processing.\n- Conduct experiments to benchmark the framework against other existing solutions, focusing on performance, scalability, and usability metrics.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W1662133657",
    "x": 2010.5874724998098,
    "y": -3.0876220235427603,
    "title": "From Frequency to Meaning: Vector Space Models of Semantics",
    "authors": [
      "Peter D. Turney",
      "Patrick Pantel"
    ],
    "first_author": "Peter D. Turney",
    "first_author_surname": "Turney",
    "year": 2010,
    "cited_by_count": 2831,
    "venue": "",
    "size": 46.11626857544874,
    "color": "hsl(99, 70%, 60%)",
    "label": "Turney ,2010",
    "rag_problem": "Computers lack an understanding of the meaning of human language, which limits their ability to process, analyze, and interact with text meaningfully.",
    "rag_method": "Vector Space Models (VSMs) of semantics are introduced as a method to represent and process the meaning of text computationally.\n\n**Explanation:** VSMs represent words and phrases as vectors in a high-dimensional space, where semantic relationships between words are encoded as geometric relationships (e.g., distance or angle) in the vector space. This allows computers to perform operations such as similarity measurement, clustering, and analogy detection, which are essential for understanding and processing text meaningfully.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2045108252",
    "x": 2003.0483931382726,
    "y": -0.43582389225765705,
    "title": "Visualizing knowledge domains",
    "authors": [
      "Katy BÃ¶rner",
      "Chaomei Chen",
      "Kevin W. Boyack"
    ],
    "first_author": "Katy BÃ¶rner",
    "first_author_surname": "BÃ¶rner",
    "year": 2003,
    "cited_by_count": 1736,
    "venue": "",
    "size": 43.77223183231149,
    "color": "hsl(148, 70%, 60%)",
    "label": "BÃ¶rner ,2003",
    "rag_problem": "æœªæ‰¾åˆ°æ˜ç¡®çš„ç ”ç©¶é—®é¢˜æè¿°",
    "rag_method": "æœªæ‰¾åˆ°æ˜ç¡®çš„æ–¹æ³•æè¿°",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2301363727",
    "x": 2003.6158014566436,
    "y": -0.31461688671369903,
    "title": "EXPERT SYSTEMS WITH APPLICATIONS",
    "authors": [
      "Short Communication",
      "Been-chian Chien A",
      "Jung Yi Lin A"
    ],
    "first_author": "Short Communication",
    "first_author_surname": "Communication",
    "year": 2004,
    "cited_by_count": 1660,
    "venue": "",
    "size": 43.55769354077087,
    "color": "hsl(141, 70%, 60%)",
    "label": "Communication ,2004",
    "rag_problem": "æœªæ‰¾åˆ°æ˜ç¡®çš„ç ”ç©¶é—®é¢˜æè¿°",
    "rag_method": "æœªæ‰¾åˆ°æ˜ç¡®çš„æ–¹æ³•æè¿°",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2075006521",
    "x": 1972.8982811185224,
    "y": -3.98562042747123,
    "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
    "authors": [
      "G. Salton",
      "Chulâ€Su Yang"
    ],
    "first_author": "G. Salton",
    "first_author_surname": "Salton",
    "year": 1973,
    "cited_by_count": 571,
    "venue": "",
    "size": 38.44577304864422,
    "color": "hsl(240, 70%, 60%)",
    "label": "Salton ,1973",
    "rag_problem": "Standard theories for specifying term values (or weights) in automatic indexing are inadequate for effectively assigning weights to index terms.",
    "rag_method": "New techniques are introduced for assigning weights to index terms based on the characteristics of individual document collections.\n\n**Explanation:** The inadequacy of standard theories stems from their inability to account for the unique properties and variations in different document collections. By introducing new techniques tailored to the characteristics of individual collections, the assignment of term weights becomes more context-sensitive and accurate, improving the effectiveness of automatic indexing.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate more robust methods for assigning term weights tailored to diverse document collections, addressing the inadequacies of current standard theories.\n- Explore the application and evaluation of the newly proposed techniques across larger and more varied datasets to validate their effectiveness.\n- Develop adaptive indexing systems that dynamically adjust term values based on evolving characteristics of document collections.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2144211451",
    "x": 1972.4099771282768,
    "y": -0.16014121017569694,
    "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
    "authors": [
      "Karen SpÃ¤rck Jones"
    ],
    "first_author": "Karen SpÃ¤rck Jones",
    "first_author_surname": "Jones",
    "year": 1972,
    "cited_by_count": 4313,
    "venue": "",
    "size": 52.947959936160245,
    "color": "hsl(240, 70%, 60%)",
    "label": "Jones ,1972",
    "rag_problem": "Term specificity in information retrieval is traditionally interpreted based on term meaning, which does not adequately account for statistical variations in term usage and their impact on retrieval performance.",
    "rag_method": "The authors propose interpreting term specificity statistically, as a function of term usage frequency rather than term meaning, and suggest weighting terms based on their statistical specificity.\n\n**Explanation:** By focusing on statistical term usage rather than semantic meaning, the proposed approach captures the actual behavior of terms in a corpus, allowing frequently occurring terms to be appropriately weighted for better retrieval performance. This statistical interpretation ensures that terms contributing significantly to document relevance are not overlooked, improving overall retrieval effectiveness.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Explore more comprehensive statistical models for interpreting term specificity, focusing on additional factors beyond term frequency to enhance retrieval performance.\n- Investigate the relationship between term specificity and term meaning to determine how semantic aspects could complement statistical interpretations in retrieval systems.\n- Conduct experiments with larger and more diverse test collections to validate the findings and assess the generalizability of the proposed approach.\n- Develop advanced term weighting schemes that integrate the proposed statistical interpretation of term specificity to optimize retrieval effectiveness.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W1908696901",
    "x": 1974.6637789581023,
    "y": 3.580583507004197,
    "title": "A Theory of Indexing",
    "authors": [
      "Gerard Salton"
    ],
    "first_author": "Gerard Salton",
    "first_author_surname": "Salton",
    "year": 1975,
    "cited_by_count": 120,
    "venue": "",
    "size": 24.7976505461054,
    "color": "hsl(240, 70%, 60%)",
    "label": "Salton ,1975",
    "rag_problem": "The content of the paper is insufficient to identify a specific problem as no detailed information or context is provided.",
    "rag_method": "No solution can be determined due to the lack of substantive content in the provided text.\n\n**Explanation:** Without additional content or context from the paper, it is impossible to establish a causal relationship between a problem and a solution.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2568360633",
    "x": 1972.903215682291,
    "y": 4.06743796762805,
    "title": "Contribution to the Theory of Indexing",
    "authors": [
      "Gerard Salton",
      "Chulâ€Su Yang",
      "C. Yu"
    ],
    "first_author": "Gerard Salton",
    "first_author_surname": "Salton",
    "year": 1973,
    "cited_by_count": 17,
    "venue": "",
    "size": 17.48806755518746,
    "color": "hsl(240, 70%, 60%)",
    "label": "Salton ,1973",
    "rag_problem": "The effectiveness of indexing terms in stored documents and user queries is inconsistent, with terms of very high or very low frequency being less useful for retrieval purposes.",
    "rag_method": "Focus on terms with medium frequency and skewed frequency distributions across the document collection to improve indexing vocabulary.\n\n**Explanation:** Terms with medium frequency and skewed distributions are more likely to capture meaningful distinctions between documents and queries. High-frequency terms tend to be too generic and lack specificity, while low-frequency terms are often too rare to contribute significantly. By targeting medium-frequency, skewed terms, the indexing vocabulary becomes more representative and effective for retrieval tasks.",
    "rag_limitation": "- The method relies heavily on medium frequency terms and skewed frequency distributions, which may limit its effectiveness in handling terms with very high or very low document frequency.\n- The approach may struggle to fully incorporate low-frequency terms into the indexing vocabulary, potentially reducing its ability to capture rare but significant terms.",
    "rag_future_work": "- Investigate methods to group low-frequency terms effectively to enhance the indexing vocabulary, as these terms are currently less useful for indexing.\n- Explore the impact of skewed frequency distributions on term usefulness in greater detail to refine the criteria for selecting optimal indexing terms.\n- Develop strategies to address the limitations of terms with very high or very low document frequency, aiming to improve their contribution to indexing systems.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3181361218",
    "x": 2021.7204185417697,
    "y": -7.9699918142863195,
    "title": "Neural Natural Language Processing for unstructured data in electronic health records: A review",
    "authors": [
      "Irene Li",
      "Jessica Pan",
      "Jeremy Goldwasser"
    ],
    "first_author": "Irene Li",
    "first_author_surname": "Li",
    "year": 2022,
    "cited_by_count": 200,
    "venue": "",
    "size": 26.744581255221615,
    "color": "hsl(14, 70%, 60%)",
    "label": "Li ,2022",
    "rag_problem": "Unstructured data in electronic health records (EHRs) is difficult to process and analyze due to its lack of standardization and complexity, hindering effective use in clinical decision-making and research.",
    "rag_method": "Neural Natural Language Processing (NLP) techniques are applied to process and analyze unstructured data in EHRs, leveraging deep learning models to extract meaningful insights and patterns.\n\n**Explanation:** Neural NLP techniques, such as transformer-based models and recurrent neural networks, are capable of understanding and processing complex language structures present in unstructured EHR data. These models can identify relationships, extract key medical concepts, and standardize information, making the data usable for clinical applications and research. By automating the analysis of unstructured data, these techniques address the challenge of manual processing and improve the efficiency and accuracy of data utilization.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3092399442",
    "x": 2021.1433814117615,
    "y": -11.44329791071519,
    "title": "Publication patternsâ€™ changes due to the COVID-19 pandemic: a longitudinal and short-term scientometric analysis",
    "authors": [
      "Shir Aviv-Reuven",
      "Ariel Rosenfeld"
    ],
    "first_author": "Shir Aviv-Reuven",
    "first_author_surname": "Aviv-Reuven",
    "year": 2021,
    "cited_by_count": 180,
    "venue": "",
    "size": 26.342516429122846,
    "color": "hsl(21, 70%, 60%)",
    "label": "Aviv-Reuven ,2021",
    "rag_problem": "The COVID-19 pandemic has caused significant disruptions and changes in biomedical publishing patterns, including increased publication volume and altered timelines for peer-reviewed journal acceptance.",
    "rag_method": "A longitudinal and short-term scientometric analysis was conducted to systematically study and quantify the changes in biomedical publishing patterns during the pandemic.\n\n**Explanation:** By employing scientometric analysis, the authors were able to measure and track key metrics such as publication volume, time to acceptance, and the use of preprint servers. This systematic approach provides insights into how the pandemic has impacted scholarly communication and helps identify trends and inefficiencies in the publishing process during a global health crisis.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4229045617",
    "x": 2022.36056262673,
    "y": -4.78189693887723,
    "title": "Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases",
    "authors": [
      "Michael Gusenbauer"
    ],
    "first_author": "Michael Gusenbauer",
    "first_author_surname": "Gusenbauer",
    "year": 2022,
    "cited_by_count": 145,
    "venue": "",
    "size": 25.518152098954577,
    "color": "hsl(14, 70%, 60%)",
    "label": "Gusenbauer ,2022",
    "rag_problem": "Researchers struggle to identify which bibliographic databases provide the most comprehensive coverage for their specific disciplinary needs, leading to inefficiencies in literature searches.",
    "rag_method": "The authors compare the disciplinary coverage of 56 bibliographic databases to provide insights into which databases are most suitable for different research fields.\n\n**Explanation:** By systematically analyzing and comparing the coverage of 56 databases, the study provides researchers with actionable information about which databases are most comprehensive for specific disciplines. This reduces the time and effort spent searching through irrelevant or incomplete databases and ensures a more targeted and efficient literature search process.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3129344682",
    "x": 2021.532164423072,
    "y": -8.210389923461813,
    "title": "How to detect and reduce potential sources of biases in studies of SARS-CoV-2 and COVID-19",
    "authors": [
      "Emma K. Accorsi",
      "Xueting Qiu",
      "Ã‰va Rumpler"
    ],
    "first_author": "Emma K. Accorsi",
    "first_author_surname": "Accorsi",
    "year": 2021,
    "cited_by_count": 139,
    "venue": "",
    "size": 25.357168698380494,
    "color": "hsl(21, 70%, 60%)",
    "label": "Accorsi ,2021",
    "rag_problem": "Studies on SARS-CoV-2 and COVID-19 are prone to biases that can compromise the validity of findings, such as selection bias, measurement bias, and confounding factors.",
    "rag_method": "A systematic framework to detect and mitigate biases through careful study design, robust statistical methods, and transparent reporting practices.\n\n**Explanation:** The framework addresses the problem by providing structured guidelines to identify potential sources of bias during study design and execution. It includes strategies for minimizing selection bias (e.g., ensuring representative sampling), reducing measurement bias (e.g., standardizing data collection methods), and controlling for confounding factors (e.g., using multivariate statistical models). Transparent reporting practices further ensure that biases are disclosed and accounted for, improving the reliability and reproducibility of research findings.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3198704192",
    "x": 2021.0789198631967,
    "y": -5.441541602350512,
    "title": "Science Mapping of the Global Knowledge Base on Management, Leadership, and Administration Related to COVID-19 for Promoting the Sustainability of Scientific Research",
    "authors": [
      "Turgut KarakÃ¶se",
      "Ramazan Yirci",
      "Stamatios Papadakis"
    ],
    "first_author": "Turgut KarakÃ¶se",
    "first_author_surname": "KarakÃ¶se",
    "year": 2021,
    "cited_by_count": 93,
    "venue": "",
    "size": 23.829024253132165,
    "color": "hsl(21, 70%, 60%)",
    "label": "KarakÃ¶se ,2021",
    "rag_problem": "The COVID-19 pandemic has caused radical disruptions in management, leadership, and administration practices, creating challenges in sustaining scientific research and developing effective strategies for future crises.",
    "rag_method": "The authors conducted a bibliometric analysis of global publications related to management, leadership, and administration during COVID-19 to identify thematic and methodological recommendations for sustainable research programs.\n\n**Explanation:** By analyzing a large dataset of publications, the study identifies patterns, themes, and gaps in existing research. This provides actionable insights and recommendations for improving management and leadership strategies, ensuring that scientific research can adapt and remain sustainable during crises like COVID-19. The bibliometric approach systematically maps the knowledge base, helping stakeholders understand the current state of research and prioritize areas for development.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Future studies could focus on expanding the thematic scope of bibliometric analyses to include additional aspects of management, leadership, and administration beyond those directly related to COVID-19, promoting a broader understanding of sustainable research practices.\n- Further research is recommended to refine and enhance the methodological approaches used in bibliometric studies, ensuring more comprehensive and accurate mapping of global knowledge bases.\n- Exploration of the long-term impacts of COVID-19 on management and leadership practices could provide valuable insights for developing resilient and sustainable strategies in future crises.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3009926341",
    "x": 2019.988260495384,
    "y": -4.300874561051028,
    "title": "Keep up with the latest coronavirus research",
    "authors": [
      "Qingyu Chen",
      "Alexis Allot",
      "Zhiyong Lu"
    ],
    "first_author": "Qingyu Chen",
    "first_author_surname": "Chen",
    "year": 2020,
    "cited_by_count": 322,
    "venue": "",
    "size": 28.564276652394565,
    "color": "hsl(28, 70%, 60%)",
    "label": "Chen ,2020",
    "rag_problem": "æœªæ‰¾åˆ°æ˜ç¡®çš„ç ”ç©¶é—®é¢˜æè¿°",
    "rag_method": "æœªæ‰¾åˆ°æ˜ç¡®çš„æ–¹æ³•æè¿°",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2120109270",
    "x": 2006.5090400778365,
    "y": -0.33910989927914714,
    "title": "Comparison of PubMed, Scopus, Web of Science, and Google Scholar: strengths and weaknesses",
    "authors": [
      "Matthew E. Falagas",
      "Eleni Pitsouni",
      "George Malietzis"
    ],
    "first_author": "Matthew E. Falagas",
    "first_author_surname": "Falagas",
    "year": 2007,
    "cited_by_count": 4509,
    "venue": "",
    "size": 53.1823271532867,
    "color": "hsl(120, 70%, 60%)",
    "label": "Falagas ,2007",
    "rag_problem": "Researchers face challenges in selecting the most appropriate medical database for their needs due to differences in content coverage, search facilities, citation analysis capabilities, and update frequency.",
    "rag_method": "The authors conducted a comparative analysis of PubMed, Scopus, Web of Science, and Google Scholar to evaluate their strengths and weaknesses in terms of journal coverage, search functionalities, citation analysis, and update frequency.\n\n**Explanation:** By systematically comparing the databases, the study provides researchers with detailed insights into the specific features and limitations of each platform. This allows researchers to make informed decisions about which database best suits their specific research requirements, addressing the problem of uncertainty and inefficiency in database selection.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3106505254",
    "x": 2019.9453602182573,
    "y": 9.679039637162026,
    "title": "Preprinting the COVID-19 pandemic",
    "authors": [
      "Nicholas Fraser",
      "Liam Brierley",
      "G.K. Dey"
    ],
    "first_author": "Nicholas Fraser",
    "first_author_surname": "Fraser",
    "year": 2020,
    "cited_by_count": 119,
    "venue": "",
    "size": 24.76581461151147,
    "color": "hsl(28, 70%, 60%)",
    "label": "Fraser ,2020",
    "rag_problem": "The scientific community needs to rapidly disseminate research findings to address the urgent challenges posed by the COVID-19 pandemic, but traditional publishing processes are slow and hinder timely access to critical information.",
    "rag_method": "The adoption of preprinting platforms allows researchers to share their findings immediately without waiting for formal peer review and publication.\n\n**Explanation:** Preprinting bypasses the delays associated with traditional publishing by enabling researchers to upload their manuscripts directly to open-access platforms. This ensures that critical information, such as data on SARS-CoV-2, treatments, and vaccines, is available to the global scientific community in real-time, accelerating research collaboration and response to the pandemic.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2772492596",
    "x": 2017.287931412784,
    "y": 3.7960270749913727,
    "title": "A survey of author name disambiguation techniques: 2010â€“2016",
    "authors": [
      "Ijaz Hussain",
      "Sohail Asghar"
    ],
    "first_author": "Ijaz Hussain",
    "first_author_surname": "Hussain",
    "year": 2017,
    "cited_by_count": 63,
    "venue": "",
    "size": 22.354340979301455,
    "color": "hsl(49, 70%, 60%)",
    "label": "Hussain ,2017",
    "rag_problem": "Author name ambiguity in citations negatively impacts the content quality and services of digital libraries, making it difficult to correctly attribute works to their respective authors.",
    "rag_method": "A review and analysis of recently proposed author name disambiguation techniques from 2010 to 2016, highlighting their advancements, challenges, and future research directions.\n\n**Explanation:** By systematically reviewing and analyzing recent techniques, the paper provides insights into effective methods for resolving author name ambiguity. This helps digital library researchers understand the strengths and weaknesses of existing approaches, guiding them to adopt or improve upon these techniques to address the issue more effectively.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Develop more robust techniques to address the author name ambiguity problem in digital libraries, focusing on improving accuracy and scalability in diverse datasets.\n- Explore advanced machine learning and data integration methods to enhance the disambiguation process, particularly for handling complex citation patterns.\n- Investigate new approaches to tackle challenges in multilingual and interdisciplinary author name disambiguation scenarios.\n- Design systems that can adapt to evolving citation practices and dynamic author profiles over time.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4247191450",
    "x": 2019.748023023325,
    "y": 6.854268412924313,
    "title": "Coronavirus disease 2019: the harms of exaggerated information and non-evidence-based measures",
    "authors": [
      "John P. A. Ioannidis"
    ],
    "first_author": "John P. A. Ioannidis",
    "first_author_surname": "Ioannidis",
    "year": 2020,
    "cited_by_count": 131,
    "venue": "",
    "size": 25.13144429547179,
    "color": "hsl(28, 70%, 60%)",
    "label": "Ioannidis ,2020",
    "rag_problem": "Exaggerated information and non-evidence-based measures during the COVID-19 pandemic lead to unnecessary panic, misinformation, and potentially harmful societal and health outcomes.",
    "rag_method": "Promoting accurate, evidence-based communication and measures to manage the pandemic effectively.\n\n**Explanation:** By ensuring that information disseminated to the public is accurate and based on scientific evidence, the solution minimizes misinformation and prevents unnecessary panic. Evidence-based measures provide clear guidance for managing the pandemic, reducing the likelihood of implementing harmful or ineffective strategies.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2914159168",
    "x": 2018.9224889079592,
    "y": -4.9268387322763445,
    "title": "The impact of using machine translation on EFL studentsâ€™ writing",
    "authors": [
      "Sangminâ€Michelle Lee"
    ],
    "first_author": "Sangminâ€Michelle Lee",
    "first_author_surname": "Lee",
    "year": 2019,
    "cited_by_count": 295,
    "venue": "",
    "size": 28.229403041486364,
    "color": "hsl(35, 70%, 60%)",
    "label": "Lee ,2019",
    "rag_problem": "There is limited understanding of how machine translation (MT) can be effectively used as a pedagogical tool in EFL (English as a Foreign Language) writing classrooms, beyond its typical use for postediting translations.",
    "rag_method": "The study investigates the role of machine translation as a CALL (Computer-Assisted Language Learning) tool in EFL writing, employing a novel design that goes beyond the traditional focus on postediting.\n\n**Explanation:** By exploring MT as a CALL tool and employing a different design, the study aims to provide insights into how MT can be integrated into EFL writing instruction in a way that enhances learning outcomes. This approach addresses the gap in understanding by shifting the focus from merely postediting to broader pedagogical applications, thus expanding the scope of MT's utility in language learning.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate the use of machine translation as a pedagogical tool in different EFL contexts to better understand its broader applicability and effectiveness in diverse learning environments.\n- Explore the impact of machine translation on other language skills beyond writing, such as speaking, listening, and reading, to provide a more comprehensive view of its role in language learning.\n- Conduct longitudinal studies to examine the long-term effects of using machine translation on EFL students' language development and writing proficiency.\n- Analyze the effectiveness of different instructional designs involving machine translation, such as comparing post-editing tasks with other MT-based activities, to identify best practices for its integration in EFL classrooms.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4211028722",
    "x": 2017.0887089859693,
    "y": -3.616221577038873,
    "title": "Crowdsourcing and online collaborative translations",
    "authors": [
      "Miguel A. JimÃ©nez-Crespo"
    ],
    "first_author": "Miguel A. JimÃ©nez-Crespo",
    "first_author_surname": "JimÃ©nez-Crespo",
    "year": 2017,
    "cited_by_count": 162,
    "venue": "",
    "size": 25.94068578946587,
    "color": "hsl(49, 70%, 60%)",
    "label": "JimÃ©nez-Crespo ,2017",
    "rag_problem": "Traditional translation theories and practices struggle to adapt to the dynamic and unpredictable nature of crowdsourcing and online collaborative translation processes.",
    "rag_method": "The study of crowdsourcing and online collaborative translations aims to reframe existing translation theories and redefine key tenets of the discipline to better accommodate these emerging processes.\n\n**Explanation:** By analyzing the varied translational processes involved in crowdsourcing and online collaboration, researchers can identify patterns and frameworks that align with these modern practices. This allows for the development of updated theories that reflect the technological advancements and collaborative dynamics inherent to these methods, addressing the limitations of traditional approaches.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate how crowdsourcing and online collaborative translations can reshape existing translation theories, providing a deeper understanding of their impact on the discipline.\n- Explore the implications of these translational processes on public perceptions of translation, aiming to assess their broader societal and cultural influence.\n- Advance research in the \"technological turn\" within Translation Studies, focusing on the integration of technology in collaborative translation practices.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2517695692",
    "x": 2015.4089778022708,
    "y": -3.815432123253455,
    "title": "The impact of translation technologies on the process and product of translation",
    "authors": [
      "Stephen Doherty"
    ],
    "first_author": "Stephen Doherty",
    "first_author_surname": "Doherty",
    "year": 2016,
    "cited_by_count": 140,
    "venue": "",
    "size": 25.384472785900535,
    "color": "hsl(56, 70%, 60%)",
    "label": "Doherty ,2016",
    "rag_problem": "The traditional process of translation is time-consuming and labor-intensive, limiting productivity and efficiency in interlingual communication.",
    "rag_method": "The use of computer-assisted translation (CAT) tools to streamline and support the translation process.\n\n**Explanation:** CAT tools provide translators with functionalities such as translation memory, terminology management, and real-time suggestions, which reduce repetitive work, improve consistency, and speed up the translation process. This directly addresses the inefficiencies of traditional manual translation methods.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate the integration of emerging technologies with existing translation tools to further enhance productivity and quality in translation processes. This could address limitations in current systems and explore innovative solutions.\n- Explore the impact of translation technologies on less-studied languages and linguistic diversity. This would help assess their effectiveness in supporting global communication across underrepresented languages.\n- Develop advanced machine translation systems that better handle complex linguistic structures and cultural nuances. This could improve the accuracy and contextual relevance of translations.\n- Study the long-term effects of reliance on translation technologies on professional translators' skills and the translation industry as a whole. This would provide insights into how technology reshapes the profession.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2911800777",
    "x": 2019.4278468822126,
    "y": 1.6291266408202283,
    "title": "Modeling the intention to use machine translation for student translators: An extension of Technology Acceptance Model",
    "authors": [
      "Yanxia Yang",
      "Xiangling Wang"
    ],
    "first_author": "Yanxia Yang",
    "first_author_surname": "Yang",
    "year": 2019,
    "cited_by_count": 135,
    "venue": "",
    "size": 25.24596647457277,
    "color": "hsl(35, 70%, 60%)",
    "label": "Yang ,2019",
    "rag_problem": "Student translators often face challenges in adopting machine translation tools due to a lack of understanding of their usefulness and ease of use, leading to low acceptance and utilization.",
    "rag_method": "The authors extend the Technology Acceptance Model (TAM) to include additional factors specific to student translators, such as perceived usefulness, perceived ease of use, and intention to use machine translation tools.\n\n**Explanation:** By extending TAM, the authors provide a structured framework that incorporates factors influencing student translators' attitudes toward machine translation tools. This model helps identify the key drivers (e.g., perceived usefulness and ease of use) that increase the likelihood of adoption. The inclusion of these factors addresses the problem by directly targeting the reasons behind low acceptance and providing actionable insights to improve adoption rates.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2025173440",
    "x": 2015.257139933049,
    "y": -4.154734131341624,
    "title": "A survey of machine translation competences: Insights for translation technology educators and practitioners",
    "authors": [
      "Federico Gaspari",
      "Hala Almaghout",
      "Stephen Doherty"
    ],
    "first_author": "Federico Gaspari",
    "first_author_surname": "Gaspari",
    "year": 2015,
    "cited_by_count": 134,
    "venue": "",
    "size": 25.217654847164628,
    "color": "hsl(64, 70%, 60%)",
    "label": "Gaspari ,2015",
    "rag_problem": "The translation and localisation industry lacks a clear understanding of the specific competencies required to effectively use machine translation (MT) technologies.",
    "rag_method": "A large-scale survey was conducted to identify and analyze the competencies needed for machine translation usage, involving 438 validated respondents from diverse roles such as freelance translators, language service providers, translator trainers, and academics.\n\n**Explanation:** By surveying a broad range of stakeholders in the translation industry, the study collects and synthesizes data on the practical skills and knowledge required for effective MT usage. This provides educators and practitioners with actionable insights to design training programs and improve MT integration in workflows, addressing the gap in understanding competencies.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3175985315",
    "x": 2010.5701727725636,
    "y": 5.970714826096969,
    "title": "Toward a Model of Active and Situated Learning in the Teaching of Computer-Aided Translation: Introducing the CERTT Project",
    "authors": [
      "Lynne Bowker",
      "Elizabeth Marshman"
    ],
    "first_author": "Lynne Bowker",
    "first_author_surname": "Bowker",
    "year": 2010,
    "cited_by_count": 17,
    "venue": "",
    "size": 17.48806755518746,
    "color": "hsl(99, 70%, 60%)",
    "label": "Bowker ,2010",
    "rag_problem": "Translator education programs struggle to produce graduates who are proficient with modern translation tools due to limited time and resources.",
    "rag_method": "The CERTT Project introduces a model of active and situated learning that integrates the use of translation tools across different elements of the program.\n\n**Explanation:** By adopting a holistic approach, the CERTT Project ensures that students learn translation tools in a practical and contextually relevant manner, embedding tool use into various aspects of the curriculum. This active and situated learning model allows students to develop both technical proficiency and contextual understanding, addressing the challenge of limited time and resources by making tool training an integral part of the educational process.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Develop a more holistic approach to translator education by integrating the use of translation tools across various program elements, including core technology courses and other subjects. This aims to better prepare students for the evolving demands of the language industry.\n- Explore strategies to address the challenge of limited time and resources in translator education programs while ensuring that graduates are proficient in modern translation tools. This could involve optimizing curriculum design or leveraging innovative teaching methods.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2126512988",
    "x": 1995.3208900688423,
    "y": -0.013763323340123756,
    "title": "Computer Self-Efficacy: Development of a Measure and Initial Test",
    "authors": [
      "Deborah Compeau",
      "Christopher A. Higgins"
    ],
    "first_author": "Deborah Compeau",
    "first_author_surname": "Compeau",
    "year": 1995,
    "cited_by_count": 6058,
    "venue": "",
    "size": 59.71603111465672,
    "color": "hsl(205, 70%, 60%)",
    "label": "Compeau ,1995",
    "rag_problem": "There is no established measure to assess individuals' beliefs about their ability to competently use computers (computer self-efficacy), which limits understanding of its impact on computer use behaviors and outcomes.",
    "rag_method": "The authors developed and validated a measure of computer self-efficacy through a survey of Canadian managers and professionals.\n\n**Explanation:** By creating a validated measure of computer self-efficacy, the authors provide a tool to quantify individuals' beliefs about their computer-related abilities. This enables researchers and practitioners to systematically study its impact on computer use behaviors, emotional reactions, and expected outcomes, addressing the gap in understanding and facilitating targeted interventions.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Further refinement and validation of the computer self-efficacy measure across diverse populations and contexts could be conducted to ensure its generalizability and reliability beyond the initial sample of Canadian managers and professionals.\n- Exploration of the longitudinal effects of computer self-efficacy on individuals' computer usage patterns and their adaptation to new technologies could provide deeper insights into its long-term impacts.\n- Investigation into the role of computer self-efficacy in specific organizational or cultural settings could help understand how contextual factors influence its development and outcomes.\n- Development of targeted interventions or training programs to enhance computer self-efficacy could be explored, focusing on improving individuals' confidence and emotional responses to technology use.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2168353148",
    "x": 2009.649012187348,
    "y": -0.36978939578866343,
    "title": "Studies of expansive learning: Foundations, findings and future challenges",
    "authors": [
      "YrjÃ¶ EngestrÃ¶m",
      "Annalisa Sannino"
    ],
    "first_author": "YrjÃ¶ EngestrÃ¶m",
    "first_author_surname": "EngestrÃ¶m",
    "year": 2010,
    "cited_by_count": 1554,
    "venue": "",
    "size": 43.24147354028107,
    "color": "hsl(99, 70%, 60%)",
    "label": "EngestrÃ¶m ,2010",
    "rag_problem": "Traditional learning theories and practices often fail to address the dynamic, collective, and transformative nature of learning in complex and evolving environments.",
    "rag_method": "The concept of expansive learning, which emphasizes collective activity, boundary-crossing, and the creation of new knowledge and practices, is proposed as a framework to address these limitations.\n\n**Explanation:** Expansive learning shifts the focus from individual acquisition of knowledge to collective problem-solving and innovation. By engaging learners in collaborative activities that transcend traditional boundaries, it enables the generation of new practices and tools that are better suited to dynamic and complex environments. This directly addresses the limitations of traditional learning approaches by fostering adaptability and innovation.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2039483660",
    "x": 1993.8771110802988,
    "y": -0.2633703344194007,
    "title": "Enhancing self-efficacy for computer technologies through the use of positive classroom experiences",
    "authors": [
      "Peggy A. Ertmer",
      "Elizabeth Evenbeck",
      "Katherine S. Cennamo"
    ],
    "first_author": "Peggy A. Ertmer",
    "first_author_surname": "Ertmer",
    "year": 1994,
    "cited_by_count": 103,
    "venue": "",
    "size": 24.21684999574625,
    "color": "hsl(212, 70%, 60%)",
    "label": "Ertmer ,1994",
    "rag_problem": "Students often lack self-efficacy in using computer technologies, which hinders their ability to engage effectively with digital tools and learning environments.",
    "rag_method": "Creating positive classroom experiences specifically designed to enhance students' confidence and competence in using computer technologies.\n\n**Explanation:** Positive classroom experiences, such as supportive teaching methods, collaborative activities, and opportunities for hands-on practice, help students build confidence in their abilities by reducing anxiety and fostering a sense of accomplishment. These experiences directly address the issue of low self-efficacy by providing a structured environment where students can succeed and gradually develop their skills.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W1986845215",
    "x": 2011.0343719565367,
    "y": -3.9700591089251906,
    "title": "Translating by post-editing: is it the way forward?",
    "authors": [
      "Ignacio GonzÃ¡lez GarcÃ­a"
    ],
    "first_author": "Ignacio GonzÃ¡lez GarcÃ­a",
    "first_author_surname": "GarcÃ­a",
    "year": 2011,
    "cited_by_count": 107,
    "venue": "",
    "size": 24.361629747722983,
    "color": "hsl(92, 70%, 60%)",
    "label": "GarcÃ­a ,2011",
    "rag_problem": "Traditional machine translation often produces output with errors and inaccuracies, requiring extensive manual correction to achieve acceptable quality.",
    "rag_method": "Post-editing, where human translators refine and correct machine-generated translations to improve accuracy and fluency.\n\n**Explanation:** Post-editing leverages the initial output of machine translation as a starting point, reducing the time and effort required compared to translating from scratch. Human intervention ensures that errors are corrected and the final translation meets quality standards, addressing the inaccuracies inherent in raw machine translation.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3106637466",
    "x": 2020.0825128095018,
    "y": -1.6769166071793717,
    "title": "Research lines on the impact of the COVID-19 pandemic on business. A text mining analysis",
    "authors": [
      "Patricia Carracedo",
      "Rosa Puertas",
      "Luisa MartÃ­"
    ],
    "first_author": "Patricia Carracedo",
    "first_author_surname": "Carracedo",
    "year": 2020,
    "cited_by_count": 180,
    "venue": "",
    "size": 26.342516429122846,
    "color": "hsl(28, 70%, 60%)",
    "label": "Carracedo ,2020",
    "rag_problem": "The COVID-19 pandemic has caused widespread disruptions in business operations, creating a need to understand its multifaceted impact on different industries and sectors.",
    "rag_method": "The authors employ text mining techniques to analyze existing research and identify key themes and research lines related to the impact of the COVID-19 pandemic on business.\n\n**Explanation:** Text mining allows for the systematic extraction of patterns, trends, and themes from a large corpus of research articles. By applying this method, the authors can synthesize fragmented information and provide a comprehensive overview of the pandemic's effects on business, helping stakeholders and researchers identify critical areas of concern and opportunities for further study.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4313494085",
    "x": 2023.5597823101464,
    "y": -8.660585910834053,
    "title": "Toward the design of ultrahigh-entropy alloys via mining six million texts",
    "authors": [
      "Zongrui Pei",
      "Junqi Yin",
      "Peter K. Liaw"
    ],
    "first_author": "Zongrui Pei",
    "first_author_surname": "Pei",
    "year": 2023,
    "cited_by_count": 72,
    "venue": "",
    "size": 22.859095269071002,
    "color": "hsl(7, 70%, 60%)",
    "label": "Pei ,2023",
    "rag_problem": "Designing ultrahigh-entropy alloys (UHEAs) is challenging due to the vast compositional space and the lack of efficient methods to identify promising alloy combinations.",
    "rag_method": "Utilizing text mining techniques to analyze six million scientific texts and extract relevant data for UHEA design.\n\n**Explanation:** By mining a large corpus of scientific literature, the approach identifies patterns, correlations, and insights about alloy compositions and properties that are otherwise difficult to discern manually. This enables the systematic narrowing down of the compositional space and provides data-driven guidance for UHEA design, addressing the inefficiency and complexity of traditional trial-and-error methods.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3161985147",
    "x": 2020.429699765482,
    "y": -2.843840466608871,
    "title": "Research on Covid-19: a disruptive phenomenon for bibliometrics",
    "authors": [
      "Yves Fassin"
    ],
    "first_author": "Yves Fassin",
    "first_author_surname": "Fassin",
    "year": 2021,
    "cited_by_count": 55,
    "venue": "",
    "size": 21.842086769055275,
    "color": "hsl(21, 70%, 60%)",
    "label": "Fassin ,2021",
    "rag_problem": "The Covid-19 pandemic has caused an unprecedented explosion of scientific literature, making it challenging to analyze and understand its impact on bibliometric indicators.",
    "rag_method": "The authors propose analyzing Covid-19 research using bibliometric methods to study the publication explosion and its impact on bibliometric indicators.\n\n**Explanation:** Bibliometric methods provide quantitative tools to measure and analyze patterns in scientific publications, such as citation trends, publication volume, and collaboration networks. By applying these methods to Covid-19 research, the authors aim to systematically capture and interpret the effects of the pandemic on academic publishing, addressing the challenge of understanding this disruptive phenomenon.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4206541507",
    "x": 2022.5007977902767,
    "y": -1.4480092028555678,
    "title": "Changes in the use of mobile devices during the crisis: Immediate response to the COVID-19 pandemic",
    "authors": [
      "Sotaro Katsumata",
      "Takeyasu Ichikohji",
      "Satoshi Nakano"
    ],
    "first_author": "Sotaro Katsumata",
    "first_author_surname": "Katsumata",
    "year": 2022,
    "cited_by_count": 46,
    "venue": "",
    "size": 21.169967423248593,
    "color": "hsl(14, 70%, 60%)",
    "label": "Katsumata ,2022",
    "rag_problem": "The COVID-19 pandemic caused sudden disruptions to daily life, leading to changes in how individuals interact with mobile devices for communication, work, and entertainment.",
    "rag_method": "The study analyzes the immediate changes in mobile device usage patterns during the crisis to understand behavioral adaptations and technological reliance.\n\n**Explanation:** By examining mobile device usage data during the pandemic, the study identifies shifts in user behavior, such as increased reliance on communication apps, remote work tools, and streaming services. This analysis provides insights into how mobile technology supported individuals in adapting to the challenges posed by social distancing and lockdown measures.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4322743452",
    "x": 2023.3660861283167,
    "y": -5.761981544007039,
    "title": "Integrating Structured and Unstructured EHR Data for Predicting Mortality by Machine Learning and Latent Dirichlet Allocation Method",
    "authors": [
      "Chihâ€Chou Chiu",
      "Chung-Min Wu",
      "Te-Nien Chien"
    ],
    "first_author": "Chihâ€Chou Chiu",
    "first_author_surname": "Chiu",
    "year": 2023,
    "cited_by_count": 33,
    "venue": "",
    "size": 19.927852814805618,
    "color": "hsl(7, 70%, 60%)",
    "label": "Chiu ,2023",
    "rag_problem": "Existing models for predicting ICU patient mortality primarily rely on structured clinical data, neglecting the valuable information contained in unstructured clinical data such as physician notes and admission records.",
    "rag_method": "The authors propose integrating both structured and unstructured EHR (Electronic Health Record) data using a combination of machine learning techniques and the Latent Dirichlet Allocation (LDA) method to improve mortality prediction accuracy.\n\n**Explanation:** Structured data provides quantifiable metrics (e.g., lab results, vital signs), while unstructured data contains contextual and narrative information (e.g., physician observations) that can offer additional insights into a patient's condition. By using the LDA method, the unstructured data is transformed into topic distributions, making it compatible with machine learning models. This integration allows the model to leverage a more comprehensive dataset, leading to more accurate predictions of ICU patient mortality.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3009951436",
    "x": 2020.1248591218473,
    "y": -7.161960448514802,
    "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
    "authors": [
      "Alfonso J. RodrÃ­guezâ€Morales",
      "Jaime A. Cardonaâ€Ospina",
      "EstefanÃ­a GutiÃ©rrezâ€Ocampo"
    ],
    "first_author": "Alfonso J. RodrÃ­guezâ€Morales",
    "first_author_surname": "RodrÃ­guezâ€Morales",
    "year": 2020,
    "cited_by_count": 2477,
    "venue": "",
    "size": 45.47595081264101,
    "color": "hsl(28, 70%, 60%)",
    "label": "RodrÃ­guezâ€Morales ,2020",
    "rag_problem": "There is a lack of consolidated and systematic understanding of the clinical, laboratory, and imaging features of COVID-19, which hampers effective diagnosis and management of the disease.",
    "rag_method": "The authors conducted a systematic review and meta-analysis to aggregate and analyze data from multiple studies on the clinical, laboratory, and imaging features of COVID-19.\n\n**Explanation:** By systematically reviewing and synthesizing data from various studies, the meta-analysis provides a comprehensive overview of the common and significant features associated with COVID-19. This consolidated information helps healthcare professionals identify patterns and make informed decisions regarding diagnosis and treatment, addressing the fragmented understanding of the disease.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3002108456",
    "x": 2020.2854706107498,
    "y": -11.805968313492869,
    "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
    "authors": [
      "Nanshan Chen",
      "Min Zhou",
      "Xuan Dong"
    ],
    "first_author": "Nanshan Chen",
    "first_author_surname": "Chen",
    "year": 2020,
    "cited_by_count": 22489,
    "venue": "",
    "size": 60,
    "color": "hsl(28, 70%, 60%)",
    "label": "Chen ,2020",
    "rag_problem": "Lack of detailed epidemiological and clinical data on patients affected by the 2019 novel coronavirus pneumonia (NCP) in Wuhan, China, which hinders understanding of disease characteristics and informs public health responses.",
    "rag_method": "Conducted a descriptive study analyzing epidemiological and clinical characteristics of 99 confirmed NCP cases in Wuhan, including demographic data, symptoms, laboratory findings, and outcomes.\n\n**Explanation:** By systematically collecting and analyzing data from 99 patients, the study provides a detailed understanding of the disease's epidemiology and clinical presentation. This information helps identify patterns such as common symptoms, risk factors, and disease progression, which are essential for guiding clinical management and public health strategies.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3001118548",
    "x": 2020.4115517952987,
    "y": -14.708848136788422,
    "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
    "authors": [
      "Chaolin Huang",
      "Yeming Wang",
      "Xingwang Li"
    ],
    "first_author": "Chaolin Huang",
    "first_author_surname": "Huang",
    "year": 2020,
    "cited_by_count": 51227,
    "venue": "",
    "size": 60,
    "color": "hsl(28, 70%, 60%)",
    "label": "Huang ,2020",
    "rag_problem": "Lack of understanding of clinical features and progression of 2019 novel coronavirus infection in patients, which hinders effective diagnosis and treatment strategies.",
    "rag_method": "Comprehensive analysis of clinical features, laboratory findings, and outcomes of patients infected with 2019 novel coronavirus in Wuhan, China.\n\n**Explanation:** By systematically analyzing the clinical data of infected patients, the study identifies patterns such as symptoms, progression, and laboratory abnormalities. This provides critical insights into the disease's behavior, enabling healthcare professionals to recognize key indicators for diagnosis and tailor treatment approaches effectively.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3014892682",
    "x": 2019.595182079234,
    "y": 1.0880964378542457,
    "title": "Severe acute respiratory syndrome (SARS) and coronavirus disease-2019 (COVID-19): From causes to preventions in Hong Kong",
    "authors": [
      "Siukan Law",
      "Albert Wingnang Leung",
      "Chuanshan Xu"
    ],
    "first_author": "Siukan Law",
    "first_author_surname": "Law",
    "year": 2020,
    "cited_by_count": 158,
    "venue": "",
    "size": 25.845371368388058,
    "color": "hsl(28, 70%, 60%)",
    "label": "Law ,2020",
    "rag_problem": "The rapid spread and severe impact of SARS and COVID-19 in Hong Kong, leading to public health crises and economic disruptions.",
    "rag_method": "Implementation of comprehensive prevention strategies, including early detection, quarantine measures, public education, and vaccination programs.\n\n**Explanation:** The solution addresses the problem by targeting the root causes of disease transmission and mitigating its impact. Early detection allows for timely identification of cases, reducing the risk of widespread outbreaks. Quarantine measures prevent infected individuals from transmitting the virus to others. Public education promotes awareness and adherence to preventive behaviors, while vaccination programs reduce susceptibility to severe disease and lower transmission rates.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3003465021",
    "x": 2019.6624538970136,
    "y": -9.135334628401662,
    "title": "First Case of 2019 Novel Coronavirus in the United States",
    "authors": [
      "Michelle Holshue",
      "Chas DeBolt",
      "Scott Lindquist"
    ],
    "first_author": "Michelle Holshue",
    "first_author_surname": "Holshue",
    "year": 2020,
    "cited_by_count": 6331,
    "venue": "",
    "size": 59.96963154647183,
    "color": "hsl(28, 70%, 60%)",
    "label": "Holshue ,2020",
    "rag_problem": "The need for rapid identification and management of novel coronavirus (2019-nCoV) cases to prevent further spread and understand the clinical progression of the disease.",
    "rag_method": "The authors describe the identification, diagnosis, clinical course, and management of the first confirmed case of 2019-nCoV in the United States, including monitoring symptoms, progression to pneumonia, and coordination between clinicians and public health authorities.\n\n**Explanation:** By documenting the clinical progression of the disease and the management strategies employed, the study provides critical insights into the behavior of the virus, enabling healthcare professionals to recognize symptoms early, predict disease progression, and implement effective containment and treatment measures. Coordination between clinicians and public health authorities ensures timely reporting and response to cases, which is essential for controlling the outbreak.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate the transmission dynamics of 2019-nCoV, including modes of spread and factors influencing transmission rates, to better inform public health interventions.\n- Develop and evaluate diagnostic tools for earlier and more precise detection of 2019-nCoV, especially in asymptomatic or mildly symptomatic cases.\n- Study the progression and clinical management of 2019-nCoV infections to improve treatment protocols and patient outcomes.\n- Explore the effectiveness of coordinated response strategies between healthcare systems and public health authorities to enhance preparedness for future outbreaks.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4381332452",
    "x": 2022.9814492980713,
    "y": -11.716457759086019,
    "title": "Generative artificial intelligence in the metaverse era",
    "authors": [
      "Zhihan Lv"
    ],
    "first_author": "Zhihan Lv",
    "first_author_surname": "Lv",
    "year": 2023,
    "cited_by_count": 261,
    "venue": "",
    "size": 27.761328299581965,
    "color": "hsl(7, 70%, 60%)",
    "label": "Lv ,2023",
    "rag_problem": "The paper content provided is insufficient to identify any specific problem addressed by the authors.",
    "rag_method": "No solution can be determined due to the lack of detailed content in the paper.\n\n**Explanation:** Without additional information or context from the paper, it is impossible to establish a causal relationship between a problem and a proposed solution.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4390583680",
    "x": 2022.558886142867,
    "y": 0.10411463510491747,
    "title": "The Integration and Utilization of Artificial Intelligence (AI) in Supporting Older/Senior Lecturers to Adapt to the Changing Landscape in Translation Pedagogy",
    "authors": [
      "Nisar Ahmad Koka"
    ],
    "first_author": "Nisar Ahmad Koka",
    "first_author_surname": "Koka",
    "year": 2023,
    "cited_by_count": 7,
    "venue": "",
    "size": 14.37717048965073,
    "color": "hsl(7, 70%, 60%)",
    "label": "Koka ,2023",
    "rag_problem": "Older/senior lecturers face difficulties in adapting to the rapidly changing requirements of translation pedagogy, particularly due to the integration of AI translation tools.",
    "rag_method": "The utilization of Artificial Intelligence (AI) systems to support older/senior lecturers in learning and adapting to new AI translation tools and methodologies.\n\n**Explanation:** AI systems can provide tailored training, user-friendly interfaces, and adaptive learning environments that simplify the process of understanding and using AI translation tools. These systems can reduce the cognitive load and technical barriers for older lecturers, enabling them to effectively incorporate AI tools into their teaching practices and stay updated with the evolving pedagogical landscape.",
    "rag_limitation": "- The proposed method may face challenges in effectively supporting older/senior lecturers who might struggle with adapting to rapidly evolving AI translation tools, as their familiarity with such technologies may be limited.\n- The approach might not fully address the diverse levels of technological proficiency among senior educators, potentially leading to uneven outcomes in their adaptation process.",
    "rag_future_work": "- Investigate tailored AI training programs for older/senior lecturers to enhance their adaptability to evolving translation pedagogy requirements. This could address the challenges they face in mastering new AI tools effectively.\n- Explore the development of user-friendly AI translation tools specifically designed for educators with limited technical expertise. This would help bridge the gap between technological advancements and practical usability for senior lecturers.\n- Conduct longitudinal studies to assess the long-term impact of AI integration on the teaching methodologies and effectiveness of older educators in translation pedagogy. This could provide insights into sustained adaptation strategies.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4389456104",
    "x": 2022.6366979135037,
    "y": 5.448092191111543,
    "title": "Unifying Linguistic Landscapes",
    "authors": [
      "Ray Gutierrez"
    ],
    "first_author": "Ray Gutierrez",
    "first_author_surname": "Gutierrez",
    "year": 2023,
    "cited_by_count": 4,
    "venue": "",
    "size": 12.5741387592088,
    "color": "hsl(7, 70%, 60%)",
    "label": "Gutierrez ,2023",
    "rag_problem": "Persistent global language barriers hinder effective communication across different linguistic groups.",
    "rag_method": "Leveraging neural networks in machine translation to achieve near-human-level accuracy.\n\n**Explanation:** Neural networks are capable of learning complex linguistic patterns and nuances, enabling machine translation systems to produce more accurate and contextually appropriate translations. This directly addresses the issue of miscommunication caused by language barriers by providing a reliable tool for cross-linguistic understanding.",
    "rag_limitation": "- The proposed integration of nanotechnology and AI for real-time translation may face challenges related to potential misuse, which could raise ethical and security concerns.\n- Achieving near-human-level accuracy in machine translation still poses difficulties, particularly in handling nuanced or context-dependent language scenarios.",
    "rag_future_work": "- Investigate advancements in neural network-based machine translation to achieve consistent near-human-level accuracy across diverse languages and contexts.\n- Explore the integration of nanotechnology with augmented reality and wearable devices to enable seamless real-time language translation.\n- Address challenges related to potential misuse and ethical concerns of these emerging technologies to ensure responsible deployment.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4399213274",
    "x": 2024.0446092563645,
    "y": -1.5343799902864659,
    "title": "Human Intelligence and Artificial Intelligence in Professional Translations â€” Redesigning the Translator Profession",
    "authors": [
      "Felicia Constantin",
      "Anamaria-Mirabela Pop",
      "Monica-Ariana Sim"
    ],
    "first_author": "Felicia Constantin",
    "first_author_surname": "Constantin",
    "year": 2024,
    "cited_by_count": 4,
    "venue": "",
    "size": 12.5741387592088,
    "color": "hsl(0, 70%, 60%)",
    "label": "Constantin ,2024",
    "rag_problem": "The translator profession is at risk of losing its consistency and identity due to the increasing dominance of AI, which shifts the focus of translation work closer to post-editing rather than creative or nuanced translation.",
    "rag_method": "Redesigning the translator profession by redefining the roles and collaboration between human intelligence (HI) and artificial intelligence (AI) in translation workflows.\n\n**Explanation:** By redesigning the profession, the authors aim to establish a new framework where human intelligence focuses on tasks requiring creativity, cultural nuance, and contextual understanding, while AI handles repetitive and mechanical aspects of translation. This division of labor preserves the unique contributions of human translators and mitigates the risk of the profession devolving into mere post-editing.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4394684629",
    "x": 2023.5966978987792,
    "y": 4.111902619559349,
    "title": "Enhancing translation pedagogy through culture-specific terms",
    "authors": [
      "Matteo Sanesi"
    ],
    "first_author": "Matteo Sanesi",
    "first_author_surname": "Sanesi",
    "year": 2024,
    "cited_by_count": 3,
    "venue": "",
    "size": 11.718113659767154,
    "color": "hsl(0, 70%, 60%)",
    "label": "Sanesi ,2024",
    "rag_problem": "Culture-specific terms lack direct equivalents in other languages, creating challenges in communication and translation, which can lead to misinterpretations and hinder cross-cultural understanding.",
    "rag_method": "Integrating culture-specific terms into translation pedagogy to enhance translators' ability to understand and convey cultural nuances effectively.\n\n**Explanation:** By incorporating culture-specific terms into translation education, translators are trained to recognize, analyze, and appropriately translate these culturally embedded expressions. This approach equips them with the skills to navigate linguistic and cultural discrepancies, ensuring more accurate and culturally sensitive translations, thus addressing the problem of misinterpretations and communication barriers.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2083078026",
    "x": 2014.056434127688,
    "y": 3.882808456927224,
    "title": "Ethical Aspects of Translation: Striking a Balance between Following Translation Ethics and Producing a TT for Serving a Specific Purpose",
    "authors": [
      "Rafat Y. Alwazna"
    ],
    "first_author": "Rafat Y. Alwazna",
    "first_author_surname": "Alwazna",
    "year": 2014,
    "cited_by_count": 10,
    "venue": "",
    "size": 15.5988252730527,
    "color": "hsl(71, 70%, 60%)",
    "label": "Alwazna ,2014",
    "rag_problem": "Strict adherence to translation ethics, defined as preserving the undistorted meaning of the source text, often conflicts with the need to adapt the translation to meet audience expectations or serve specific purposes.",
    "rag_method": "Propose a balanced approach to translation ethics that allows for selective distortion of the source text meaning to align with audience expectations and the intended purpose of the translation.\n\n**Explanation:** The solution addresses the problem by acknowledging the limitations of rigid translation ethics and introducing flexibility. This approach enables translators to adapt the text in ways that fulfill the functional requirements of the target audience while maintaining ethical considerations. By striking a balance, the translator can navigate the tension between fidelity to the source text and the practical needs of the translation's purpose.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate broader definitions of translation ethics to encompass scenarios where distortion of the source text meaning is necessary to meet audience expectations. This could help refine ethical frameworks in translation studies.\n- Explore practical applications of balancing strict translation ethics with the need to adapt translations for specific purposes, analyzing case studies across different languages and contexts.\n- Examine the impact of audience expectations on translation decisions, focusing on how cultural and situational factors influence ethical considerations in translation practices.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3133702157",
    "x": 2020.9430091149943,
    "y": -13.270755487386104,
    "title": "On the Dangers of Stochastic Parrots",
    "authors": [
      "Emily M. Bender",
      "Timnit Gebru",
      "Angelina McMillan-Major"
    ],
    "first_author": "Emily M. Bender",
    "first_author_surname": "Bender",
    "year": 2021,
    "cited_by_count": 4418,
    "venue": "",
    "size": 53.07480735069235,
    "color": "hsl(21, 70%, 60%)",
    "label": "Bender ,2021",
    "rag_problem": "Large language models, such as GPT-2/3 and BERT, pose ethical risks due to their potential to generate harmful, biased, or misleading content when deployed at scale.",
    "rag_method": "The authors propose critical examination and responsible development practices for large language models, emphasizing transparency, accountability, and ethical considerations in their design and deployment.\n\n**Explanation:** By advocating for responsible development practices, the authors aim to mitigate the risks associated with harmful outputs, biases, and misinformation. Transparency ensures that stakeholders understand the limitations and potential dangers of these models, while accountability ensures that developers and organizations are held responsible for the consequences of deploying such models. Ethical considerations guide the design process to prioritize societal well-being and reduce harm.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3183428091",
    "x": 2014.3150554723572,
    "y": -0.19529881990814968,
    "title": "TEACHING ETHICS AND CRITICAL THINKING IN CONTEMPORARY SCHOOLS",
    "authors": [
      "Bojan Borstner",
      "Smiljana Gartner"
    ],
    "first_author": "Bojan Borstner",
    "first_author_surname": "Borstner",
    "year": 2014,
    "cited_by_count": 11,
    "venue": "",
    "size": 15.932619022419093,
    "color": "hsl(71, 70%, 60%)",
    "label": "Borstner ,2014",
    "rag_problem": "Ethical dilemmas and decisions, especially in professional contexts, can have irreversible consequences, yet individuals often lack the critical thinking and systematic decision-making skills necessary to address these issues effectively.",
    "rag_method": "Promoting critical, reflective decision-making and systematic thought processes as the most effective method for teaching ethics in contemporary schools.\n\n**Explanation:** By emphasizing critical and reflective decision-making, the approach equips individuals with the ability to analyze ethical dilemmas systematically, consider the consequences of their actions, and make informed decisions. This method addresses the lack of preparedness for handling complex ethical issues by fostering skills that are directly applicable to real-life situations, especially in professional contexts where decisions can have significant impacts.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4244669226",
    "x": 2020.2943856160198,
    "y": 12.24251168138478,
    "title": "The Routledge Handbook of Translation and Ethics",
    "authors": [
      "Koskinen, Kaisa 1966-",
      "Pokorn, Nike K. 1967-"
    ],
    "first_author": "Koskinen, Kaisa 1966-",
    "first_author_surname": "1966-",
    "year": 2020,
    "cited_by_count": 77,
    "venue": "",
    "size": 23.113241698631043,
    "color": "hsl(28, 70%, 60%)",
    "label": "1966- ,2020",
    "rag_problem": "The lack of a comprehensive framework to address ethical dilemmas in translation and interpreting, which results in inconsistent approaches and unresolved challenges for translatorial actors such as trainers, researchers, and practitioners.",
    "rag_method": "The Routledge Handbook of Translation and Ethics provides a structured and comprehensive overview of philosophical, theoretical, and practical aspects of ethics in Translation Studies, offering insights into emerging issues and dilemmas faced by translatorial actors.\n\n**Explanation:** By compiling contributions from leading scholars and new voices, the handbook systematically explores ethical thinking and dilemmas, equipping translatorial actors with a deeper understanding and tools to navigate ethical challenges. This structured approach helps unify perspectives and provides actionable guidance for consistent ethical decision-making.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4241903662",
    "x": 2018.6077125672525,
    "y": -1.2272822631629632,
    "title": "Machine Translation and Global Research: Towards Improved Machine Translation Literacy in the Scholarly Community",
    "authors": [
      "Lynne Bowker",
      "Jairo Buitrago"
    ],
    "first_author": "Lynne Bowker",
    "first_author_surname": "Bowker",
    "year": 2019,
    "cited_by_count": 177,
    "venue": "",
    "size": 26.27840003800018,
    "color": "hsl(35, 70%, 60%)",
    "label": "Bowker ,2019",
    "rag_problem": "Scholars and librarians lack sufficient literacy in effectively using machine translation tools, which limits the global reach and impact of scholarly work.",
    "rag_method": "Introduction and promotion of the concept of 'machine translation literacy' tailored for researchers and information professionals.\n\n**Explanation:** By educating scholars and librarians about machine translation literacy, they can better understand how to use these tools effectively, avoid common pitfalls, and optimize the accuracy and accessibility of their research. This directly addresses the issue of limited global reach by empowering users to leverage machine translation in a more informed and strategic manner.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Develop targeted training programs to enhance machine translation literacy among scholars and librarians, focusing on practical skills to effectively use and evaluate machine translation tools in academic contexts.\n- Investigate the impact of improved machine translation literacy on the global reach and accessibility of scholarly work, including how it influences collaboration and knowledge dissemination across linguistic boundaries.\n- Explore the integration of machine translation literacy into existing academic and library curricula, assessing its effectiveness in preparing researchers and information professionals for the digital age.\n- Conduct further studies on the limitations and ethical implications of machine translation in scholarly communication, aiming to establish guidelines for responsible and effective use.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4403637392",
    "x": 2023.6677021148146,
    "y": -9.870692851078871,
    "title": "How developments in natural language processing help us in understanding human behaviour",
    "authors": [
      "Rada Mihalcea",
      "Laura Biester",
      "Ryan L. Boyd"
    ],
    "first_author": "Rada Mihalcea",
    "first_author_surname": "Mihalcea",
    "year": 2024,
    "cited_by_count": 14,
    "venue": "",
    "size": 16.78864412186074,
    "color": "hsl(0, 70%, 60%)",
    "label": "Mihalcea ,2024",
    "rag_problem": "Understanding human behavior is complex due to the vast amount of unstructured data in natural language, which is difficult to analyze systematically.",
    "rag_method": "Developments in natural language processing (NLP) provide tools and techniques to systematically analyze and extract meaningful patterns from unstructured text data.\n\n**Explanation:** NLP techniques, such as sentiment analysis, topic modeling, and entity recognition, enable the processing and interpretation of large-scale textual data. These methods help identify patterns, sentiments, and relationships within text, making it possible to derive insights about human behavior that were previously inaccessible due to the complexity and volume of the data.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4402418067",
    "x": 2024.1402534283786,
    "y": -6.908459103774474,
    "title": "Governing with Intelligence: The Impact of Artificial Intelligence on Policy Development",
    "authors": [
      "Muhammad Asfand E Yar",
      "Mahani Hamdan",
      "Muhammad Anshari"
    ],
    "first_author": "Muhammad Asfand E Yar",
    "first_author_surname": "Yar",
    "year": 2024,
    "cited_by_count": 7,
    "venue": "",
    "size": 14.37717048965073,
    "color": "hsl(0, 70%, 60%)",
    "label": "Yar ,2024",
    "rag_problem": "Public policy development faces challenges in processing vast amounts of data, identifying patterns, and making informed, evidence-based decisions efficiently.",
    "rag_method": "The integration of artificial intelligence (AI) into policy development processes to analyze large datasets, identify trends, and provide data-driven insights.\n\n**Explanation:** AI systems are capable of processing and analyzing massive amounts of data at speeds and accuracies far beyond human capabilities. By leveraging AI, policymakers can identify patterns, predict outcomes, and make evidence-based decisions more efficiently. This directly addresses the challenge of handling complex and large-scale data in policy development.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4394828653",
    "x": 2024.589880659548,
    "y": -4.467505901111906,
    "title": "Artificial Intelligence for the Internal Democracy of Political Parties",
    "authors": [
      "Claudio Novelli",
      "Giuliano Formisano",
      "Prathm Juneja"
    ],
    "first_author": "Claudio Novelli",
    "first_author_surname": "Novelli",
    "year": 2024,
    "cited_by_count": 6,
    "venue": "",
    "size": 13.864916279404547,
    "color": "hsl(0, 70%, 60%)",
    "label": "Novelli ,2024",
    "rag_problem": "Political parties often face challenges in ensuring fair, transparent, and inclusive internal democratic processes, such as candidate selection, decision-making, and member participation.",
    "rag_method": "Utilizing artificial intelligence (AI) systems to enhance transparency, streamline decision-making, and facilitate member engagement within political parties.\n\n**Explanation:** AI systems can analyze large datasets, identify biases, and provide recommendations that improve fairness in candidate selection and decision-making processes. Additionally, AI tools can enable more efficient communication and participation mechanisms, ensuring that all members have equal opportunities to contribute and engage in party activities. This directly addresses the issues of transparency and inclusivity by leveraging AI's ability to process information impartially and at scale.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4393097350",
    "x": 2024.4879908819228,
    "y": 1.8616608585146286,
    "title": "Exploring the role of uncertainty, emotions, and scientific discourse during the COVID-19 pandemic",
    "authors": [
      "Antoine Lemor",
      "Ã‰ric Montpetit"
    ],
    "first_author": "Antoine Lemor",
    "first_author_surname": "Lemor",
    "year": 2024,
    "cited_by_count": 4,
    "venue": "",
    "size": 12.5741387592088,
    "color": "hsl(0, 70%, 60%)",
    "label": "Lemor ,2024",
    "rag_problem": "Policymakers faced significant uncertainty during the COVID-19 pandemic, which impacted their ability to make informed decisions and led to emotional responses that influenced policy outcomes.",
    "rag_method": "The authors developed indices using natural language processing (NLP) techniques to measure sentiments of uncertainty, negative emotions, and the prevalence of scientific statements in policymaking discourse.\n\n**Explanation:** By quantifying uncertainty, negative sentiments, and scientific discourse, the indices provide a structured way to analyze how these factors influenced policy decisions. This mechanism helps policymakers understand the emotional and informational drivers behind their decisions, enabling more balanced and evidence-based policymaking in future crises.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate the role of uncertainty and emotions in shaping policies in other regions or countries to compare with the findings from Quebec, Canada. This could provide a broader understanding of how cultural and political contexts influence decision-making during pandemics.\n- Enhance the natural language processing (NLP) techniques used in the study to develop more refined indices for measuring sentiments and scientific discourse. This improvement could lead to more accurate and detailed analyses of policymaker communications.\n- Explore the long-term impact of uncertainty and emotional sentiments on public trust in science and policy decisions during health crises. This would help assess the lasting effects of communication strategies used during the COVID-19 pandemic.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4367397709",
    "x": 2022.928493798278,
    "y": 8.645754257003608,
    "title": "A Study of Ethical Issues in Natural Language Processing with Artificial Intelligence",
    "authors": [
      "Yongfeng Ma"
    ],
    "first_author": "Yongfeng Ma",
    "first_author_surname": "Ma",
    "year": 2023,
    "cited_by_count": 4,
    "venue": "",
    "size": 12.5741387592088,
    "color": "hsl(7, 70%, 60%)",
    "label": "Ma ,2023",
    "rag_problem": "Ethical issues in natural language processing (NLP) are increasingly significant due to its widespread application, yet these issues remain underexplored and lack systematic study.",
    "rag_method": "Conducting a focused study on the ethical issues in NLP, analyzing the challenges and proposing frameworks or guidelines for addressing these ethical concerns.\n\n**Explanation:** By systematically studying the ethical issues in NLP, the authors aim to identify specific challenges and provide structured approaches to mitigate them. This solution addresses the problem by bringing attention to overlooked ethical concerns and offering actionable insights or frameworks that can guide the responsible development and deployment of NLP systems.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4238374879",
    "x": 2011.379694928039,
    "y": 3.95385019083682,
    "title": "Congressional Reforms",
    "authors": [
      "E. Scott Adler"
    ],
    "first_author": "E. Scott Adler",
    "first_author_surname": "Adler",
    "year": 2011,
    "cited_by_count": 7,
    "venue": "",
    "size": 14.37717048965073,
    "color": "hsl(92, 70%, 60%)",
    "label": "Adler ,2011",
    "rag_problem": "The provided text does not contain sufficient information about the specific problems addressed by congressional reforms.",
    "rag_method": "No solution can be identified due to the absence of detailed content or mechanisms in the text.\n\n**Explanation:** Without detailed content or context, it is impossible to determine the causal relationship between a solution and a problem. The text only includes the title of the paper, which does not provide any substantive information.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2117002298",
    "x": 2006.452464159966,
    "y": 0.1792109019959255,
    "title": "Whose Deaths Matter? Mortality, Advocacy, and Attention to Disease in the Mass Media",
    "authors": [
      "Elizabeth Armstrong",
      "Daniel Carpenter",
      "Marie Hojnacki"
    ],
    "first_author": "Elizabeth Armstrong",
    "first_author_surname": "Armstrong",
    "year": 2006,
    "cited_by_count": 75,
    "venue": "",
    "size": 23.013594327239673,
    "color": "hsl(127, 70%, 60%)",
    "label": "Armstrong ,2006",
    "rag_problem": "Media attention to diseases does not consistently align with mortality rates, leading to potential misallocation of public awareness and resources.",
    "rag_method": "The authors analyze a unique dataset covering print and broadcast media attention to seven diseases across nineteen years, examining the relationship between mortality rates and media coverage.\n\n**Explanation:** By systematically analyzing cross-disease and cross-temporal variations in media attention, the authors identify patterns and discrepancies between mortality rates and media coverage. This provides insights into whether media attention is driven by actual mortality statistics or other factors, such as advocacy efforts. The findings can inform strategies to align media coverage more closely with public health priorities.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2251172991",
    "x": 2013.4064671422634,
    "y": 0.07861872579184848,
    "title": "The New Eye of Government: Citizen Sentiment Analysis in Social Media",
    "authors": [
      "R. Arunachalam",
      "Sandipan Sarkar"
    ],
    "first_author": "R. Arunachalam",
    "first_author_surname": "Arunachalam",
    "year": 2013,
    "cited_by_count": 38,
    "venue": "",
    "size": 20.45418486874747,
    "color": "hsl(78, 70%, 60%)",
    "label": "Arunachalam ,2013",
    "rag_problem": "Governments face challenges in achieving transparency and engagement with citizens due to limited mechanisms for understanding public sentiment effectively.",
    "rag_method": "The authors propose an approach to monitor and analyze citizen sentiment in social media using sentiment analysis techniques.\n\n**Explanation:** By leveraging social media data, the proposed approach enables governments to gain insights into public sentiment in real-time. This allows them to better understand citizen concerns, preferences, and feedback, fostering transparency and engagement. Social media serves as a rich and accessible source of citizen opinions, and sentiment analysis techniques systematically process and interpret this data to provide actionable insights.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W1889043906",
    "x": 1980.0588106145326,
    "y": -0.3799161029498225,
    "title": "The Conflict and Peace Data Bank (COPDAB) Project",
    "authors": [
      "Edward E. Azar"
    ],
    "first_author": "Edward E. Azar",
    "first_author_surname": "Azar",
    "year": 1980,
    "cited_by_count": 300,
    "venue": "",
    "size": 28.293662636151687,
    "color": "hsl(240, 70%, 60%)",
    "label": "Azar ,1980",
    "rag_problem": "Lack of systematic procedures and theories to analyze events leading to war, instability, and international tension, as well as events promoting peace and equitable interdependence.",
    "rag_method": "The Conflict and Peace Data Bank (COPDAB) Project, which aims to systematize observations and improve analytical skills by providing structured data on conflict and peace-related events.\n\n**Explanation:** The COPDAB project addresses the problem by creating a centralized and systematic repository of data related to conflict and peace events. This structured data allows researchers to identify patterns, test theories, and develop better analytical tools for understanding the causes and resolutions of international conflicts and tensions. By organizing observations into a coherent framework, it enhances the ability to study and predict political phenomena effectively.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2099921486",
    "x": 2012.1573949398241,
    "y": -0.4136760214211658,
    "title": "Measuring party positions in Europe",
    "authors": [
      "Ryan Bakker",
      "Catherine E. De Vries",
      "Erica Edwards"
    ],
    "first_author": "Ryan Bakker",
    "first_author_surname": "Bakker",
    "year": 2012,
    "cited_by_count": 829,
    "venue": "",
    "size": 40.23098466224065,
    "color": "hsl(85, 70%, 60%)",
    "label": "Bakker ,2012",
    "rag_problem": "There is a lack of reliable and consistent measures of national party positions on European integration, ideology, and EU/non-EU policies over time.",
    "rag_method": "The authors developed the Chapel Hill expert surveys (CHES) and introduced the CHES trend file, which provides systematic measures of party positioning from 1999 to 2010.\n\n**Explanation:** The CHES trend file aggregates expert judgments on party positions and cross-validates these data with other sources like the Comparative Manifesto Project and the European Elections Studies survey. This ensures reliability and consistency in tracking party positions over time, addressing the need for robust longitudinal data on political trends.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4407572863",
    "x": 2024.6493660495726,
    "y": -1.1834245148738378,
    "title": "Tweet question classification for enhancing Tweet Question Answering System",
    "authors": [
      "Chindukuri Mallikarjuna",
      "S. Sangeetha"
    ],
    "first_author": "Chindukuri Mallikarjuna",
    "first_author_surname": "Mallikarjuna",
    "year": 2025,
    "cited_by_count": 1,
    "venue": "",
    "size": 9.059056829883577,
    "color": "hsl(0, 70%, 60%)",
    "label": "Mallikarjuna ,2025",
    "rag_problem": "The informal and noisy nature of social media texts, such as tweets, makes it challenging to classify questions accurately for Question Answering (QA) systems. Existing general domain question classification datasets and methods are not well-suited for handling the unique characteristics of tweets.",
    "rag_method": "The authors propose a specialized tweet question classification (QC) system tailored for social media QA by annotating questions in the Tweet QA dataset.\n\n**Explanation:** By creating a specialized tweet QC system and annotating questions in the Tweet QA dataset, the solution directly addresses the problem of handling informal and noisy text. This tailored approach ensures that the classification system is trained on data that reflects the unique linguistic and structural characteristics of tweets, thereby improving the accuracy and efficiency of QA systems for social media.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Develop a specialized tweet question classification (QC) system tailored to the informal and noisy nature of social media texts, addressing the unique challenges posed by this domain.\n- Expand the annotated dataset for tweet question classification to improve the robustness and generalizability of the QA system.\n- Explore advanced machine learning models or techniques to enhance the efficiency and accuracy of tweet question classification in social media contexts.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4409203754",
    "x": 2024.545121339901,
    "y": 1.2871463847896223,
    "title": "NER for Albanian Language: A Manually Annotated Corpus and Machine Learning Models",
    "authors": [
      "Nelda Kote",
      "Klea Kalliri",
      "Kejsi Kalliri"
    ],
    "first_author": "Nelda Kote",
    "first_author_surname": "Kote",
    "year": 2025,
    "cited_by_count": 0,
    "venue": "",
    "size": 8,
    "color": "hsl(0, 70%, 60%)",
    "label": "Kote ,2025",
    "rag_problem": "There is a lack of annotated corpora and effective Named Entity Recognition (NER) models for the Albanian language, which hinders the development of NLP tools for this language.",
    "rag_method": "The authors developed a manually annotated corpus specifically for the Albanian language and trained machine learning models for NER tasks using this corpus.\n\n**Explanation:** By creating a manually annotated corpus, the authors provide high-quality training data tailored to the Albanian language, addressing the data scarcity issue. Training machine learning models on this corpus ensures that the models are specifically optimized for the linguistic characteristics of Albanian, improving their performance on NER tasks.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4399304543",
    "x": 2024.5900781683613,
    "y": 7.25906427121116,
    "title": "Improve Question Classification Genetic Algorithm Based Feature Selection and Convolution Neural Network",
    "authors": [
      "Asmaa Ahmed Shama"
    ],
    "first_author": "Asmaa Ahmed Shama",
    "first_author_surname": "Shama",
    "year": 2024,
    "cited_by_count": 0,
    "venue": "",
    "size": 8,
    "color": "hsl(0, 70%, 60%)",
    "label": "Shama ,2024",
    "rag_problem": "Determining the question type in Question Answering Systems (QAS) is challenging and crucial, as it directly impacts subsequent processes like information retrieval and answer selection.",
    "rag_method": "A genetic algorithm-based feature selection combined with a Convolutional Neural Network (CNN) is proposed to improve question classification accuracy.\n\n**Explanation:** The genetic algorithm optimizes feature selection by identifying the most relevant features for question classification, reducing noise and enhancing the input quality for the CNN. The CNN then processes these optimized features to learn hierarchical patterns and representations, improving the model's ability to accurately classify question types. This combination addresses the challenge by ensuring that the classification process is both efficient and precise, which in turn benefits downstream QAS components.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4393621158",
    "x": 2023.5800201317,
    "y": 10.26152120419609,
    "title": "Enhancing Low-Resource Question-Answering Performance Through Word Seeding and Customized Refinement",
    "authors": [
      "Hariom Pandya",
      "Brijesh Bhatt"
    ],
    "first_author": "Hariom Pandya",
    "first_author_surname": "Pandya",
    "year": 2024,
    "cited_by_count": 0,
    "venue": "",
    "size": 8,
    "color": "hsl(0, 70%, 60%)",
    "label": "Pandya ,2024",
    "rag_problem": "Low-resource languages (LRL) face significant challenges in Question-Answering (QA) systems due to the scarcity of labeled training data, making it difficult to achieve high performance without extensive manual annotation.",
    "rag_method": "The authors propose a method that combines word seeding and customized refinement to enhance QA performance in low-resource languages by leveraging existing data and improving syntactic compatibility.\n\n**Explanation:** Word seeding introduces relevant linguistic elements from resource-rich languages (RRL) into the low-resource language context, enabling the transfer of knowledge and reducing dependency on extensive labeled datasets. Customized refinement further optimizes the syntactic and semantic alignment of the transferred data, ensuring compatibility and improving the quality of QA predictions. Together, these mechanisms address the data scarcity problem by efficiently utilizing cross-lingual resources and refining their applicability to LRLs.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate the integration of additional low-resource language datasets to improve the robustness of the proposed QA system, addressing the current limitations in data scarcity.\n- Explore advanced cross-lingual transfer learning techniques to enhance compatibility between resource-rich and low-resource languages, overcoming syntactic differences.\n- Develop automated or semi-automated annotation tools to reduce the manual effort required for creating labeled datasets in low-resource languages.\n- Examine the scalability of the proposed word seeding and refinement methods across a broader range of languages and domains to ensure generalizability.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3137778411",
    "x": 2020.9488153391037,
    "y": 13.77718051299324,
    "title": "Context Transformer with Stacked Pointer Networks for Conversational Question Answering over Knowledge Graphs",
    "authors": [
      "Joan Plepi",
      "Endri Kacupaj",
      "Kuldeep Singh"
    ],
    "first_author": "Joan Plepi",
    "first_author_surname": "Plepi",
    "year": 2021,
    "cited_by_count": 1,
    "venue": "",
    "size": 9.059056829883577,
    "color": "hsl(21, 70%, 60%)",
    "label": "Plepi ,2021",
    "rag_problem": "Conversational question answering over knowledge graphs requires maintaining context across multiple turns, but existing methods struggle with effectively capturing and utilizing conversational context.",
    "rag_method": "The authors propose a Context Transformer with Stacked Pointer Networks, which integrates a transformer-based architecture for context understanding and stacked pointer networks for precise answer selection.\n\n**Explanation:** The Context Transformer is designed to model conversational context by leveraging the transformer architecture, which excels at capturing sequential dependencies and contextual relationships. The Stacked Pointer Networks are used to pinpoint specific answers in the knowledge graph by iteratively refining the selection process. Together, these mechanisms ensure that the conversational context is preserved and utilized effectively, enabling accurate answers to multi-turn questions.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3152067692",
    "x": 2020.829215841813,
    "y": 11.080700434904577,
    "title": "Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks",
    "authors": [
      "Endri Kacupaj",
      "Joan Plepi",
      "Kuldeep Singh"
    ],
    "first_author": "Endri Kacupaj",
    "first_author_surname": "Kacupaj",
    "year": 2021,
    "cited_by_count": 3,
    "venue": "",
    "size": 11.718113659767154,
    "color": "hsl(21, 70%, 60%)",
    "label": "Kacupaj ,2021",
    "rag_problem": "Existing methods for conversational question answering over knowledge graphs struggle to handle complex queries and fail to effectively exploit the structural relationships within the knowledge graph.",
    "rag_method": "LASAGNE integrates a transformer architecture with Graph Attention Networks (GATs) to perform multi-task neural semantic parsing, leveraging the transformer for generating base logical forms and GATs for exploiting correlations between graph entities.\n\n**Explanation:** The transformer component in LASAGNE generates logical forms that represent the structure of the query, while the Graph Attention Networks enhance the model's ability to understand and utilize the relationships between entities in the knowledge graph. This combined approach allows the system to better interpret complex queries and retrieve relevant answers by capturing both semantic and structural aspects of the knowledge graph.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3147450229",
    "x": 2020.8245977528604,
    "y": 2.558212457574821,
    "title": "Question Answering Systems: A Systematic Literature Review",
    "authors": [
      "Sarah Saad Alanazi",
      "Nazar Elfadil",
      "Mutsam A. Jarajreh"
    ],
    "first_author": "Sarah Saad Alanazi",
    "first_author_surname": "Alanazi",
    "year": 2021,
    "cited_by_count": 16,
    "venue": "",
    "size": 17.268795984922043,
    "color": "hsl(21, 70%, 60%)",
    "label": "Alanazi ,2021",
    "rag_problem": "Users need to manually sift through extensive search results to find accurate answers to their natural language questions, which is time-consuming and inefficient.",
    "rag_method": "Development of Question Answering Systems (QAS) that extract precise answers to natural language questions directly from the web or structured data sources.\n\n**Explanation:** QAS are designed to process natural language queries, understand their intent, and retrieve or generate concise answers, eliminating the need for users to navigate through irrelevant or excessive search results. By focusing on extracting the exact answer, QAS streamline the information retrieval process, making it more user-friendly and efficient.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate advanced techniques for improving the accuracy of answer extraction in question answering systems, addressing current limitations in handling complex queries.\n- Explore methods to enhance the adaptability of QAS to diverse domains and languages, as existing systems often struggle with domain-specific or multilingual queries.\n- Develop approaches to integrate more sophisticated natural language understanding capabilities, enabling QAS to better interpret nuanced or ambiguous questions.\n- Conduct experiments to evaluate the scalability of QAS in real-world applications, particularly in handling large-scale datasets and high user demand.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2481450925",
    "x": 2016.3637051466878,
    "y": 3.940473869146361,
    "title": "Arabic Text Question Answering from an Answer Retrieval Point of View: a survey",
    "authors": [
      "A. BODOR",
      "Mohammed A. H. Ali",
      "M. M. Sherif"
    ],
    "first_author": "A. BODOR",
    "first_author_surname": "BODOR",
    "year": 2016,
    "cited_by_count": 2,
    "venue": "",
    "size": 10.614505362651943,
    "color": "hsl(56, 70%, 60%)",
    "label": "BODOR ,2016",
    "rag_problem": "The lack of a comprehensive review and classification of Arabic Question Answering (QA) systems, which hinders the understanding of current methods, their applications, and challenges.",
    "rag_method": "The authors conduct a survey to review state-of-the-art Arabic QA methods, classify them into categories based on answer retrieval approaches, and analyze their applications, issues, and trends.\n\n**Explanation:** By systematically reviewing and categorizing Arabic QA systems, the survey provides a structured understanding of existing methods and their limitations. This classification helps researchers identify gaps and opportunities for improvement, thereby addressing the lack of clarity in the field.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4306412553",
    "x": 2021.7175775777291,
    "y": 7.054355469800084,
    "title": "Question Classification for Albanian Language: An Annotated Corpus and Classification Models",
    "authors": [
      "Nelda Kote",
      "Evis Trandafili",
      "Gjergj Plepi"
    ],
    "first_author": "Nelda Kote",
    "first_author_surname": "Kote",
    "year": 2022,
    "cited_by_count": 2,
    "venue": "",
    "size": 10.614505362651943,
    "color": "hsl(14, 70%, 60%)",
    "label": "Kote ,2022",
    "rag_problem": "There is a lack of annotated corpora and effective classification models specifically tailored for question classification in the Albanian language.",
    "rag_method": "The authors created an annotated corpus for the Albanian language and developed classification models to address question classification tasks.\n\n**Explanation:** The annotated corpus provides structured and labeled data necessary for training machine learning models, enabling the development of effective classifiers for question classification in Albanian. The classification models leverage this corpus to learn patterns and accurately categorize questions, addressing the scarcity of resources and tools for this specific language.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4410104850",
    "x": 2025.1540432424067,
    "y": -5.2286492273775185,
    "title": "The role of HR analytics in driving organizational agility and operational performance: evidence from the construction sector",
    "authors": [
      "Rakesh Naik Vadithe",
      "Nenavath Sreenu",
      "Bikrant Kesari"
    ],
    "first_author": "Rakesh Naik Vadithe",
    "first_author_surname": "Vadithe",
    "year": 2025,
    "cited_by_count": 3,
    "venue": "",
    "size": 11.718113659767154,
    "color": "hsl(0, 70%, 60%)",
    "label": "Vadithe ,2025",
    "rag_problem": "Organizations in the construction sector struggle to achieve agility and operational performance due to inefficient utilization of HR data and lack of advanced HR analytics capabilities.",
    "rag_method": "Implementation of HR analytics as a mediating tool to leverage human resource information systems (HRIS) and human resource big data (HRBD) for improved decision-making and organizational outcomes.\n\n**Explanation:** HR analytics acts as a bridge between HR technology (HRIS and HRBD) and organizational goals. By analyzing HR data effectively, it provides actionable insights that enhance organizational agility and operational performance. This mechanism enables organizations to respond quickly to changes and optimize operations by making data-driven decisions.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate the application of HR analytics in other industries beyond the construction sector to understand its broader impact on organizational agility and operational performance. This would provide comparative insights and validate the findings in diverse contexts.\n- Explore the integration of advanced technologies, such as artificial intelligence and machine learning, within HR analytics to enhance decision-making processes and predictive capabilities.\n- Conduct longitudinal studies to examine the long-term effects of HR analytics on organizational agility and operational performance, addressing potential temporal variations and sustainability of outcomes.\n- Analyze the role of cultural and regional differences in the adoption and effectiveness of HR analytics, particularly in global or multi-national organizations, to identify context-specific strategies.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W7119097770",
    "x": 2025.9265774391433,
    "y": -0.20296602368520156,
    "title": "Performance analysis of Lean Six Sigma 4.0 using neutrosophic DEMATEL approach: a case study",
    "authors": [
      "R. Vigneshvaran",
      "S. Vinodh"
    ],
    "first_author": "R. Vigneshvaran",
    "first_author_surname": "Vigneshvaran",
    "year": 2026,
    "cited_by_count": 0,
    "venue": "",
    "size": 8,
    "color": "hsl(0, 70%, 60%)",
    "label": "Vigneshvaran ,2026",
    "rag_problem": "Performance measurement in Lean Six Sigma 4.0 (LSS4.0) implementations is challenging due to the involvement of both quantifiable and non-quantifiable factors, making it difficult to accurately assess and improve organizational performance.",
    "rag_method": "The use of the neutrosophic DEMATEL (Decision-Making Trial and Evaluation Laboratory) approach to analyze the overall performance index score of LSS4.0.\n\n**Explanation:** The neutrosophic DEMATEL approach allows for the handling of uncertainty and imprecision in decision-making processes by incorporating neutrosophic logic. This method evaluates the interrelationships between multiple criteria (both quantifiable and non-quantifiable) and determines their influence on the overall performance index. By doing so, it provides a structured and comprehensive framework to measure and analyze the performance of LSS4.0 implementations, addressing the complexity of performance assessment.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Explore the application of the neutrosophic DEMATEL approach in other industries beyond automotive manufacturing to validate its versatility and effectiveness in different contexts.\n- Investigate the integration of additional advanced Industry 4.0 technologies with Lean Six Sigma 4.0 to enhance performance measurement and decision-making processes.\n- Develop more comprehensive frameworks that incorporate both quantifiable and non-quantifiable factors to improve the accuracy and reliability of performance analysis.\n- Conduct longitudinal studies to assess the long-term impact of Lean Six Sigma 4.0 implementation on organizational performance and sustainability.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4409185491",
    "x": 2025.1135984555526,
    "y": 4.654912747135644,
    "title": "Evaluating Turkish Manufacturers' Perspectives on Industry 4.0, Circular Economy, and Corporate Performance",
    "authors": [
      "Ä°brahim Sarper KarakadÄ±lar"
    ],
    "first_author": "Ä°brahim Sarper KarakadÄ±lar",
    "first_author_surname": "KarakadÄ±lar",
    "year": 2025,
    "cited_by_count": 0,
    "venue": "",
    "size": 8,
    "color": "hsl(0, 70%, 60%)",
    "label": "KarakadÄ±lar ,2025",
    "rag_problem": "Turkish manufacturing managers lack a comprehensive understanding of how Industry 4.0 and circular economy principles can improve supply chain sustainability and corporate performance.",
    "rag_method": "The authors propose a research framework to evaluate executives' attitudes towards Industry 4.0 and circular economy approaches, using statistical analysis of survey data to identify perceptions and gaps.\n\n**Explanation:** By collecting and analyzing data from 126 valid responses, the research framework provides insights into the current attitudes and understanding of managers. This helps to identify areas where awareness and knowledge need to be improved, thereby addressing the lack of understanding and enabling targeted strategies to enhance supply chain sustainability and corporate performance.",
    "rag_limitation": "- The study relies on self-reported data collected through an online questionnaire, which may introduce biases such as social desirability or misinterpretation of questions by participants.\n- The sample size of 126 valid responses may limit the generalizability of the findings to the broader population of Turkish manufacturers.\n- The research framework focuses on executives' attitudes, which might not fully capture the operational realities or practical challenges faced in implementing Industry 4.0 and circular economy practices.",
    "rag_future_work": "- Investigate the integration of Industry 4.0 technologies with circular economy practices in specific manufacturing sectors to provide sector-specific insights and strategies.\n- Expand the sample size and diversity of respondents to include more manufacturers and industries for a broader understanding of perceptions and impacts.\n- Explore longitudinal studies to assess the long-term effects of Industry 4.0 and circular economy adoption on corporate performance and sustainability.\n- Develop qualitative research methods, such as interviews or case studies, to complement quantitative findings and provide deeper insights into managerial attitudes and challenges.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2985262935",
    "x": 2018.529486915299,
    "y": 5.187999945502123,
    "title": "Hybrid Integrations of Value Stream Mapping, Theory of Constraints and Simulation: Application to Wooden Furniture Industry",
    "authors": [
      "Emad Alzubi",
      "Anas M. Atieh",
      "Khaleel Abu Shgair"
    ],
    "first_author": "Emad Alzubi",
    "first_author_surname": "Alzubi",
    "year": 2019,
    "cited_by_count": 31,
    "venue": "",
    "size": 19.69528414941788,
    "color": "hsl(35, 70%, 60%)",
    "label": "Alzubi ,2019",
    "rag_problem": "The wooden furniture manufacturing company suffers from long manufacturing lead times and an unbalanced production line, leading to inefficiencies and delays.",
    "rag_method": "The integration of Value Stream Mapping (VSM), Theory of Constraints (TOC), and discrete event simulation is proposed to identify sources of waste and delays, optimize processes, and improve production flow.\n\n**Explanation:** Value Stream Mapping (VSM) is used to visualize and analyze the major processes, providing quantifiable KPIs such as manufacturing lead-time and Overall Equipment Effectiveness (OEE). The Theory of Constraints (TOC) identifies bottlenecks in the production line, focusing on the most critical constraints that limit throughput. Discrete event simulation models the production processes to test and validate improvements in a virtual environment before implementation. Together, these methods systematically address inefficiencies by identifying waste, optimizing constraints, and simulating solutions, thereby reducing lead times and balancing the production line.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate the integration of additional lean tools: Future work could explore the incorporation of other lean manufacturing tools alongside VSM, TOC, and simulation to further optimize production processes in the wooden furniture industry.\n- Expand application to other industries: The methodology could be tested and adapted for use in other manufacturing sectors to evaluate its generalizability and effectiveness beyond the wooden furniture industry.\n- Enhance simulation model complexity: Future research could focus on developing more detailed and complex simulation models to capture additional variables and dynamics of the production process.\n- Address real-time implementation: Efforts could be directed towards implementing the proposed hybrid approach in real-time production environments to assess its practical feasibility and impact on operational efficiency.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W1699664671",
    "x": 2014.9985127995428,
    "y": 4.1742599385400325,
    "title": "A value stream mapping and simulation hybrid approach: application to glass industry",
    "authors": [
      "Anas M. Atieh",
      "Hazem Kaylani",
      "Ahmad Almuhtady"
    ],
    "first_author": "Anas M. Atieh",
    "first_author_surname": "Atieh",
    "year": 2015,
    "cited_by_count": 49,
    "venue": "",
    "size": 21.407334348301177,
    "color": "hsl(64, 70%, 60%)",
    "label": "Atieh ,2015",
    "rag_problem": "The glass industry faces inefficiencies in production processes, including waste, delays, and suboptimal resource utilization, which hinder operational performance and profitability.",
    "rag_method": "A hybrid approach combining Value Stream Mapping (VSM) and simulation techniques to analyze and optimize production processes.\n\n**Explanation:** Value Stream Mapping (VSM) provides a visual representation of the current production workflow, identifying inefficiencies and bottlenecks. Simulation techniques complement VSM by enabling dynamic modeling and testing of process improvements without disrupting actual operations. Together, these methods allow for a comprehensive analysis and optimization of workflows, reducing waste, improving resource allocation, and enhancing overall efficiency in the glass industry.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3191336279",
    "x": 2021.1790018273975,
    "y": 5.259662835945657,
    "title": "Integration of the buyerâ€“supplier interface for Global sourcing",
    "authors": [
      "Lydia Bals",
      "Virpi Turkulainen"
    ],
    "first_author": "Lydia Bals",
    "first_author_surname": "Bals",
    "year": 2021,
    "cited_by_count": 11,
    "venue": "",
    "size": 15.932619022419093,
    "color": "hsl(21, 70%, 60%)",
    "label": "Bals ,2021",
    "rag_problem": "Global sourcing requires firms to redesign procurement organizations and determine what functions to centralize versus manage locally, while also addressing challenges in managing the buyerâ€“supplier interface effectively.",
    "rag_method": "The study proposes an organization design approach to integrate the buyerâ€“supplier interface specifically for global sourcing, focusing on structural and operational alignment.\n\n**Explanation:** By adopting an organization design approach, the solution provides a framework for firms to systematically address the structural and operational challenges posed by global sourcing. This includes decisions on centralization versus localization and aligning buyerâ€“supplier interactions to ensure smooth communication and collaboration across global operations. The integration of the buyerâ€“supplier interface ensures that sourcing activities are streamlined and aligned with organizational goals, reducing inefficiencies and fostering better supplier relationships.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate specific strategies for redesigning procurement organizations to optimize the integration of the buyerâ€“supplier interface in global sourcing. This could include exploring the balance between centralization and local management in different organizational contexts.\n- Explore the development and implementation of advanced tools or frameworks to enhance the integration of buyerâ€“supplier interfaces, focusing on improving communication, collaboration, and efficiency in global sourcing operations.\n- Conduct empirical studies to assess the impact of different organizational design choices on the effectiveness of buyerâ€“supplier integration in diverse industries and global markets. This would provide data-driven insights to refine integration practices.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2047655553",
    "x": 2009.756513515952,
    "y": 3.3053612607979415,
    "title": "Qualitative case studies in operations management: Trends, research outcomes, and future research implications",
    "authors": [
      "Mark Barratt",
      "Thomas Y. Choi",
      "Mei Li"
    ],
    "first_author": "Mark Barratt",
    "first_author_surname": "Barratt",
    "year": 2010,
    "cited_by_count": 1238,
    "venue": "",
    "size": 42.15212977528654,
    "color": "hsl(99, 70%, 60%)",
    "label": "Barratt ,2010",
    "rag_problem": "Lack of understanding about the state, trends, and research outcomes of qualitative case studies in operations management.",
    "rag_method": "Conducting a systematic examination of qualitative case studies published in five major operations management journals between 1992 and 2007.\n\n**Explanation:** By analyzing qualitative case studies from influential journals over a 15-year period, the study identifies trends, evaluates research outcomes, and provides insights into the evolution and impact of qualitative case studies in the field of operations management. This systematic review addresses the lack of clarity by offering structured data and analysis on the subject.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3174421755",
    "x": 2020.9424833945898,
    "y": -0.00031796571331077494,
    "title": "Key factors for operational performance in manufacturing systems: Conceptual model, systematic literature review and implications",
    "authors": [
      "Marcelo Battesini",
      "Carla Schwengber ten Caten",
      "Diego Augusto de JesÃºs Pacheco"
    ],
    "first_author": "Marcelo Battesini",
    "first_author_surname": "Battesini",
    "year": 2021,
    "cited_by_count": 36,
    "venue": "",
    "size": 20.25223255183564,
    "color": "hsl(21, 70%, 60%)",
    "label": "Battesini ,2021",
    "rag_problem": "Manufacturing systems face challenges in identifying and integrating the key factors that influence operational performance, leading to inefficiencies and suboptimal outcomes.",
    "rag_method": "The authors propose a conceptual model and conduct a systematic literature review to identify and categorize the key factors influencing operational performance in manufacturing systems.\n\n**Explanation:** By developing a conceptual model and systematically reviewing existing literature, the authors provide a structured framework that highlights the critical factors affecting operational performance. This approach helps organizations understand and prioritize these factors, enabling targeted improvements and more efficient decision-making processes in manufacturing systems.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  }
];
                const edgesData = [
  {
    "from": "W4205802268",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4205802268",
    "to": "W2165612380",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4205802268",
    "to": "W2118020653",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3122397014",
    "to": "W3082051346",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3122397014",
    "to": "W3082307550",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W4205802268",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W2083078026",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W3133702157",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W3183428091",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "è¿ç§»/åº”ç”¨ - Bå°†Açš„æ–¹æ³•åº”ç”¨åˆ°æ–°é¢†åŸŸï¼ˆæ¨ªå‘æ‰©æ•£ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W4244669226",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W4241903662",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W4211028722",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W4205802268",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W4238374879",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W2117002298",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W2251172991",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W1889043906",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W2099921486",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4362571217",
    "to": "W3122397014",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4362571217",
    "to": "W3137778411",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4362571217",
    "to": "W3152067692",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4362571217",
    "to": "W3147450229",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4362571217",
    "to": "W2481450925",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4362571217",
    "to": "W4306412553",
    "type": "Extends",
    "color": "#2ECC71",
    "original_color": "#2ECC71",
    "width": 2.0,
    "dash": "solid",
    "description": "æ–¹æ³•æ‰©å±• - Båœ¨Açš„æ–¹æ³•åŸºç¡€ä¸Šåšå¢é‡æ”¹è¿›ï¼ˆå¾®åˆ›æ–°ï¼‰"
  },
  {
    "from": "W4386328785",
    "to": "W3122397014",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4386328785",
    "to": "W2985262935",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4386328785",
    "to": "W1699664671",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4386328785",
    "to": "W3191336279",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4386328785",
    "to": "W2047655553",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4386328785",
    "to": "W3174421755",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2041578073",
    "to": "W3175985315",
    "type": "Realizes",
    "color": "#9B59B6",
    "original_color": "#9B59B6",
    "width": 2.5,
    "dash": "solid",
    "description": "å®ç°æ„¿æ™¯ - Bå®ç°äº†Açš„æœªæ¥å·¥ä½œå»ºè®®ï¼ˆç§‘ç ”ä¼ æ‰¿ï¼‰"
  },
  {
    "from": "W2041578073",
    "to": "W2126512988",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2041578073",
    "to": "W2168353148",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2041578073",
    "to": "W2039483660",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2041578073",
    "to": "W1986845215",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2165612380",
    "to": "W2075006521",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W2165612380",
    "to": "W2144211451",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W2165612380",
    "to": "W1908696901",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W2165612380",
    "to": "W2568360633",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W3082051346",
    "to": "W3009926341",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3082051346",
    "to": "W2120109270",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3082051346",
    "to": "W3106505254",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3082051346",
    "to": "W2772492596",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3082051346",
    "to": "W4247191450",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3082307550",
    "to": "W3009951436",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3082307550",
    "to": "W3002108456",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3082307550",
    "to": "W3001118548",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3082307550",
    "to": "W3014892682",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3082307550",
    "to": "W3003465021",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2118020653",
    "to": "W2165612380",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W168564468",
    "to": "W2165612380",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W1662133657",
    "to": "W2165612380",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "è¿ç§»/åº”ç”¨ - Bå°†Açš„æ–¹æ³•åº”ç”¨åˆ°æ–°é¢†åŸŸï¼ˆæ¨ªå‘æ‰©æ•£ï¼‰"
  },
  {
    "from": "W2045108252",
    "to": "W2165612380",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2301363727",
    "to": "W2165612380",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2075006521",
    "to": "W2144211451",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W3181361218",
    "to": "W3082051346",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3092399442",
    "to": "W3082051346",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3092399442",
    "to": "W3106505254",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4229045617",
    "to": "W3082051346",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3129344682",
    "to": "W3082051346",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3198704192",
    "to": "W3082051346",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2914159168",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4211028722",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2517695692",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2517695692",
    "to": "W2025173440",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2911800777",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2025173440",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3106637466",
    "to": "W3082307550",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4313494085",
    "to": "W3082307550",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "è¿ç§»/åº”ç”¨ - Bå°†Açš„æ–¹æ³•åº”ç”¨åˆ°æ–°é¢†åŸŸï¼ˆæ¨ªå‘æ‰©æ•£ï¼‰"
  },
  {
    "from": "W3161985147",
    "to": "W3082307550",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3161985147",
    "to": "W3082051346",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W4206541507",
    "to": "W3082307550",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4206541507",
    "to": "W3106637466",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4322743452",
    "to": "W3082307550",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "è¿ç§»/åº”ç”¨ - Bå°†Açš„æ–¹æ³•åº”ç”¨åˆ°æ–°é¢†åŸŸï¼ˆæ¨ªå‘æ‰©æ•£ï¼‰"
  },
  {
    "from": "W3009951436",
    "to": "W3002108456",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W3009951436",
    "to": "W3001118548",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W3009951436",
    "to": "W3003465021",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3002108456",
    "to": "W3001118548",
    "type": "Extends",
    "color": "#2ECC71",
    "original_color": "#2ECC71",
    "width": 2.0,
    "dash": "solid",
    "description": "æ–¹æ³•æ‰©å±• - Båœ¨Açš„æ–¹æ³•åŸºç¡€ä¸Šåšå¢é‡æ”¹è¿›ï¼ˆå¾®åˆ›æ–°ï¼‰"
  },
  {
    "from": "W3014892682",
    "to": "W3002108456",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3014892682",
    "to": "W3001118548",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3003465021",
    "to": "W3001118548",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W3003465021",
    "to": "W3002108456",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4381332452",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4390583680",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4389456104",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4399213274",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4394684629",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4403637392",
    "to": "W4317823603",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4402418067",
    "to": "W4317823603",
    "type": "Alternative",
    "color": "#E67E22",
    "original_color": "#E67E22",
    "width": 2.0,
    "dash": "dot",
    "description": "å¦è¾Ÿè¹Šå¾„ - Bç”¨å®Œå…¨ä¸åŒçš„èŒƒå¼è§£å†³é—®é¢˜ï¼ˆé¢ è¦†åˆ›æ–°ï¼‰"
  },
  {
    "from": "W4394828653",
    "to": "W4317823603",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4393097350",
    "to": "W4317823603",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4367397709",
    "to": "W4317823603",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4407572863",
    "to": "W4362571217",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4409203754",
    "to": "W4362571217",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4399304543",
    "to": "W4362571217",
    "type": "Alternative",
    "color": "#E67E22",
    "original_color": "#E67E22",
    "width": 2.0,
    "dash": "dot",
    "description": "å¦è¾Ÿè¹Šå¾„ - Bç”¨å®Œå…¨ä¸åŒçš„èŒƒå¼è§£å†³é—®é¢˜ï¼ˆé¢ è¦†åˆ›æ–°ï¼‰"
  },
  {
    "from": "W4393621158",
    "to": "W4362571217",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W4410104850",
    "to": "W4386328785",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W7119097770",
    "to": "W4386328785",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4409185491",
    "to": "W4386328785",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  }
];

                // Deep Surveyæ•°æ®
                const deepSurveyData = {
  "topic": "natural language processing",
  "timestamp": "2026-01-14T16:34:05.390372",
  "pruning_stats": {
    "original_papers": 86,
    "pruned_papers": 16,
    "removed_papers": 70,
    "pruning_mode": "comprehensive",
    "strong_components_count": 4,
    "components_with_seed": 0,
    "largest_component_size": 6,
    "seed_papers": 2,
    "original_edges": 99,
    "strong_edges": 14,
    "weak_edges_removed": 3,
    "retention_rate": 0.18604651162790697,
    "relation_type_distribution": {
      "Extends": 2,
      "Overcomes": 8,
      "Baselines": 3,
      "Adapts_to": 3,
      "Alternative": 1
    }
  },
  "evolutionary_paths": [
    {
      "thread_type": "convergence",
      "pattern_type": "The Convergence (æ±‡èšæ¨¡å¼)",
      "title": "å¤šæŠ€æœ¯è·¯çº¿æ±‡èšåˆ° The authors conducted a systematic review and meta-analysis to aggregate and ana",
      "narrative": "**èƒŒæ™¯**: åœ¨2020å¹´ä¹‹å‰ï¼Œè¯¥é¢†åŸŸå­˜åœ¨å¤šä¸ªç‹¬ç«‹çš„ç ”ç©¶æ–¹å‘ã€‚**è·¯çº¿1** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿2** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ã€‚è¿™äº›æ–¹å‘å„è‡ªä¸ºæ”¿ï¼Œç¼ºä¹ç³»ç»Ÿæ€§æ•´åˆã€‚\n\n**æ±‡èš**: è®ºæ–‡ã€ŠClinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysisã€‹(2020) åœ¨æ­¤èƒŒæ™¯ä¸‹åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡ The authors conducted a systematic review and meta-analysis to aggregate and ana å°†è¿™ 2 æ¡ç‹¬ç«‹è·¯çº¿æœ‰æœºæ•´åˆï¼Œå½¢æˆäº†ç»Ÿä¸€çš„æŠ€æœ¯æ¡†æ¶æ¥è§£å†³ There is a lack of consolidated and systematic understanding of the clinical, laã€‚\n\n**æ„ä¹‰**: è¿™ç§å¤šæ–¹å‘æ±‡èšä¸ºè¯¥é¢†åŸŸæä¾›äº†ç†è®ºæ•´åˆçš„èŒƒä¾‹ï¼Œä½¿å¾—åŸæœ¬åˆ†æ•£çš„æŠ€æœ¯è·¯çº¿å¾—ä»¥ååŒå‘æŒ¥ä½œç”¨ï¼Œæ¨åŠ¨äº†é¢†åŸŸçš„ç†è®ºç»Ÿä¸€å’Œå®è·µæ·±åŒ–ã€‚",
      "center_paper": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
      "routes_count": 2,
      "routes": [
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W3002108456",
              "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
              "year": 2020,
              "cited_by_count": 22489
            },
            {
              "paper_id": "W3001118548",
              "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
              "year": 2020,
              "cited_by_count": 51227
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W3001118548",
              "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
              "year": 2020,
              "cited_by_count": 51227
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W3009951436",
          "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
          "year": 2020,
          "cited_by_count": 2477,
          "role": "center"
        },
        {
          "paper_id": "W3002108456",
          "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
          "year": 2020,
          "cited_by_count": 22489
        },
        {
          "paper_id": "W3001118548",
          "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
          "year": 2020,
          "cited_by_count": 51227
        },
        {
          "paper_id": "W3001118548",
          "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
          "year": 2020,
          "cited_by_count": 51227
        }
      ],
      "total_citations": 127420,
      "visual_structure": "2 Routes -> Center",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W3002108456",
            "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
            "year": 2020
          },
          "to_paper": {
            "id": "W3009951436",
            "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
            "year": 2020
          },
          "relation_type": "Overcomes",
          "narrative_relation": "è¢«Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysisæ•´åˆ",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W3001118548",
            "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
            "year": 2020
          },
          "to_paper": {
            "id": "W3009951436",
            "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
            "year": 2020
          },
          "relation_type": "Overcomes",
          "narrative_relation": "è¢«Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysisæ•´åˆ",
          "route_id": 2,
          "direction": "chronological"
        }
      ],
      "component_id": 130223326670592
    },
    {
      "thread_type": "divergence",
      "pattern_type": "The Divergence (åˆ†åŒ–æ¨¡å¼)",
      "title": "é’ˆå¯¹ Lack of understanding of clinical features and progression of 2019 novel coronav çš„å¤šæŠ€æœ¯è·¯çº¿åšå¼ˆ",
      "narrative": "**ç„¦ç‚¹**: è®ºæ–‡ã€ŠClinical features of patients infected with 2019 novel coronavirus in Wuhan, Chinaã€‹(2020) æ˜¯è¯¥é¢†åŸŸçš„åŸºçŸ³å·¥ä½œï¼Œå®ƒèšç„¦äº Lack of understanding of clinical features and progression of 2019 novel coronavï¼Œä½†ç•™ä¸‹äº† æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° çš„é—®é¢˜ã€‚\n\n**åˆ†æ­§**: å­¦æœ¯ç•Œå¯¹æ­¤äº§ç”Ÿäº†ä¸åŒçš„æ¼”è¿›è·¯çº¿ã€‚**è·¯çº¿1** (çºµå‘æ·±åŒ–): é’ˆå¯¹ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œé€šè¿‡ The authors conducted a systematic review and meta-analysis to aggregate and ana å®ç°çªç ´ï¼›**è·¯çº¿2** (å¾®åˆ›æ–°): ä¿ç•™æ ¸å¿ƒæ¶æ„å¹¶æ‰©å±•ï¼Œé‡‡ç”¨ The authors conducted a systematic review and meta-analysis to aggregate and anaã€‚\n\n**å¯¹æ¯”**: è¿™äº›è·¯çº¿å…±åŒæ¨åŠ¨äº†è¯¥é¢†åŸŸç—›ç‚¹é—®é¢˜çš„è§£å†³ï¼Œå½¢æˆäº†å¤šè§’åº¦æ”»å…‹çš„å±€é¢ã€‚ï¼ˆè¯¦è§å„è·¯çº¿è®ºæ–‡çš„æ€§èƒ½å¯¹æ¯”ï¼‰",
      "center_paper": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
      "routes_count": 2,
      "routes": [
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W3009951436",
              "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
              "year": 2020,
              "cited_by_count": 2477
            }
          ]
        },
        {
          "relation_type": "Extends",
          "papers": [
            {
              "paper_id": "W3002108456",
              "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
              "year": 2020,
              "cited_by_count": 22489
            },
            {
              "paper_id": "W3009951436",
              "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
              "year": 2020,
              "cited_by_count": 2477
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W3001118548",
          "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
          "year": 2020,
          "cited_by_count": 51227,
          "role": "center"
        },
        {
          "paper_id": "W3009951436",
          "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
          "year": 2020,
          "cited_by_count": 2477
        },
        {
          "paper_id": "W3002108456",
          "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
          "year": 2020,
          "cited_by_count": 22489
        },
        {
          "paper_id": "W3009951436",
          "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
          "year": 2020,
          "cited_by_count": 2477
        }
      ],
      "total_citations": 78670,
      "visual_structure": "Center -> 2 Routes",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W3001118548",
            "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
            "year": 2020
          },
          "to_paper": {
            "id": "W3009951436",
            "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
            "year": 2020
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W3001118548",
            "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
            "year": 2020
          },
          "to_paper": {
            "id": "W3002108456",
            "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
            "year": 2020
          },
          "relation_type": "Extends",
          "narrative_relation": "Was_Extended_By",
          "route_id": 2,
          "direction": "chronological"
        }
      ],
      "component_id": 130223326670592
    },
    {
      "thread_type": "convergence",
      "pattern_type": "The Convergence (æ±‡èšæ¨¡å¼)",
      "title": "å¤šæŠ€æœ¯è·¯çº¿æ±‡èšåˆ° The authors propose a vector space model for automatic indexing, where entities ",
      "narrative": "**èƒŒæ™¯**: åœ¨1975å¹´ä¹‹å‰ï¼Œè¯¥é¢†åŸŸå­˜åœ¨å¤šä¸ªç‹¬ç«‹çš„ç ”ç©¶æ–¹å‘ã€‚**è·¯çº¿1** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿2** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿3** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿4** (å…‹æœå‹): è¯†åˆ«äº†ã€Œ- The method relies heavily on medium frequency terms and skewed frequency distrã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ã€‚è¿™äº›æ–¹å‘å„è‡ªä¸ºæ”¿ï¼Œç¼ºä¹ç³»ç»Ÿæ€§æ•´åˆã€‚\n\n**æ±‡èš**: è®ºæ–‡ã€ŠA vector space model for automatic indexingã€‹(1975) åœ¨æ­¤èƒŒæ™¯ä¸‹åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡ The authors propose a vector space model for automatic indexing, where entities  å°†è¿™ 4 æ¡ç‹¬ç«‹è·¯çº¿æœ‰æœºæ•´åˆï¼Œå½¢æˆäº†ç»Ÿä¸€çš„æŠ€æœ¯æ¡†æ¶æ¥è§£å†³ In document retrieval systems, stored entities (documents) are often compared wiã€‚\n\n**æ„ä¹‰**: è¿™ç§å¤šæ–¹å‘æ±‡èšæ ‡å¿—ç€è¯¥é¢†åŸŸä»æ¢ç´¢é˜¶æ®µè¿ˆå…¥ç³»ç»ŸåŒ–é˜¶æ®µï¼Œä½¿å¾—åŸæœ¬åˆ†æ•£çš„æŠ€æœ¯è·¯çº¿å¾—ä»¥ååŒå‘æŒ¥ä½œç”¨ï¼Œæ¨åŠ¨äº†é¢†åŸŸçš„ç†è®ºç»Ÿä¸€å’Œå®è·µæ·±åŒ–ã€‚",
      "center_paper": "A vector space model for automatic indexing",
      "routes_count": 4,
      "routes": [
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W2075006521",
              "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
              "year": 1973,
              "cited_by_count": 571
            },
            {
              "paper_id": "W2144211451",
              "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
              "year": 1972,
              "cited_by_count": 4313
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W2144211451",
              "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
              "year": 1972,
              "cited_by_count": 4313
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W1908696901",
              "title": "A Theory of Indexing",
              "year": 1975,
              "cited_by_count": 120
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W2568360633",
              "title": "Contribution to the Theory of Indexing",
              "year": 1973,
              "cited_by_count": 17
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W2165612380",
          "title": "A vector space model for automatic indexing",
          "year": 1975,
          "cited_by_count": 7329,
          "role": "center"
        },
        {
          "paper_id": "W2075006521",
          "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
          "year": 1973,
          "cited_by_count": 571
        },
        {
          "paper_id": "W2144211451",
          "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
          "year": 1972,
          "cited_by_count": 4313
        },
        {
          "paper_id": "W2144211451",
          "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
          "year": 1972,
          "cited_by_count": 4313
        },
        {
          "paper_id": "W1908696901",
          "title": "A Theory of Indexing",
          "year": 1975,
          "cited_by_count": 120
        },
        {
          "paper_id": "W2568360633",
          "title": "Contribution to the Theory of Indexing",
          "year": 1973,
          "cited_by_count": 17
        }
      ],
      "total_citations": 16663,
      "visual_structure": "4 Routes -> Center",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2075006521",
            "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
            "year": 1973
          },
          "to_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "relation_type": "Overcomes",
          "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2144211451",
            "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
            "year": 1972
          },
          "to_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "relation_type": "Overcomes",
          "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
          "route_id": 2,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W1908696901",
            "title": "A Theory of Indexing",
            "year": 1975
          },
          "to_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "relation_type": "Overcomes",
          "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
          "route_id": 3,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2568360633",
            "title": "Contribution to the Theory of Indexing",
            "year": 1973
          },
          "to_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "relation_type": "Overcomes",
          "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
          "route_id": 4,
          "direction": "chronological"
        }
      ],
      "component_id": 130223330780800
    },
    {
      "thread_type": "chain",
      "pattern_type": "The Chain (çº¿æ€§é“¾æ¡)",
      "title": "ä» The authors propose interpreting term specificity statistically, as a function o åˆ° Vector Space Models (VSMs) of semantics are introduced as a method to represent  çš„æ¼”è¿›ä¹‹è·¯",
      "narrative": "**èµ·æº** (1972å¹´): è®ºæ–‡ã€ŠA STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVALã€‹é¦–æ¬¡æå‡ºäº† The authors propose interpreting term specificity statistically, as a function o æ¥è§£å†³ Term specificity in information retrieval is traditionally interpreted based on ï¼Œå¼€åˆ›äº†è¿™ä¸€ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œè¯¥å·¥ä½œåœ¨ æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚\n\n**å…‹æœå±€é™** (1975å¹´): é’ˆå¯¹å‰äººå·¥ä½œåœ¨ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€æ–¹é¢çš„ä¸è¶³ï¼Œè®ºæ–‡ã€ŠA vector space model for automatic indexingã€‹é€šè¿‡ The authors propose a vector space model for automatic indexing, where entities  å®ç°äº†çªç ´æ€§æ”¹è¿›ã€‚\n\n**è·¨åŸŸè¿ç§»** (2010å¹´): è®ºæ–‡ã€ŠFrom Frequency to Meaning: Vector Space Models of Semanticsã€‹å°†å‰äººåœ¨ã€ŒåŸé¢†åŸŸã€çš„æŠ€æœ¯æˆåŠŸè¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé€šè¿‡ Vector Space Models (VSMs) of semantics are introduced as a method to represent  éªŒè¯äº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "papers": [
        {
          "paper_id": "W2144211451",
          "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
          "year": 1972,
          "cited_by_count": 4313
        },
        {
          "paper_id": "W2165612380",
          "title": "A vector space model for automatic indexing",
          "year": 1975,
          "cited_by_count": 7329
        },
        {
          "paper_id": "W1662133657",
          "title": "From Frequency to Meaning: Vector Space Models of Semantics",
          "year": 2010,
          "cited_by_count": 2831
        }
      ],
      "total_citations": 14473,
      "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2144211451",
            "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
            "year": 1972
          },
          "to_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "to_paper": {
            "id": "W1662133657",
            "title": "From Frequency to Meaning: Vector Space Models of Semantics",
            "year": 2010
          },
          "relation_type": "Adapts_to",
          "narrative_relation": "Was_Adapted_By",
          "direction": "chronological"
        }
      ],
      "component_id": 130223330780800
    },
    {
      "thread_type": "chain",
      "pattern_type": "The Chain (çº¿æ€§é“¾æ¡)",
      "title": "ä» New techniques are introduced for assigning weights to index terms based on the  åˆ° Vector Space Models (VSMs) of semantics are introduced as a method to represent  çš„æ¼”è¿›ä¹‹è·¯",
      "narrative": "**èµ·æº** (1973å¹´): è®ºæ–‡ã€ŠON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXINGã€‹é¦–æ¬¡æå‡ºäº† New techniques are introduced for assigning weights to index terms based on the  æ¥è§£å†³ Standard theories for specifying term values (or weights) in automatic indexing ï¼Œå¼€åˆ›äº†è¿™ä¸€ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œè¯¥å·¥ä½œåœ¨ æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚\n\n**å…‹æœå±€é™** (1975å¹´): é’ˆå¯¹å‰äººå·¥ä½œåœ¨ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€æ–¹é¢çš„ä¸è¶³ï¼Œè®ºæ–‡ã€ŠA vector space model for automatic indexingã€‹é€šè¿‡ The authors propose a vector space model for automatic indexing, where entities  å®ç°äº†çªç ´æ€§æ”¹è¿›ã€‚\n\n**è·¨åŸŸè¿ç§»** (2010å¹´): è®ºæ–‡ã€ŠFrom Frequency to Meaning: Vector Space Models of Semanticsã€‹å°†å‰äººåœ¨ã€ŒåŸé¢†åŸŸã€çš„æŠ€æœ¯æˆåŠŸè¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé€šè¿‡ Vector Space Models (VSMs) of semantics are introduced as a method to represent  éªŒè¯äº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "papers": [
        {
          "paper_id": "W2075006521",
          "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
          "year": 1973,
          "cited_by_count": 571
        },
        {
          "paper_id": "W2165612380",
          "title": "A vector space model for automatic indexing",
          "year": 1975,
          "cited_by_count": 7329
        },
        {
          "paper_id": "W1662133657",
          "title": "From Frequency to Meaning: Vector Space Models of Semantics",
          "year": 2010,
          "cited_by_count": 2831
        }
      ],
      "total_citations": 10731,
      "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2075006521",
            "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
            "year": 1973
          },
          "to_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "to_paper": {
            "id": "W1662133657",
            "title": "From Frequency to Meaning: Vector Space Models of Semantics",
            "year": 2010
          },
          "relation_type": "Adapts_to",
          "narrative_relation": "Was_Adapted_By",
          "direction": "chronological"
        }
      ],
      "component_id": 130223330780800
    }
  ],
  "survey_report": {
    "title": "Deep Survey: natural language processing",
    "abstract": "æœ¬ç»¼è¿°åŸºäºçŸ¥è¯†å›¾è°±åˆ†æäº† natural language processing é¢†åŸŸçš„æ¼”è¿›å†ç¨‹ã€‚é€šè¿‡å…³ç³»å‰ªæï¼Œæˆ‘ä»¬ä»åŸå§‹å›¾è°±ä¸­ç­›é€‰å‡º 16 ç¯‡é«˜è´¨é‡è®ºæ–‡ï¼Œå¹¶è¯†åˆ«å‡º 5 æ¡å…³é”®æ¼”åŒ–è·¯å¾„ã€‚å…¶ä¸­åŒ…æ‹¬ 2 æ¡çº¿æ€§æŠ€æœ¯é“¾æ¡ã€1 ä¸ªåˆ†åŒ–ç»“æ„å’Œ 2 ä¸ªæ±‡èšç»“æ„ï¼Œå®Œæ•´å‘ˆç°äº†è¯¥é¢†åŸŸçš„æŠ€æœ¯æ¼”è¿›è„‰ç»œã€åˆ†åŒ–è¶‹åŠ¿å’Œæ•´åˆæ¨¡å¼ã€‚",
    "threads": [
      {
        "thread_id": 1,
        "thread_name": "Thread 1: The Convergence (æ±‡èšæ¨¡å¼)",
        "title": "å¤šæŠ€æœ¯è·¯çº¿æ±‡èšåˆ° The authors conducted a systematic review and meta-analysis to aggregate and ana",
        "pattern_type": "The Convergence (æ±‡èšæ¨¡å¼)",
        "thread_type": "convergence",
        "narrative": "**èƒŒæ™¯**: åœ¨2020å¹´ä¹‹å‰ï¼Œè¯¥é¢†åŸŸå­˜åœ¨å¤šä¸ªç‹¬ç«‹çš„ç ”ç©¶æ–¹å‘ã€‚**è·¯çº¿1** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿2** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ã€‚è¿™äº›æ–¹å‘å„è‡ªä¸ºæ”¿ï¼Œç¼ºä¹ç³»ç»Ÿæ€§æ•´åˆã€‚\n\n**æ±‡èš**: è®ºæ–‡ã€ŠClinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysisã€‹(2020) åœ¨æ­¤èƒŒæ™¯ä¸‹åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡ The authors conducted a systematic review and meta-analysis to aggregate and ana å°†è¿™ 2 æ¡ç‹¬ç«‹è·¯çº¿æœ‰æœºæ•´åˆï¼Œå½¢æˆäº†ç»Ÿä¸€çš„æŠ€æœ¯æ¡†æ¶æ¥è§£å†³ There is a lack of consolidated and systematic understanding of the clinical, laã€‚\n\n**æ„ä¹‰**: è¿™ç§å¤šæ–¹å‘æ±‡èšä¸ºè¯¥é¢†åŸŸæä¾›äº†ç†è®ºæ•´åˆçš„èŒƒä¾‹ï¼Œä½¿å¾—åŸæœ¬åˆ†æ•£çš„æŠ€æœ¯è·¯çº¿å¾—ä»¥ååŒå‘æŒ¥ä½œç”¨ï¼Œæ¨åŠ¨äº†é¢†åŸŸçš„ç†è®ºç»Ÿä¸€å’Œå®è·µæ·±åŒ–ã€‚",
        "papers": [
          {
            "paper_id": "W3009951436",
            "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
            "year": 2020,
            "cited_by_count": 2477,
            "role": "center"
          },
          {
            "paper_id": "W3002108456",
            "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
            "year": 2020,
            "cited_by_count": 22489
          },
          {
            "paper_id": "W3001118548",
            "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
            "year": 2020,
            "cited_by_count": 51227
          },
          {
            "paper_id": "W3001118548",
            "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
            "year": 2020,
            "cited_by_count": 51227
          }
        ],
        "total_citations": 127420,
        "visual_structure": "2 Routes -> Center",
        "relation_stats": {
          "total_relations": 2,
          "relation_distribution": {
            "Overcomes": 2
          },
          "dominant_relation": "Overcomes"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W3002108456",
              "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
              "year": 2020
            },
            "to_paper": {
              "id": "W3009951436",
              "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
              "year": 2020
            },
            "relation_type": "Overcomes",
            "narrative_relation": "è¢«Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysisæ•´åˆ",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W3001118548",
              "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
              "year": 2020
            },
            "to_paper": {
              "id": "W3009951436",
              "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
              "year": 2020
            },
            "relation_type": "Overcomes",
            "narrative_relation": "è¢«Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysisæ•´åˆ",
            "route_id": 2,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W3009951436",
              "label": "Center",
              "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
              "year": 2020,
              "citations": 2477,
              "role": "center"
            },
            {
              "id": "W3002108456",
              "label": "Epidemiological and clinical c...",
              "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
              "year": 2020,
              "citations": 22489
            },
            {
              "id": "W3001118548",
              "label": "Clinical features of patients ...",
              "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
              "year": 2020,
              "citations": 51227
            },
            {
              "id": "W3001118548",
              "label": "Clinical features of patients ...",
              "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
              "year": 2020,
              "citations": 51227
            }
          ],
          "edges": [
            {
              "source": "W3002108456",
              "target": "W3009951436",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W3001118548",
              "target": "W3009951436",
              "type": "Overcomes",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "ç®­å¤´æ–¹å‘è¡¨ç¤ºæ—¶é—´æ¼”è¿›æ–¹å‘ï¼ˆæ—©å¹´ä»½ â†’ æ™šå¹´ä»½ï¼‰",
          "pattern_note": "divergence=ä¸­å¿ƒæ‰©æ•£, convergence=å¤šæºæ±‡èš"
        }
      },
      {
        "thread_id": 2,
        "thread_name": "Thread 2: The Divergence (åˆ†åŒ–æ¨¡å¼)",
        "title": "é’ˆå¯¹ Lack of understanding of clinical features and progression of 2019 novel coronav çš„å¤šæŠ€æœ¯è·¯çº¿åšå¼ˆ",
        "pattern_type": "The Divergence (åˆ†åŒ–æ¨¡å¼)",
        "thread_type": "divergence",
        "narrative": "**ç„¦ç‚¹**: è®ºæ–‡ã€ŠClinical features of patients infected with 2019 novel coronavirus in Wuhan, Chinaã€‹(2020) æ˜¯è¯¥é¢†åŸŸçš„åŸºçŸ³å·¥ä½œï¼Œå®ƒèšç„¦äº Lack of understanding of clinical features and progression of 2019 novel coronavï¼Œä½†ç•™ä¸‹äº† æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° çš„é—®é¢˜ã€‚\n\n**åˆ†æ­§**: å­¦æœ¯ç•Œå¯¹æ­¤äº§ç”Ÿäº†ä¸åŒçš„æ¼”è¿›è·¯çº¿ã€‚**è·¯çº¿1** (çºµå‘æ·±åŒ–): é’ˆå¯¹ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œé€šè¿‡ The authors conducted a systematic review and meta-analysis to aggregate and ana å®ç°çªç ´ï¼›**è·¯çº¿2** (å¾®åˆ›æ–°): ä¿ç•™æ ¸å¿ƒæ¶æ„å¹¶æ‰©å±•ï¼Œé‡‡ç”¨ The authors conducted a systematic review and meta-analysis to aggregate and anaã€‚\n\n**å¯¹æ¯”**: è¿™äº›è·¯çº¿å…±åŒæ¨åŠ¨äº†è¯¥é¢†åŸŸç—›ç‚¹é—®é¢˜çš„è§£å†³ï¼Œå½¢æˆäº†å¤šè§’åº¦æ”»å…‹çš„å±€é¢ã€‚ï¼ˆè¯¦è§å„è·¯çº¿è®ºæ–‡çš„æ€§èƒ½å¯¹æ¯”ï¼‰",
        "papers": [
          {
            "paper_id": "W3001118548",
            "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
            "year": 2020,
            "cited_by_count": 51227,
            "role": "center"
          },
          {
            "paper_id": "W3009951436",
            "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
            "year": 2020,
            "cited_by_count": 2477
          },
          {
            "paper_id": "W3002108456",
            "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
            "year": 2020,
            "cited_by_count": 22489
          },
          {
            "paper_id": "W3009951436",
            "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
            "year": 2020,
            "cited_by_count": 2477
          }
        ],
        "total_citations": 78670,
        "visual_structure": "Center -> 2 Routes",
        "relation_stats": {
          "total_relations": 2,
          "relation_distribution": {
            "Overcomes": 1,
            "Extends": 1
          },
          "dominant_relation": "Overcomes"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W3001118548",
              "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
              "year": 2020
            },
            "to_paper": {
              "id": "W3009951436",
              "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
              "year": 2020
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W3001118548",
              "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
              "year": 2020
            },
            "to_paper": {
              "id": "W3002108456",
              "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
              "year": 2020
            },
            "relation_type": "Extends",
            "narrative_relation": "Was_Extended_By",
            "route_id": 2,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W3001118548",
              "label": "Center",
              "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
              "year": 2020,
              "citations": 51227,
              "role": "center"
            },
            {
              "id": "W3009951436",
              "label": "Clinical, laboratory and imagi...",
              "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
              "year": 2020,
              "citations": 2477
            },
            {
              "id": "W3002108456",
              "label": "Epidemiological and clinical c...",
              "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study",
              "year": 2020,
              "citations": 22489
            },
            {
              "id": "W3009951436",
              "label": "Clinical, laboratory and imagi...",
              "title": "Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis",
              "year": 2020,
              "citations": 2477
            }
          ],
          "edges": [
            {
              "source": "W3001118548",
              "target": "W3009951436",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W3001118548",
              "target": "W3002108456",
              "type": "Extends",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "ç®­å¤´æ–¹å‘è¡¨ç¤ºæ—¶é—´æ¼”è¿›æ–¹å‘ï¼ˆæ—©å¹´ä»½ â†’ æ™šå¹´ä»½ï¼‰",
          "pattern_note": "divergence=ä¸­å¿ƒæ‰©æ•£, convergence=å¤šæºæ±‡èš"
        }
      },
      {
        "thread_id": 3,
        "thread_name": "Thread 3: The Convergence (æ±‡èšæ¨¡å¼)",
        "title": "å¤šæŠ€æœ¯è·¯çº¿æ±‡èšåˆ° The authors propose a vector space model for automatic indexing, where entities ",
        "pattern_type": "The Convergence (æ±‡èšæ¨¡å¼)",
        "thread_type": "convergence",
        "narrative": "**èƒŒæ™¯**: åœ¨1975å¹´ä¹‹å‰ï¼Œè¯¥é¢†åŸŸå­˜åœ¨å¤šä¸ªç‹¬ç«‹çš„ç ”ç©¶æ–¹å‘ã€‚**è·¯çº¿1** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿2** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿3** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿4** (å…‹æœå‹): è¯†åˆ«äº†ã€Œ- The method relies heavily on medium frequency terms and skewed frequency distrã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ã€‚è¿™äº›æ–¹å‘å„è‡ªä¸ºæ”¿ï¼Œç¼ºä¹ç³»ç»Ÿæ€§æ•´åˆã€‚\n\n**æ±‡èš**: è®ºæ–‡ã€ŠA vector space model for automatic indexingã€‹(1975) åœ¨æ­¤èƒŒæ™¯ä¸‹åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡ The authors propose a vector space model for automatic indexing, where entities  å°†è¿™ 4 æ¡ç‹¬ç«‹è·¯çº¿æœ‰æœºæ•´åˆï¼Œå½¢æˆäº†ç»Ÿä¸€çš„æŠ€æœ¯æ¡†æ¶æ¥è§£å†³ In document retrieval systems, stored entities (documents) are often compared wiã€‚\n\n**æ„ä¹‰**: è¿™ç§å¤šæ–¹å‘æ±‡èšæ ‡å¿—ç€è¯¥é¢†åŸŸä»æ¢ç´¢é˜¶æ®µè¿ˆå…¥ç³»ç»ŸåŒ–é˜¶æ®µï¼Œä½¿å¾—åŸæœ¬åˆ†æ•£çš„æŠ€æœ¯è·¯çº¿å¾—ä»¥ååŒå‘æŒ¥ä½œç”¨ï¼Œæ¨åŠ¨äº†é¢†åŸŸçš„ç†è®ºç»Ÿä¸€å’Œå®è·µæ·±åŒ–ã€‚",
        "papers": [
          {
            "paper_id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975,
            "cited_by_count": 7329,
            "role": "center"
          },
          {
            "paper_id": "W2075006521",
            "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
            "year": 1973,
            "cited_by_count": 571
          },
          {
            "paper_id": "W2144211451",
            "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
            "year": 1972,
            "cited_by_count": 4313
          },
          {
            "paper_id": "W2144211451",
            "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
            "year": 1972,
            "cited_by_count": 4313
          },
          {
            "paper_id": "W1908696901",
            "title": "A Theory of Indexing",
            "year": 1975,
            "cited_by_count": 120
          },
          {
            "paper_id": "W2568360633",
            "title": "Contribution to the Theory of Indexing",
            "year": 1973,
            "cited_by_count": 17
          }
        ],
        "total_citations": 16663,
        "visual_structure": "4 Routes -> Center",
        "relation_stats": {
          "total_relations": 4,
          "relation_distribution": {
            "Overcomes": 4
          },
          "dominant_relation": "Overcomes"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2075006521",
              "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
              "year": 1973
            },
            "to_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "relation_type": "Overcomes",
            "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2144211451",
              "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
              "year": 1972
            },
            "to_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "relation_type": "Overcomes",
            "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
            "route_id": 2,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W1908696901",
              "title": "A Theory of Indexing",
              "year": 1975
            },
            "to_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "relation_type": "Overcomes",
            "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
            "route_id": 3,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2568360633",
              "title": "Contribution to the Theory of Indexing",
              "year": 1973
            },
            "to_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "relation_type": "Overcomes",
            "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
            "route_id": 4,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2165612380",
              "label": "Center",
              "title": "A vector space model for automatic indexing",
              "year": 1975,
              "citations": 7329,
              "role": "center"
            },
            {
              "id": "W2075006521",
              "label": "ON THE SPECIFICATION OF TERM V...",
              "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
              "year": 1973,
              "citations": 571
            },
            {
              "id": "W2144211451",
              "label": "A STATISTICAL INTERPRETATION O...",
              "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
              "year": 1972,
              "citations": 4313
            },
            {
              "id": "W2144211451",
              "label": "A STATISTICAL INTERPRETATION O...",
              "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
              "year": 1972,
              "citations": 4313
            },
            {
              "id": "W1908696901",
              "label": "A Theory of Indexing...",
              "title": "A Theory of Indexing",
              "year": 1975,
              "citations": 120
            },
            {
              "id": "W2568360633",
              "label": "Contribution to the Theory of ...",
              "title": "Contribution to the Theory of Indexing",
              "year": 1973,
              "citations": 17
            }
          ],
          "edges": [
            {
              "source": "W2075006521",
              "target": "W2165612380",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W2144211451",
              "target": "W2165612380",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W1908696901",
              "target": "W2165612380",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W2568360633",
              "target": "W2165612380",
              "type": "Overcomes",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "ç®­å¤´æ–¹å‘è¡¨ç¤ºæ—¶é—´æ¼”è¿›æ–¹å‘ï¼ˆæ—©å¹´ä»½ â†’ æ™šå¹´ä»½ï¼‰",
          "pattern_note": "divergence=ä¸­å¿ƒæ‰©æ•£, convergence=å¤šæºæ±‡èš"
        }
      },
      {
        "thread_id": 4,
        "thread_name": "Thread 4: The Chain (çº¿æ€§é“¾æ¡)",
        "title": "ä» The authors propose interpreting term specificity statistically, as a function o åˆ° Vector Space Models (VSMs) of semantics are introduced as a method to represent  çš„æ¼”è¿›ä¹‹è·¯",
        "pattern_type": "The Chain (çº¿æ€§é“¾æ¡)",
        "thread_type": "chain",
        "narrative": "**èµ·æº** (1972å¹´): è®ºæ–‡ã€ŠA STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVALã€‹é¦–æ¬¡æå‡ºäº† The authors propose interpreting term specificity statistically, as a function o æ¥è§£å†³ Term specificity in information retrieval is traditionally interpreted based on ï¼Œå¼€åˆ›äº†è¿™ä¸€ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œè¯¥å·¥ä½œåœ¨ æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚\n\n**å…‹æœå±€é™** (1975å¹´): é’ˆå¯¹å‰äººå·¥ä½œåœ¨ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€æ–¹é¢çš„ä¸è¶³ï¼Œè®ºæ–‡ã€ŠA vector space model for automatic indexingã€‹é€šè¿‡ The authors propose a vector space model for automatic indexing, where entities  å®ç°äº†çªç ´æ€§æ”¹è¿›ã€‚\n\n**è·¨åŸŸè¿ç§»** (2010å¹´): è®ºæ–‡ã€ŠFrom Frequency to Meaning: Vector Space Models of Semanticsã€‹å°†å‰äººåœ¨ã€ŒåŸé¢†åŸŸã€çš„æŠ€æœ¯æˆåŠŸè¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé€šè¿‡ Vector Space Models (VSMs) of semantics are introduced as a method to represent  éªŒè¯äº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚",
        "papers": [
          {
            "paper_id": "W2144211451",
            "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
            "year": 1972,
            "cited_by_count": 4313
          },
          {
            "paper_id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975,
            "cited_by_count": 7329
          },
          {
            "paper_id": "W1662133657",
            "title": "From Frequency to Meaning: Vector Space Models of Semantics",
            "year": 2010,
            "cited_by_count": 2831
          }
        ],
        "total_citations": 14473,
        "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
        "relation_stats": {
          "total_relations": 0,
          "relation_distribution": {},
          "dominant_relation": "Unknown"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2144211451",
              "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
              "year": 1972
            },
            "to_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "to_paper": {
              "id": "W1662133657",
              "title": "From Frequency to Meaning: Vector Space Models of Semantics",
              "year": 2010
            },
            "relation_type": "Adapts_to",
            "narrative_relation": "Was_Adapted_By",
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2144211451",
              "label": "Paper 1",
              "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
              "year": 1972,
              "citations": 4313
            },
            {
              "id": "W2165612380",
              "label": "Paper 2",
              "title": "A vector space model for automatic indexing",
              "year": 1975,
              "citations": 7329
            },
            {
              "id": "W1662133657",
              "label": "Paper 3",
              "title": "From Frequency to Meaning: Vector Space Models of Semantics",
              "year": 2010,
              "citations": 2831
            }
          ],
          "edges": [
            {
              "source": "W2144211451",
              "target": "W2165612380",
              "type": "chronological_evolution",
              "label": "1972 â†’ 1975"
            },
            {
              "source": "W2165612380",
              "target": "W1662133657",
              "type": "chronological_evolution",
              "label": "1975 â†’ 2010"
            }
          ],
          "layout": "hierarchical",
          "direction_note": "ç®­å¤´æ–¹å‘è¡¨ç¤ºæ—¶é—´æ¼”è¿›æ–¹å‘ï¼ˆæ—©å¹´ä»½ â†’ æ™šå¹´ä»½ï¼‰",
          "pattern_note": ""
        }
      },
      {
        "thread_id": 5,
        "thread_name": "Thread 5: The Chain (çº¿æ€§é“¾æ¡)",
        "title": "ä» New techniques are introduced for assigning weights to index terms based on the  åˆ° Vector Space Models (VSMs) of semantics are introduced as a method to represent  çš„æ¼”è¿›ä¹‹è·¯",
        "pattern_type": "The Chain (çº¿æ€§é“¾æ¡)",
        "thread_type": "chain",
        "narrative": "**èµ·æº** (1973å¹´): è®ºæ–‡ã€ŠON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXINGã€‹é¦–æ¬¡æå‡ºäº† New techniques are introduced for assigning weights to index terms based on the  æ¥è§£å†³ Standard theories for specifying term values (or weights) in automatic indexing ï¼Œå¼€åˆ›äº†è¿™ä¸€ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œè¯¥å·¥ä½œåœ¨ æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚\n\n**å…‹æœå±€é™** (1975å¹´): é’ˆå¯¹å‰äººå·¥ä½œåœ¨ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€æ–¹é¢çš„ä¸è¶³ï¼Œè®ºæ–‡ã€ŠA vector space model for automatic indexingã€‹é€šè¿‡ The authors propose a vector space model for automatic indexing, where entities  å®ç°äº†çªç ´æ€§æ”¹è¿›ã€‚\n\n**è·¨åŸŸè¿ç§»** (2010å¹´): è®ºæ–‡ã€ŠFrom Frequency to Meaning: Vector Space Models of Semanticsã€‹å°†å‰äººåœ¨ã€ŒåŸé¢†åŸŸã€çš„æŠ€æœ¯æˆåŠŸè¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé€šè¿‡ Vector Space Models (VSMs) of semantics are introduced as a method to represent  éªŒè¯äº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚",
        "papers": [
          {
            "paper_id": "W2075006521",
            "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
            "year": 1973,
            "cited_by_count": 571
          },
          {
            "paper_id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975,
            "cited_by_count": 7329
          },
          {
            "paper_id": "W1662133657",
            "title": "From Frequency to Meaning: Vector Space Models of Semantics",
            "year": 2010,
            "cited_by_count": 2831
          }
        ],
        "total_citations": 10731,
        "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
        "relation_stats": {
          "total_relations": 0,
          "relation_distribution": {},
          "dominant_relation": "Unknown"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2075006521",
              "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
              "year": 1973
            },
            "to_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "to_paper": {
              "id": "W1662133657",
              "title": "From Frequency to Meaning: Vector Space Models of Semantics",
              "year": 2010
            },
            "relation_type": "Adapts_to",
            "narrative_relation": "Was_Adapted_By",
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2075006521",
              "label": "Paper 1",
              "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
              "year": 1973,
              "citations": 571
            },
            {
              "id": "W2165612380",
              "label": "Paper 2",
              "title": "A vector space model for automatic indexing",
              "year": 1975,
              "citations": 7329
            },
            {
              "id": "W1662133657",
              "label": "Paper 3",
              "title": "From Frequency to Meaning: Vector Space Models of Semantics",
              "year": 2010,
              "citations": 2831
            }
          ],
          "edges": [
            {
              "source": "W2075006521",
              "target": "W2165612380",
              "type": "chronological_evolution",
              "label": "1973 â†’ 1975"
            },
            {
              "source": "W2165612380",
              "target": "W1662133657",
              "type": "chronological_evolution",
              "label": "1975 â†’ 2010"
            }
          ],
          "layout": "hierarchical",
          "direction_note": "ç®­å¤´æ–¹å‘è¡¨ç¤ºæ—¶é—´æ¼”è¿›æ–¹å‘ï¼ˆæ—©å¹´ä»½ â†’ æ™šå¹´ä»½ï¼‰",
          "pattern_note": ""
        }
      }
    ],
    "metadata": {
      "total_papers_analyzed": 86,
      "papers_after_pruning": 16,
      "total_threads": 5,
      "generation_date": "2026-01-14T16:34:05.390302"
    }
  },
  "summary": {
    "original_papers": 86,
    "pruned_papers": 16,
    "total_threads": 5
  }
};

                // Research Ideasæ•°æ®
                const researchIdeasData = {
  "topic": "natural language processing",
  "total_ideas": 10,
  "successful_ideas": 10,
  "ideas": [
    {
      "limitation": "- Investigate how crowdsourcing and online collaborative translations can reshape existing translation theories, providing a deeper understanding of their impact on the discipline.\n- Explore the implications of these translational processes on public perceptions of translation, aiming to assess their broader societal and cultural influence.\n- Advance research in the \"technological turn\" within Translation Studies, focusing on the integration of technology in collaborative translation practices.",
      "method": "The authors propose using text mining techniques combined with latent Dirichlet allocation (LDA) to systematically analyze and summarize the vast body of literature.\n\n**Explanation:** Text mining techniques enable automated processing and extraction of information from large datasets, while latent Dirichlet allocation (LDA) is a topic modeling algorithm that identifies latent topics within text data. By applying these methods, the authors can categorize and summarize the literature, making it easier for researchers to identify patterns, trends, and relevant insights without manually reviewing each document. This reduces the cognitive burden and enhances the efficiency of literature review processes.",
      "status": "SUCCESS",
      "title": "Integrating Text Mining and Topic Modeling to Analyze Collaborative Translation Practices: A Technological Turn in Translation Studies",
      "abstract": "Background: The emergence of crowdsourcing and online collaborative translation has challenged traditional translation theories, necessitating a deeper understanding of their societal, cultural, and disciplinary impacts. However, the vast and fragmented body of literature on these topics presents a significant barrier to systematic analysis. Gap: Current research lacks a consolidated framework to analyze and summarize this literature efficiently, particularly in the context of the 'technological turn' in Translation Studies. Proposed Method: We propose a novel integration of text mining techniques with latent Dirichlet allocation (LDA) to systematically analyze the literature on collaborative translation practices. By leveraging automated text processing and topic modeling, this method identifies latent themes, trends, and patterns across large datasets, enabling a comprehensive understanding of the field. Expected Result: This approach will provide a structured, data-driven framework for exploring the implications of collaborative translation on public perceptions, societal impacts, and theoretical advancements, thereby advancing the technological turn in Translation Studies.",
      "modification": "Adapt LDA to focus on domain-specific terminologies and integrate sentiment analysis to assess public perceptions of translation practices.",
      "reasoning": "Step 1: Analyze Compatibility - The candidate method, combining text mining and LDA, is inherently suitable for analyzing large datasets of textual information. Its mathematical and algorithmic properties, such as probabilistic topic modeling and automated text processing, align well with the need to systematically analyze the fragmented literature on collaborative translation. The method's computational complexity is manageable within the scope of current technological capabilities, and its assumptions (e.g., the presence of latent topics in text data) are compatible with the nature of academic and public discourse on translation. Thus, the method is fundamentally compatible. Step 2: Identify the Gap - While the method is suitable for summarizing and categorizing literature, it does not inherently address domain-specific nuances of translation studies or the societal and cultural implications of collaborative translation. The bridging variable is the adaptation of LDA to focus on domain-specific terminologies and the integration of sentiment analysis to assess public perceptions. This modification ensures the method captures both the theoretical and societal dimensions of the research limitation. Step 3: Draft the Idea - Drawing on the evolutionary patterns of convergence in research (e.g., integrating multiple methodologies to address fragmented fields), the proposed idea integrates text mining and LDA to create a unified framework for analyzing collaborative translation practices. The core innovation lies in adapting LDA for domain-specific analysis and combining it with sentiment analysis to address the broader implications of collaborative translation.",
      "rationale": "The reasoning follows a structured chain of thought: (1) The limitation requires a systematic approach to analyze the fragmented literature on collaborative translation and its societal impacts. (2) The candidate method, text mining with LDA, is compatible due to its ability to process large textual datasets and identify latent topics. (3) The gap lies in the method's lack of domain-specific focus and inability to assess societal implications directly. The bridging variable is the adaptation of LDA to include domain-specific terminologies and the integration of sentiment analysis. (4) Drawing on the evolutionary pattern of convergence, where multiple methodologies are integrated to address fragmented research fields, the proposed idea combines text mining, domain-specific LDA, and sentiment analysis to create a comprehensive framework. This approach not only addresses the theoretical aspects of collaborative translation but also explores its societal and cultural impacts, advancing the technological turn in Translation Studies. The decision chain and evidence from evolutionary paths support the feasibility and relevance of this idea."
    },
    {
      "limitation": "- Investigate the transmission dynamics of 2019-nCoV, including modes of spread and factors influencing transmission rates, to better inform public health interventions.\n- Develop and evaluate diagnostic tools for earlier and more precise detection of 2019-nCoV, especially in asymptomatic or mildly symptomatic cases.\n- Study the progression and clinical management of 2019-nCoV infections to improve treatment protocols and patient outcomes.\n- Explore the effectiveness of coordinated response strategies between healthcare systems and public health authorities to enhance preparedness for future outbreaks.",
      "method": "The authors propose using text mining techniques combined with latent Dirichlet allocation (LDA) to systematically analyze and summarize the vast body of literature.\n\n**Explanation:** Text mining techniques enable automated processing and extraction of information from large datasets, while latent Dirichlet allocation (LDA) is a topic modeling algorithm that identifies latent topics within text data. By applying these methods, the authors can categorize and summarize the literature, making it easier for researchers to identify patterns, trends, and relevant insights without manually reviewing each document. This reduces the cognitive burden and enhances the efficiency of literature review processes.",
      "status": "SUCCESS",
      "title": "Leveraging Text Mining and Topic Modeling to Accelerate Insights into 2019-nCoV Transmission, Diagnostics, and Clinical Management",
      "abstract": "Background: The COVID-19 pandemic has generated an overwhelming volume of scientific literature, making it challenging for researchers and policymakers to extract actionable insights on transmission dynamics, diagnostic tools, clinical management, and coordinated response strategies. Gap: Existing methods for literature review are manual and time-intensive, limiting the ability to synthesize knowledge rapidly during a public health crisis. Proposed Method: We propose a novel framework that integrates text mining techniques with latent Dirichlet allocation (LDA) to systematically analyze and summarize the vast body of COVID-19 literature. By categorizing research into key thematic areas such as transmission dynamics, diagnostics, and clinical management, this method enables the identification of emerging trends, critical gaps, and actionable insights. Expected Result: This approach will significantly reduce the cognitive burden on researchers, enhance the efficiency of literature synthesis, and provide a robust foundation for evidence-based public health interventions and future outbreak preparedness.",
      "modification": "Adapt the LDA model to incorporate domain-specific ontologies and metadata, such as MeSH terms or COVID-19-specific keywords, to improve topic coherence and relevance to the research bottleneck.",
      "reasoning": "Step 1: Analyze Compatibility - Text mining and LDA are well-suited for processing and summarizing large volumes of unstructured text data, such as scientific literature. The method's computational complexity is manageable with modern hardware, and its applicability domain aligns with the need to synthesize COVID-19-related research. However, vanilla LDA may struggle with domain-specific nuances, requiring adaptation to ensure relevance to the specific research bottlenecks. Step 2: Identify the Gap - The primary gap lies in the need for domain-specific customization of LDA to capture the unique terminology and structure of COVID-19 literature. The bridging variable is the incorporation of domain-specific ontologies and metadata, which will enhance the model's ability to identify coherent and relevant topics. Step 3: Draft the Idea - The proposed framework builds on the strengths of text mining and LDA while addressing their limitations through domain-specific adaptations. This innovation ensures that the method is not only compatible but also highly effective in addressing the identified research bottlenecks.",
      "rationale": "The rationale for this idea follows a structured chain of reasoning: (1) The limitation involves synthesizing vast and rapidly growing COVID-19 literature to inform public health interventions, diagnostics, and clinical management. (2) Text mining and LDA are compatible with this task, as they are designed for automated processing and topic modeling of large text datasets. However, standard LDA lacks the specificity needed to address domain-specific challenges. (3) To bridge this gap, the method must be adapted to incorporate domain-specific ontologies and metadata, ensuring that the topics generated are coherent and relevant to COVID-19 research. (4) Drawing on evolutionary patterns, such as the convergence of multiple research directions in systematic reviews, this idea integrates and extends existing methods to create a unified framework for literature synthesis. The proposed framework is expected to accelerate the extraction of actionable insights, reduce cognitive burden, and support evidence-based decision-making during the ongoing pandemic and future outbreaks."
    },
    {
      "limitation": "- Explore more comprehensive statistical models for interpreting term specificity, focusing on additional factors beyond term frequency to enhance retrieval performance.\n- Investigate the relationship between term specificity and term meaning to determine how semantic aspects could complement statistical interpretations in retrieval systems.\n- Conduct experiments with larger and more diverse test collections to validate the findings and assess the generalizability of the proposed approach.\n- Develop advanced term weighting schemes that integrate the proposed statistical interpretation of term specificity to optimize retrieval effectiveness.",
      "method": "The authors propose a vector space model for automatic indexing, where entities are represented in a high-dimensional space designed to maximize the distance between them.\n\n**Explanation:** By representing documents and search requests as vectors in a high-dimensional space, the model ensures that entities are positioned as far apart as possible. This reduces the density of the object space and minimizes overlap between entities, improving the precision and efficiency of retrieval by making it easier to distinguish between documents and match them to relevant search requests.",
      "status": "SUCCESS",
      "title": "Integrating Semantic and Statistical Interpretations of Term Specificity in High-Dimensional Vector Spaces",
      "abstract": "Background: Current retrieval systems rely heavily on term frequency-based statistical models to interpret term specificity, often neglecting semantic aspects that could enhance retrieval performance. Gap: While the vector space model for automatic indexing provides a robust framework for distinguishing entities in high-dimensional spaces, it does not inherently incorporate semantic interpretations of term specificity or address the integration of statistical and semantic factors. Proposed Method: This study proposes an enhanced vector space model that integrates semantic embeddings with statistical measures of term specificity. By combining semantic vectors derived from pre-trained language models with statistical term weighting schemes, the method aims to position terms in a high-dimensional space that reflects both their statistical relevance and semantic meaning. Expected Result: The proposed approach is expected to improve retrieval performance by providing a more comprehensive representation of term specificity, validated through experiments on diverse and large-scale test collections.",
      "modification": "Incorporate semantic embeddings into the vector space model and combine them with statistical term weighting schemes to represent term specificity comprehensively.",
      "reasoning": "Step 1: Compatibility Analysis: The vector space model for automatic indexing is fundamentally compatible with the limitation's requirements. Its high-dimensional representation aligns well with the need to model term specificity. However, the current model focuses solely on statistical separation and does not address the integration of semantic aspects or the development of advanced term weighting schemes. Step 2: Gap Identification: The primary gap lies in the lack of semantic interpretation in the vector space model. To bridge this, the model needs to incorporate semantic embeddings derived from pre-trained language models and combine them with statistical measures of term specificity. The bridging variable is the integration of semantic and statistical representations within the high-dimensional space. Step 3: Idea Drafting: The proposed method builds on the vector space model by integrating semantic embeddings with statistical term weighting schemes. This innovation leverages the strengths of both approaches to provide a more comprehensive interpretation of term specificity, addressing the limitation effectively.",
      "rationale": "The reasoning process began with a detailed compatibility analysis of the vector space model for automatic indexing against the stated limitation. The model's high-dimensional representation is suitable for modeling term specificity, but it lacks semantic interpretation and advanced term weighting schemes. Drawing on the evolutionary context, particularly the convergence pattern observed in similar research, the solution involves integrating semantic embeddings with statistical measures. This approach combines the strengths of semantic and statistical interpretations, creating a unified framework that addresses the limitation comprehensively. The proposed method is expected to enhance retrieval performance by providing a more nuanced representation of term specificity, validated through experiments on diverse test collections. The complete reasoning chain, from limitation to solution, demonstrates the feasibility and innovation of the proposed approach."
    },
    {
      "limitation": "- Investigate the use of machine translation as a pedagogical tool in different EFL contexts to better understand its broader applicability and effectiveness in diverse learning environments.\n- Explore the impact of machine translation on other language skills beyond writing, such as speaking, listening, and reading, to provide a more comprehensive view of its role in language learning.\n- Conduct longitudinal studies to examine the long-term effects of using machine translation on EFL students' language development and writing proficiency.\n- Analyze the effectiveness of different instructional designs involving machine translation, such as comparing post-editing tasks with other MT-based activities, to identify best practices for its integration in EFL classrooms.",
      "method": "The authors propose using text mining techniques combined with latent Dirichlet allocation (LDA) to systematically analyze and summarize the vast body of literature.\n\n**Explanation:** Text mining techniques enable automated processing and extraction of information from large datasets, while latent Dirichlet allocation (LDA) is a topic modeling algorithm that identifies latent topics within text data. By applying these methods, the authors can categorize and summarize the literature, making it easier for researchers to identify patterns, trends, and relevant insights without manually reviewing each document. This reduces the cognitive burden and enhances the efficiency of literature review processes.",
      "status": "SUCCESS",
      "title": "Integrating Text Mining and Topic Modeling to Advance Machine Translation Pedagogy in EFL Contexts",
      "abstract": "Background: The use of machine translation (MT) as a pedagogical tool in English as a Foreign Language (EFL) contexts has gained attention, but its broader applicability, impact on diverse language skills, and long-term effects remain underexplored. Additionally, the effectiveness of different instructional designs involving MT is not well understood. Gap: Current research lacks a systematic synthesis of the vast and fragmented literature on MT's pedagogical applications, making it challenging to identify trends, best practices, and research gaps. Proposed Method: We propose leveraging text mining techniques combined with latent Dirichlet allocation (LDA) to systematically analyze and categorize the existing body of literature on MT in EFL contexts. By identifying latent topics, trends, and patterns, this method will provide a comprehensive and data-driven foundation for future research. Expected Result: This approach will enable researchers to uncover insights into MT's impact on various language skills, evaluate instructional designs, and identify longitudinal research opportunities, thereby advancing the integration of MT in EFL education.",
      "modification": "Adapt the text mining and LDA framework to focus on pedagogical research literature, incorporating domain-specific preprocessing (e.g., identifying EFL-related terms) and tailoring the topic modeling to highlight instructional design, skill development, and longitudinal study themes.",
      "reasoning": "The reasoning process involves three steps: (1) Compatibility Analysis: Text mining and LDA are compatible with the limitation as they are well-suited for processing large, fragmented datasets, such as the literature on MT in EFL contexts. These methods can identify latent patterns and summarize trends, aligning with the need to synthesize diverse studies. (2) Gap Identification: The primary gap lies in adapting these methods to the specific domain of EFL pedagogy. This requires domain-specific preprocessing (e.g., recognizing EFL-related terms) and tailoring the LDA model to focus on key themes like instructional design, skill development, and longitudinal impacts. (3) Idea Drafting: The proposed method integrates text mining and LDA to systematically analyze and summarize the literature, enabling a comprehensive understanding of MT's pedagogical applications. This innovation addresses the limitation by providing a scalable, data-driven approach to synthesizing research and identifying best practices.",
      "rationale": "Step 1: Compatibility Analysis - Text mining and LDA are computationally efficient and capable of processing large datasets, making them suitable for synthesizing the fragmented literature on MT in EFL contexts. The method's ability to identify latent topics aligns with the need to uncover trends and patterns in diverse studies. Theoretical properties, such as LDA's probabilistic topic modeling, are compatible with the goal of categorizing and summarizing research. Step 2: Gap Identification - The gap lies in the lack of domain-specific adaptation of text mining and LDA for EFL pedagogy research. The bridging variable is the customization of preprocessing steps and topic modeling parameters to focus on EFL-related themes, such as instructional design, skill development, and longitudinal impacts. This adaptation ensures the method addresses the specific research questions posed by the limitation. Step 3: Idea Drafting - The proposed method, titled 'Integrating Text Mining and Topic Modeling to Advance Machine Translation Pedagogy in EFL Contexts,' leverages text mining and LDA to systematically analyze and categorize the literature. By identifying latent topics and trends, this approach provides a comprehensive foundation for understanding MT's pedagogical applications. The core innovation is the domain-specific adaptation of text mining and LDA to address EFL-related research questions. Decision Chain: The limitation requires a systematic synthesis of literature, which text mining and LDA can provide. The gap is bridged by adapting these methods to the EFL context, resulting in a feasible and impactful solution. Evidence: Evolutionary paths highlight the success of integrating multiple methods to address fragmented research, supporting the proposed approach of combining text mining and LDA for this limitation."
    },
    {
      "limitation": "- Investigate how ethical concerns regarding translation technology and AI systems can be systematically integrated into pedagogical models for translator training. This could involve developing specific curricula or frameworks addressing ethical competence.\n- Explore the effectiveness of current teaching programs in addressing the intersection of technology and ethics in translation practice. Future studies could assess whether these programs adequately prepare translators for real-world ethical challenges.",
      "method": "The authors propose using text mining techniques combined with latent Dirichlet allocation (LDA) to systematically analyze and summarize the vast body of literature.\n\n**Explanation:** Text mining techniques enable automated processing and extraction of information from large datasets, while latent Dirichlet allocation (LDA) is a topic modeling algorithm that identifies latent topics within text data. By applying these methods, the authors can categorize and summarize the literature, making it easier for researchers to identify patterns, trends, and relevant insights without manually reviewing each document. This reduces the cognitive burden and enhances the efficiency of literature review processes.",
      "status": "SUCCESS",
      "title": "Ethical Competence in Translator Training: A Text Mining and Topic Modeling Framework for Curriculum Development",
      "abstract": "Background: The integration of ethical concerns regarding translation technology and AI systems into pedagogical models for translator training remains underexplored. Current teaching programs often fail to systematically address the intersection of technology and ethics, leaving translators ill-prepared for real-world ethical challenges. Gap: While text mining techniques and latent Dirichlet allocation (LDA) have been successfully applied to summarize and categorize vast bodies of literature, their potential to inform curriculum development in translator training has not been investigated. Proposed Method: This study proposes a framework that leverages text mining and LDA to systematically analyze existing literature on ethics in translation technology and AI systems. By identifying latent topics and ethical themes, the framework will provide actionable insights for designing curricula that address ethical competence. Expected Result: The method is expected to enable the development of targeted pedagogical models that systematically integrate ethical concerns, thereby enhancing the preparedness of translators for ethical challenges in technology-driven contexts.",
      "modification": "Adapt the text mining and LDA framework to focus on ethical themes and pedagogical relevance by incorporating domain-specific preprocessing techniques and validation metrics tailored to curriculum development.",
      "reasoning": "Step 1 Analysis: The candidate method, which combines text mining and LDA, is compatible with the limitation's constraints. Text mining is computationally efficient and capable of processing large datasets, while LDA is theoretically robust for identifying latent topics. These properties align with the need to systematically analyze literature on ethics in translation technology and AI systems. The method's assumptions, such as the availability of textual data and the ability to preprocess it, are reasonable within the context of academic literature. Step 2 Analysis: The gap lies in adapting the method to focus on ethical themes and pedagogical relevance. The bridging variable is the incorporation of domain-specific preprocessing techniques (e.g., identifying ethical keywords, filtering irrelevant content) and validation metrics (e.g., alignment with pedagogical goals). These modifications will ensure that the extracted topics are actionable for curriculum development. Step 3 Analysis: The idea drafting process involved synthesizing the compatibility analysis and gap identification into a coherent framework. The title emphasizes the focus on ethical competence and curriculum development, while the abstract outlines the background, gap, proposed method, and expected results. The core innovation is the adaptation of text mining and LDA to systematically inform pedagogical models for translator training.",
      "rationale": "The reasoning follows a structured chain of thought: First, the compatibility analysis confirmed that text mining and LDA are suitable for analyzing literature on ethics in translation technology and AI systems. Their computational efficiency and topic modeling capabilities align with the limitation's requirements. Second, the gap identification highlighted the need for domain-specific adaptations to ensure the method's relevance to curriculum development. The bridging variableâ€”domain-specific preprocessing and validation metricsâ€”addresses this gap by tailoring the method to ethical and pedagogical contexts. Third, the idea drafting synthesized these insights into a structured framework, leveraging evolutionary patterns from research convergence. Similar to how systematic reviews and meta-analyses have unified disparate research directions, this framework aims to consolidate ethical themes for actionable curriculum design. The evolutionary context provided examples of successful method adaptations and integrations, which informed the proposed solution. The decision chain progressed logically from limitation â†’ method compatibility â†’ gap â†’ solution, supported by evidence from the method's theoretical properties and evolutionary patterns in related research domains."
    },
    {
      "limitation": "- Investigate how ethical concerns regarding translation technology and AI systems can be systematically integrated into pedagogical models for translator training. This could involve developing specific curricula or frameworks addressing ethical competence.\n- Explore the effectiveness of current teaching programs in addressing the intersection of technology and ethics in translation practice. Future studies could assess whether these programs adequately prepare translators for real-world ethical challenges.",
      "method": "Promoting critical, reflective decision-making and systematic thought processes as the most effective method for teaching ethics in contemporary schools.\n\n**Explanation:** By emphasizing critical and reflective decision-making, the approach equips individuals with the ability to analyze ethical dilemmas systematically, consider the consequences of their actions, and make informed decisions. This method addresses the lack of preparedness for handling complex ethical issues by fostering skills that are directly applicable to real-life situations, especially in professional contexts where decisions can have significant impacts.",
      "status": "SUCCESS",
      "title": "Integrating Reflective Decision-Making into Ethical Pedagogy for Translator Training in the Age of AI",
      "abstract": "Background: The rapid integration of AI and translation technologies has introduced complex ethical challenges for translators, necessitating systematic pedagogical models to address ethical competence. Current teaching programs often lack a structured approach to prepare translators for real-world ethical dilemmas at the intersection of technology and ethics. Gap: Existing methods for teaching ethics in translator training do not adequately incorporate reflective decision-making frameworks that enable students to systematically analyze and resolve ethical dilemmas. Proposed Method: This study proposes a novel pedagogical framework that integrates critical and reflective decision-making processes into translator training curricula. By adapting reflective practices to the specific ethical challenges posed by AI and translation technologies, the framework aims to enhance ethical competence and decision-making skills. Expected Result: The proposed framework is expected to systematically prepare translation students to navigate ethical complexities in professional contexts, fostering a deeper understanding of the interplay between technology, ethics, and translation practice.",
      "modification": "Adapt the reflective decision-making method to include domain-specific ethical scenarios and AI-related challenges in translation, and design a structured curriculum around these scenarios.",
      "reasoning": "The reasoning process involves three steps: (1) Compatibility Analysis: The candidate method of promoting critical and reflective decision-making aligns well with the need for systematic ethical pedagogy in translator training. Its theoretical foundation in fostering analytical and ethical reasoning directly addresses the limitation's requirements. (2) Gap Identification: The gap lies in the lack of domain-specific adaptation of reflective decision-making to the unique ethical challenges posed by AI and translation technologies. The bridging variable is the contextualization of reflective practices to translation-specific scenarios. (3) Idea Drafting: Drawing from evolutionary patterns of convergence in research, where multiple approaches were integrated into unified frameworks, the proposed idea combines reflective decision-making with a structured curriculum tailored to the ethical challenges of translation technology. This ensures a comprehensive and practical approach to ethical competence in translator training.",
      "rationale": "Step 1 Analysis: The candidate method emphasizes critical and reflective decision-making, which is inherently compatible with the need for systematic ethical pedagogy. Its focus on systematic thought processes and ethical analysis aligns with the goal of preparing translators for real-world ethical challenges. The method's theoretical properties, such as fostering analytical skills and ethical reasoning, are well-suited to address the limitation. Computational complexity and algorithmic considerations are not directly relevant, as the method is pedagogical rather than computational. Step 2 Analysis: The gap lies in the lack of adaptation of reflective decision-making to the specific context of translator training and AI-related ethical challenges. The bridging variable is the contextualization of reflective practices to include domain-specific scenarios, such as ethical dilemmas in AI-assisted translation. This requires designing a curriculum that integrates these scenarios into teaching programs. Step 3 Analysis: The idea builds on the evolutionary pattern of convergence, where multiple research directions are integrated into a unified framework. By combining reflective decision-making with a structured curriculum tailored to translation ethics, the proposed framework addresses the limitation comprehensively. The core innovation is the contextualization of reflective practices to translation-specific ethical challenges, enabling a systematic and practical approach to ethical pedagogy. Decision Chain: The limitation highlights the need for systematic ethical pedagogy in translator training. The candidate method of reflective decision-making is compatible with this need. The gap is the lack of domain-specific adaptation, which is addressed by contextualizing reflective practices to translation ethics. The proposed idea integrates these elements into a novel pedagogical framework, drawing from evolutionary patterns of convergence. Evidence: The evolutionary context demonstrates the effectiveness of integrating multiple approaches into unified frameworks to address complex challenges. This supports the feasibility of the proposed idea, which combines reflective decision-making with a structured curriculum tailored to translation ethics."
    },
    {
      "limitation": "- Future studies could focus on expanding the thematic scope of bibliometric analyses to include additional aspects of management, leadership, and administration beyond those directly related to COVID-19, promoting a broader understanding of sustainable research practices.\n- Further research is recommended to refine and enhance the methodological approaches used in bibliometric studies, ensuring more comprehensive and accurate mapping of global knowledge bases.\n- Exploration of the long-term impacts of COVID-19 on management and leadership practices could provide valuable insights for developing resilient and sustainable strategies in future crises.",
      "method": "The authors propose using text mining techniques combined with latent Dirichlet allocation (LDA) to systematically analyze and summarize the vast body of literature.\n\n**Explanation:** Text mining techniques enable automated processing and extraction of information from large datasets, while latent Dirichlet allocation (LDA) is a topic modeling algorithm that identifies latent topics within text data. By applying these methods, the authors can categorize and summarize the literature, making it easier for researchers to identify patterns, trends, and relevant insights without manually reviewing each document. This reduces the cognitive burden and enhances the efficiency of literature review processes.",
      "status": "SUCCESS",
      "title": "Expanding the Scope of Bibliometric Analysis: A Text Mining and LDA-Based Framework for Management and Leadership Research",
      "abstract": "Background: Bibliometric analyses have traditionally focused on narrow thematic areas, with limited exploration of management, leadership, and administration topics beyond specific contexts such as COVID-19. Furthermore, existing methods often lack the comprehensiveness needed to map global knowledge bases effectively. Gap: Current bibliometric approaches do not fully leverage advanced computational techniques to broaden thematic scope and improve methodological rigor. Proposed Method: This study proposes a novel framework that integrates text mining techniques with latent Dirichlet allocation (LDA) to systematically analyze and categorize literature across diverse management and leadership domains. By incorporating domain-specific preprocessing and adaptive topic modeling, the framework aims to enhance the accuracy and relevance of bibliometric analyses. Expected Result: The proposed method is expected to provide a more comprehensive understanding of global research trends, uncovering latent patterns and long-term impacts of crises like COVID-19 on management and leadership practices, thereby informing sustainable and resilient strategies.",
      "modification": "Incorporate domain-specific preprocessing and adaptive topic modeling to tailor LDA for management and leadership research.",
      "reasoning": "The reasoning process involves three steps. Step 1: Compatibility Analysis - Text mining and LDA are inherently suitable for analyzing large textual datasets, aligning well with the need for systematic literature categorization. However, the limitation requires expanding the thematic scope and improving methodological rigor, which demands domain-specific adaptations. Step 2: Gap Identification - The primary gap lies in the lack of domain-specific preprocessing and the need for adaptive topic modeling to tailor LDA for management and leadership contexts. This requires incorporating domain-relevant terminologies, contextual embeddings, and dynamic topic refinement. Step 3: Idea Drafting - Drawing from evolutionary patterns, such as the convergence of multiple techniques to address complex problems, the proposed framework integrates text mining and LDA with domain-specific innovations to overcome the identified gaps. The innovation lies in adapting LDA through domain-specific preprocessing and dynamic topic modeling, enabling a broader and more accurate bibliometric analysis.",
      "rationale": "Step 1: Compatibility Analysis - Text mining techniques and LDA are computationally efficient and scalable, making them suitable for processing large bibliometric datasets. Their algorithmic properties allow for the identification of latent topics, aligning with the need for systematic literature categorization. However, the limitation requires thematic expansion and methodological enhancement, which are not directly addressed by the standard LDA approach. Step 2: Gap Identification - The key gap is the lack of domain-specific adaptations in LDA for management and leadership research. Bridging this gap requires incorporating domain-specific preprocessing (e.g., filtering irrelevant terms, using contextual embeddings) and adaptive topic modeling (e.g., dynamic refinement of topics based on domain knowledge). This aligns with the evolutionary pattern of extending existing methods through domain-specific innovations. Step 3: Idea Drafting - Inspired by the convergence pattern in evolutionary paths, the proposed framework integrates text mining and LDA with domain-specific preprocessing and adaptive topic modeling. This innovation ensures that the method addresses the broader thematic scope and methodological rigor required for comprehensive bibliometric analyses. The logical chain of reasoning moves from identifying the limitation (narrow thematic scope and methodological gaps) to evaluating the method's compatibility (suitability of text mining and LDA), identifying the gap (lack of domain-specific adaptations), and proposing a solution (domain-specific preprocessing and adaptive topic modeling). Evidence from evolutionary paths supports the feasibility of this approach, demonstrating how similar limitations have been overcome through method extensions and cross-domain adaptations."
    },
    {
      "limitation": "- Future studies could focus on expanding the thematic scope of bibliometric analyses to include additional aspects of management, leadership, and administration beyond those directly related to COVID-19, promoting a broader understanding of sustainable research practices.\n- Further research is recommended to refine and enhance the methodological approaches used in bibliometric studies, ensuring more comprehensive and accurate mapping of global knowledge bases.\n- Exploration of the long-term impacts of COVID-19 on management and leadership practices could provide valuable insights for developing resilient and sustainable strategies in future crises.",
      "method": "The authors propose a vector space model for automatic indexing, where entities are represented in a high-dimensional space designed to maximize the distance between them.\n\n**Explanation:** By representing documents and search requests as vectors in a high-dimensional space, the model ensures that entities are positioned as far apart as possible. This reduces the density of the object space and minimizes overlap between entities, improving the precision and efficiency of retrieval by making it easier to distinguish between documents and match them to relevant search requests.",
      "status": "SUCCESS",
      "title": "Expanding Bibliometric Horizons: A Vector Space Model for Thematic and Methodological Integration in Management Research",
      "abstract": "Background: Bibliometric analyses in management, leadership, and administration have been largely constrained to COVID-19-related themes, with limited exploration of broader sustainable research practices. Additionally, existing methods often lack the precision and adaptability needed for comprehensive mapping of global knowledge bases. Gap: Current bibliometric approaches do not adequately address the need for thematic expansion or long-term impact analysis, nor do they leverage advanced indexing techniques to improve accuracy and scope. Proposed Method: This study proposes an adapted vector space model for bibliometric analysis, where documents and thematic entities are represented in a high-dimensional space optimized for thematic diversity and methodological precision. By incorporating domain-specific weighting schemes and dynamic entity clustering, the model enables broader thematic exploration and more accurate mapping of knowledge interconnections. Expected Result: The proposed method is expected to enhance the thematic scope of bibliometric studies, refine methodological approaches, and provide actionable insights into the long-term impacts of COVID-19 on management and leadership practices, thereby contributing to the development of resilient and sustainable strategies.",
      "modification": "Incorporate domain-specific weighting schemes and dynamic entity clustering into the vector space model to enable thematic expansion and improve bibliometric precision.",
      "reasoning": "Step 1: Compatibility Analysis - The vector space model's mathematical foundation is suitable for representing high-dimensional relationships between documents and themes. Its ability to maximize distances between entities aligns with the need for thematic differentiation in bibliometric analyses. However, the model in its current form does not inherently address domain-specific requirements or dynamic thematic clustering, which are critical for the stated limitation. Step 2: Gap Identification - To bridge the gap, the model requires domain-specific weighting schemes to prioritize management and leadership themes and dynamic clustering mechanisms to adapt to evolving thematic landscapes. These modifications will enable the model to address both thematic expansion and methodological refinement. Step 3: Idea Drafting - Drawing from the evolutionary pattern of convergence, where multiple independent methods were integrated into a unified framework, the proposed idea adapts the vector space model to bibliometric contexts. The core innovation lies in tailoring the model's parameters to the specific needs of management research, thereby enhancing its applicability and impact.",
      "rationale": "The reasoning process begins with a detailed compatibility analysis. The vector space model's high-dimensional representation and ability to maximize entity separation make it a strong candidate for addressing the limitation of thematic expansion in bibliometric studies. However, its current form lacks the specificity needed for management research themes and the adaptability required for dynamic thematic exploration. To address this, the gap analysis identifies two key modifications: (1) domain-specific weighting schemes to prioritize relevant themes, and (2) dynamic clustering mechanisms to adapt to evolving research landscapes. These innovations are inspired by the convergence evolutionary pattern, where independent methods were integrated to overcome limitations. The proposed idea builds on this logic, adapting the vector space model to the bibliometric context with tailored parameters. This approach is expected to enhance thematic scope, refine methodological precision, and provide actionable insights into the long-term impacts of COVID-19 on management and leadership practices, aligning with the stated research goals."
    },
    {
      "limitation": "- Investigate the role of uncertainty and emotions in shaping policies in other regions or countries to compare with the findings from Quebec, Canada. This could provide a broader understanding of how cultural and political contexts influence decision-making during pandemics.\n- Enhance the natural language processing (NLP) techniques used in the study to develop more refined indices for measuring sentiments and scientific discourse. This improvement could lead to more accurate and detailed analyses of policymaker communications.\n- Explore the long-term impact of uncertainty and emotional sentiments on public trust in science and policy decisions during health crises. This would help assess the lasting effects of communication strategies used during the COVID-19 pandemic.",
      "method": "The authors propose using text mining techniques combined with latent Dirichlet allocation (LDA) to systematically analyze and summarize the vast body of literature.\n\n**Explanation:** Text mining techniques enable automated processing and extraction of information from large datasets, while latent Dirichlet allocation (LDA) is a topic modeling algorithm that identifies latent topics within text data. By applying these methods, the authors can categorize and summarize the literature, making it easier for researchers to identify patterns, trends, and relevant insights without manually reviewing each document. This reduces the cognitive burden and enhances the efficiency of literature review processes.",
      "status": "SUCCESS",
      "title": "Integrating Sentiment Analysis and Cross-Cultural NLP Models to Assess Emotional and Uncertainty Dynamics in Policy Communication During Pandemics",
      "abstract": "Background: Understanding how uncertainty and emotions influence policymaking across different cultural and political contexts is critical for assessing the effectiveness of communication strategies during health crises. Current research has focused on Quebec, Canada, but lacks comparative insights from other regions. Additionally, existing natural language processing (NLP) techniques for sentiment analysis and scientific discourse measurement require refinement to improve accuracy and granularity. Gap: While text mining techniques and latent Dirichlet allocation (LDA) are effective for summarizing large datasets, they do not inherently account for cross-cultural sentiment dynamics or long-term impacts on public trust in science and policy. Proposed Method: We propose an enhanced framework that integrates sentiment analysis with cross-cultural NLP models and extends LDA to include emotional and uncertainty indices tailored to diverse cultural contexts. This framework will systematically analyze policymaker communications and assess the long-term effects of emotional and uncertainty dynamics on public trust. Expected Result: The proposed method will provide a robust comparative analysis of policy communication strategies across regions, refine sentiment indices for better granularity, and offer insights into the lasting impacts of communication strategies during pandemics.",
      "modification": "Extend LDA to incorporate sentiment and uncertainty indices informed by cross-cultural NLP models, and develop a methodology for longitudinal analysis of public trust dynamics.",
      "reasoning": "Step 1 Analysis: The candidate method, combining text mining and LDA, is compatible with the limitation's requirements for processing large datasets and identifying latent topics. However, its current form does not address the need for cross-cultural sentiment analysis or the longitudinal study of public trust dynamics. The computational complexity of LDA is manageable for this task, and its applicability domain can be extended with modifications. Step 2 Analysis: To bridge the gap, LDA must be extended to include sentiment and uncertainty indices tailored to diverse cultural contexts. This requires integrating cross-cultural NLP models that account for linguistic and emotional variations across regions. Additionally, a methodology for longitudinal analysis of public trust dynamics must be developed. The bridging variable is the integration of cross-cultural sentiment analysis and uncertainty modeling into the LDA framework. Step 3 Analysis: The idea was drafted by combining insights from evolutionary paths that demonstrated successful integration of multiple methodologies (e.g., convergence patterns in systematic reviews and meta-analyses). The core innovation lies in adapting LDA to include sentiment and uncertainty indices informed by cross-cultural NLP models, enabling comparative and longitudinal analyses. Decision Chain: Limitation â†’ Compatibility of text mining and LDA â†’ Gap in cross-cultural and longitudinal analysis â†’ Solution via enhanced LDA framework â†’ Feasible idea. Evidence: Evolutionary paths show successful integration of multiple methodologies to address complex limitations, supporting the feasibility of extending LDA for this purpose.",
      "rationale": "The reasoning process began with analyzing the compatibility of text mining and LDA with the limitation's requirements. Text mining and LDA are suitable for processing large datasets and identifying latent topics, but they lack mechanisms for cross-cultural sentiment analysis and longitudinal studies. The gap was identified as the need to integrate sentiment and uncertainty indices tailored to diverse cultural contexts and develop a methodology for assessing public trust dynamics over time. The bridging variable was determined to be the integration of cross-cultural sentiment analysis and uncertainty modeling into the LDA framework. Inspired by evolutionary paths showing successful integration of methodologies, the idea was drafted to extend LDA with these capabilities. The proposed method addresses the limitation by enabling comparative and longitudinal analyses, refining sentiment indices, and providing insights into the lasting impacts of communication strategies during pandemics."
    },
    {
      "limitation": "- Investigate methods to group low-frequency terms effectively to enhance the indexing vocabulary, as these terms are currently less useful for indexing.\n- Explore the impact of skewed frequency distributions on term usefulness in greater detail to refine the criteria for selecting optimal indexing terms.\n- Develop strategies to address the limitations of terms with very high or very low document frequency, aiming to improve their contribution to indexing systems.",
      "method": "The authors propose using text mining techniques combined with latent Dirichlet allocation (LDA) to systematically analyze and summarize the vast body of literature.\n\n**Explanation:** Text mining techniques enable automated processing and extraction of information from large datasets, while latent Dirichlet allocation (LDA) is a topic modeling algorithm that identifies latent topics within text data. By applying these methods, the authors can categorize and summarize the literature, making it easier for researchers to identify patterns, trends, and relevant insights without manually reviewing each document. This reduces the cognitive burden and enhances the efficiency of literature review processes.",
      "status": "SUCCESS",
      "title": "Latent Term Clustering: Enhancing Indexing Vocabulary through LDA-Driven Low-Frequency Term Grouping",
      "abstract": "Background: The effectiveness of indexing systems is often compromised by the skewed frequency distribution of terms, where low-frequency terms are underutilized and high-frequency terms dominate, leading to suboptimal vocabulary selection. Gap: Current approaches lack systematic methods to group low-frequency terms and refine criteria for selecting optimal indexing terms, particularly in the context of skewed frequency distributions. Proposed Method: We propose a novel framework that integrates text mining techniques with latent Dirichlet allocation (LDA) to cluster low-frequency terms into semantically coherent groups. This approach leverages LDA's topic modeling capabilities to identify latent structures within the data, enabling the effective grouping of terms based on shared contextual relevance. Expected Result: The proposed method is expected to enhance the utility of low-frequency terms in indexing systems, improve the balance of term contributions across frequency distributions, and refine the criteria for term selection, ultimately leading to more robust and comprehensive indexing vocabularies.",
      "modification": "Adapt LDA to focus on low-frequency terms by introducing a frequency-sensitive weighting mechanism and refining the topic modeling process to prioritize underrepresented terms.",
      "reasoning": "Step 1: Compatibility Analysis - The candidate method, which combines text mining and LDA, is theoretically compatible with the limitation. LDA is well-suited for identifying latent structures in text data, and its probabilistic nature allows it to handle skewed frequency distributions. However, LDA in its standard form does not inherently prioritize low-frequency terms, which is a critical requirement for addressing the limitation. Step 2: Gap Identification - The gap lies in the need to adapt LDA to focus specifically on low-frequency terms. This requires introducing a frequency-sensitive weighting mechanism that amplifies the contribution of low-frequency terms during the topic modeling process. Additionally, the criteria for grouping terms must be refined to ensure semantic coherence and relevance to indexing. Step 3: Idea Drafting - The proposed idea builds on the compatibility of LDA with the problem domain and bridges the identified gap through targeted modifications. The title and abstract reflect the core innovation, which is the frequency-sensitive adaptation of LDA for low-frequency term grouping. The expected results align with the goals of improving indexing vocabulary and addressing the limitations posed by skewed frequency distributions.",
      "rationale": "The reasoning follows a structured chain of thought: 1) The limitation requires a method to group low-frequency terms and address skewed frequency distributions in indexing systems. 2) The candidate method, combining text mining and LDA, is compatible with the problem domain but requires adaptation to focus on low-frequency terms. 3) The gap is bridged by introducing a frequency-sensitive weighting mechanism and refining the topic modeling process. 4) The proposed idea leverages LDA's latent structure identification capabilities while addressing its limitations through targeted modifications. This approach aligns with evolutionary patterns observed in the field, where methods were extended and adapted to overcome specific limitations. The proposed framework is expected to enhance the utility of low-frequency terms, improve term selection criteria, and contribute to more robust indexing systems."
    }
  ],
  "pools": {
    "unsolved_limitations": 35,
    "candidate_methods": 3
  }
};

                // æŒ‰ç±»å‹åˆ†ç»„è¾¹æ•°æ®
                const edgesByType = {};
                edgesData.forEach(edge => {
                    if (!edgesByType[edge.type]) {
                        edgesByType[edge.type] = [];
                    }
                    edgesByType[edge.type].push(edge);
                });

                // åˆ›å»ºèŠ‚ç‚¹è½¨è¿¹
                const nodeTrace = {
                    x: nodesData.map(n => n.x),
                    y: nodesData.map(n => n.y),
                    mode: 'markers+text',
                    marker: {
                        size: nodesData.map(n => n.size),
                        color: nodesData.map(n => n.color),
                        line: { width: 2, color: 'white' },
                        colorscale: 'Viridis'
                    },
                    text: nodesData.map(n => n.label),
                    textposition: 'middle center',
                    textfont: { size: 10, color: 'black' },
                    customdata: nodesData,
                    hovertemplate: '<b>%{customdata.title}</b><extra></extra>',
                    type: 'scatter',
                    name: 'è®ºæ–‡èŠ‚ç‚¹'
                };

                // å›¾è¡¨å¸ƒå±€é…ç½®
                const layout = {
                    title: '',
                    showlegend: false,
                    hovermode: 'closest',
                    margin: { l: 0, r: 0, b: 40, t: 0 },
                    xaxis: {
                        title: 'å‘è¡¨å¹´ä»½',
                        showgrid: true,
                        gridcolor: 'lightgray',
                        range: [1971, 2027]
                    },
                    yaxis: {
                        title: 'è®ºæ–‡åˆ†å¸ƒ',
                        showgrid: true,
                        gridcolor: 'lightgray',
                        showticklabels: false
                    },
                    plot_bgcolor: 'white',
                    paper_bgcolor: 'white'
                };

                // ========== å·¥å…·å‡½æ•° ==========
                // åˆ›å»ºè¾¹è½¨è¿¹ï¼ˆé€šç”¨å‡½æ•°ï¼Œæ¶ˆé™¤é‡å¤é€»è¾‘ï¼‰
                function createEdgeTraces(styleMap) {
                    const traces = [];
                    Object.keys(edgesByType).forEach(type => {
                        const edges = edgesByType[type];
                        const style = styleMap.get(type) || {
                            color: edges[0].color,
                            width: edges[0].width,
                            dash: 'solid'
                        };

                        const edgeX = [];
                        const edgeY = [];

                        edges.forEach(edge => {
                            const fromNode = nodesData.find(n => n.id === edge.from);
                            const toNode = nodesData.find(n => n.id === edge.to);
                            if (fromNode && toNode) {
                                edgeX.push(fromNode.x, toNode.x, null);
                                edgeY.push(fromNode.y, toNode.y, null);
                            }
                        });

                        if (edgeX.length > 0) {
                            traces.push({
                                x: edgeX,
                                y: edgeY,
                                mode: 'lines',
                                line: {
                                    width: style.width,
                                    color: style.color,
                                    dash: style.dash
                                },
                                hoverinfo: 'none',
                                showlegend: false,
                                type: 'scatter'
                            });
                        }
                    });
                    return traces;
                }

                // æ›´æ–°å›¾è¡¨ï¼ˆé€šç”¨å‡½æ•°ï¼‰
                function updateGraph(edgeTraces, nodeColors, nodeLineStyle = null) {
                    const nodeUpdate = {
                        ...nodeTrace,
                        marker: {
                            ...nodeTrace.marker,
                            color: nodeColors,
                            line: nodeLineStyle || { width: 2, color: 'white' }
                        }
                    };

                    Plotly.react('graph', [...edgeTraces, nodeUpdate], layout);
                }

                // ========== åˆå§‹åŒ–å›¾è¡¨ ==========
                // åˆ›å»ºåˆå§‹è¾¹æ ·å¼ï¼ˆä½¿ç”¨åŸå§‹å®šä¹‰çš„é¢œè‰²ï¼‰
                const initialEdgeStyle = new Map();
                Object.keys(edgesByType).forEach(type => {
                    const firstEdge = edgesByType[type][0];
                    initialEdgeStyle.set(type, {
                        color: firstEdge.original_color,
                        width: firstEdge.width,
                        dash: firstEdge.dash
                    });
                });

                const initialEdgeTraces = createEdgeTraces(initialEdgeStyle);
                updateGraph(initialEdgeTraces, nodesData.map(n => n.color));

                // ========== äº‹ä»¶å¤„ç†å™¨ ==========
                document.getElementById('graph').on('plotly_click', function(data) {
                    if (data.points?.[0]?.customdata) {
                        const node = data.points[0].customdata;
                        const nodeIndex = data.points[0].pointIndex;
                        showPaperDetails(node);
                        highlightClickedNodeAndEdges(nodeIndex, node);
                    }
                });


                // ========== åŠŸèƒ½å‡½æ•° ==========
                // æ ‡ç­¾é¡µåˆ‡æ¢å‡½æ•°
                function switchTab(event, tabId) {
                    // ç§»é™¤æ‰€æœ‰activeç±»
                    document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
                    document.querySelectorAll('.tab-content').forEach(content => content.classList.remove('active'));

                    // æ·»åŠ activeç±»åˆ°å½“å‰æ ‡ç­¾
                    event.target.classList.add('active');
                    document.getElementById(tabId).classList.add('active');

                    // åˆ‡æ¢åˆ°"è®ºæ–‡è¯¦æƒ…"æˆ–"ç§‘ç ”åˆ›æ„"æ ‡ç­¾æ—¶ï¼Œé‡ç½®å›¾è°±é«˜äº®
                    if (tabId === 'paper-tab' || tabId === 'ideas-tab') {
                        resetGraphHighlight();
                        console.log(`åˆ‡æ¢åˆ° ${tabId}ï¼Œå·²é‡ç½®å›¾è°±é«˜äº®`);
                    }

                    // æ ¹æ®æ ‡ç­¾é¡µIDåŠ è½½ç›¸åº”å†…å®¹
                    if (tabId === 'survey-tab') {
                        renderDeepSurvey();
                    } else if (tabId === 'ideas-tab') {
                        renderResearchIdeas();
                    }
                }

                // æ¸²æŸ“æ·±åº¦ç»¼è¿° (æ–°ç‰ˆæ•°æ®ç»“æ„)
                function renderDeepSurvey() {
                    const surveyTab = document.getElementById('survey-tab');

                    if (!deepSurveyData || Object.keys(deepSurveyData).length === 0) {
                        surveyTab.innerHTML = '<div class="placeholder">æš‚æ— æ·±åº¦ç»¼è¿°æ•°æ®</div>';
                        return;
                    }

                    let html = '<div style="padding:20px;">';

                    // æ‘˜è¦ä¿¡æ¯ (æ–°ç»“æ„)
                    if (deepSurveyData.summary) {
                        html += `
                            <div class="stats">
                                <h4 style="margin-top:0;">ğŸ“Š ç»¼è¿°æ‘˜è¦</h4>
                                <div class="stat-item"><span>åŸå§‹è®ºæ–‡:</span><span>${deepSurveyData.summary.original_papers || 0} ç¯‡</span></div>
                                <div class="stat-item"><span>ç­›é€‰åè®ºæ–‡:</span><span>${deepSurveyData.summary.pruned_papers || 0} ç¯‡</span></div>
                                <div class="stat-item"><span>æ¼”åŒ–æ•…äº‹çº¿:</span><span>${deepSurveyData.summary.total_threads || 0} æ¡</span></div>
                            </div>
                        `;
                    }

                    // å‰ªæç»Ÿè®¡ä¿¡æ¯
                    if (deepSurveyData.pruning_stats) {
                        const stats = deepSurveyData.pruning_stats;
                        const retentionRate = (stats.retention_rate * 100).toFixed(1);
                        html += `
                            <div class="stats" style="margin-top:15px; background:#fff3cd;">
                                <h4 style="margin-top:0;">âœ‚ï¸ å›¾è°±å‰ªæç»Ÿè®¡</h4>
                                <div class="stat-item"><span>ä¿ç•™ç‡:</span><span>${retentionRate}%</span></div>
                                <div class="stat-item"><span>ç§å­è®ºæ–‡:</span><span>${stats.seed_papers || 0} ç¯‡</span></div>
                                <div class="stat-item"><span>å¼ºå…³ç³»è¾¹:</span><span>${stats.strong_edges || 0} æ¡</span></div>
                                <div class="stat-item"><span>å‰”é™¤å¼±å…³ç³»è¾¹:</span><span>${stats.weak_edges_removed || 0} æ¡</span></div>
                            </div>
                        `;
                    }

                    // æ¼”åŒ–è·¯å¾„ (Threads)
                    const threads = deepSurveyData.survey_report?.threads || deepSurveyData.evolutionary_paths || [];
                    if (threads.length > 0) {
                        html += `
                            <div style="display:flex; justify-content:space-between; align-items:center; margin-top:20px;">
                                <h3 style="color:#2c3e50; margin:0;">ğŸ§µ å…³é”®æ¼”åŒ–æ•…äº‹çº¿</h3>
                                <button id="resetGraphBtn" onclick="resetGraphHighlight()"
                                    style="padding:5px 12px; background:#6c757d; color:white; border:none; border-radius:4px; cursor:pointer; font-size:12px;">
                                    ğŸ”„ é‡ç½®å›¾è°±
                                </button>
                            </div>
                        `;
                        threads.forEach((thread, index) => {
                            const threadTitle = thread.title || thread.thread_name || `Thread ${index + 1}`;
                            const patternType = thread.pattern_type || thread.thread_type || 'æœªçŸ¥ç±»å‹';
                            const paperCount = thread.papers ? thread.papers.length : 0;
                            const narrative = thread.narrative || 'æš‚æ— å™äº‹æ–‡æœ¬';

                            // å®šä¹‰ä¸°å¯Œçš„é¢œè‰²è°ƒè‰²æ¿ï¼ˆæŒ‰æ•…äº‹çº¿ç´¢å¼•åˆ†é…ï¼‰
                            const colorPalette = [
                                '#E74C3C',  // çº¢è‰² - Thread 0
                                '#3498DB',  // è“è‰² - Thread 1
                                '#2ECC71',  // ç»¿è‰² - Thread 2
                                '#F39C12',  // æ©™è‰² - Thread 3
                                '#9B59B6',  // ç´«è‰² - Thread 4
                                '#1ABC9C',  // é’è‰² - Thread 5
                                '#E67E22',  // æ·±æ©™è‰² - Thread 6
                                '#95A5A6',  // ç°è‰² - Thread 7
                                '#34495E',  // æ·±è“ç° - Thread 8
                                '#16A085'   // æ·±é’è‰² - Thread 9
                            ];

                            // æ ¹æ®æ•…äº‹çº¿ç´¢å¼•åˆ†é…é¢œè‰²ï¼ˆä¿è¯æ¯æ¡æ•…äº‹çº¿é¢œè‰²å”¯ä¸€ï¼‰
                            let borderColor = colorPalette[index % colorPalette.length];
                            let highlightColor = borderColor;

                            // æ”¶é›†è¯¥æ•…äº‹çº¿çš„æ‰€æœ‰è®ºæ–‡ID
                            const paperIds = thread.papers ? thread.papers.map(p => p.paper_id) : [];

                            html += `
                                <div class="epoch-card" style="border-left-color:${borderColor}; cursor:pointer; transition:all 0.3s;"
                                     onclick="highlightThread(${index}, '${highlightColor}')"
                                     onmouseover="this.style.backgroundColor='#f8f9fa'"
                                     onmouseout="this.style.backgroundColor='white'">
                                    <h4>
                                        ${threadTitle}
                                        <span style="float:right; font-size:12px; color:#666; font-weight:normal;">
                                            ${patternType}
                                        </span>
                                    </h4>
                                    <p><strong>ğŸ“š è®ºæ–‡æ•°é‡:</strong> ${paperCount} ç¯‡</p>
                                    ${thread.total_citations ? `<p><strong>ğŸ“Š æ€»å¼•ç”¨æ•°:</strong> ${thread.total_citations}</p>` : ''}
                                    <p style="font-size:11px; color:#666; margin-top:8px;">
                                        ğŸ’¡ <em>ç‚¹å‡»æ­¤å¡ç‰‡å¯åœ¨å·¦ä¾§å›¾è°±ä¸­é«˜äº®æ˜¾ç¤ºè¯¥æ•…äº‹çº¿çš„æ‰€æœ‰è®ºæ–‡</em>
                                    </p>

                                    <details style="margin-top:10px;">
                                        <summary style="cursor:pointer; color:#3498DB; font-weight:bold;">ğŸ“– æŸ¥çœ‹æ¼”åŒ–å™äº‹</summary>
                                        <div style="margin-top:10px; padding:10px; background:#f8f9fa; border-radius:5px; line-height:1.6; white-space:pre-wrap;">
                                            ${narrative}
                                        </div>
                                    </details>

                                    ${thread.papers && thread.papers.length > 0 ? `
                                        <details style="margin-top:10px;">
                                            <summary style="cursor:pointer; color:#2ECC71; font-weight:bold;">ğŸ“„ æŸ¥çœ‹è®ºæ–‡åˆ—è¡¨</summary>
                                            <ul style="margin-top:10px; padding-left:20px;">
                                                ${thread.papers.map((p, pIndex) => `
                                                    <li style="margin:5px 0; cursor:pointer; color:#2980b9; transition:color 0.2s;"
                                                        onclick="event.stopPropagation(); showPaperFromThread('${p.paper_id}');"
                                                        onmouseover="this.style.color='#3498db'; this.style.textDecoration='underline';"
                                                        onmouseout="this.style.color='#2980b9'; this.style.textDecoration='none';"
                                                        title="ç‚¹å‡»æŸ¥çœ‹è¯¥è®ºæ–‡è¯¦æƒ…å¹¶åœ¨å›¾è°±ä¸­é«˜äº®">
                                                        <strong>${p.title}</strong>
                                                        (${p.year || 'N/A'}, å¼•ç”¨: ${p.cited_by_count || 0})
                                                    </li>
                                                `).join('')}
                                            </ul>
                                        </details>
                                    ` : ''}
                                </div>
                            `;
                        });
                    }

                    // ç»¼è¿°æŠ¥å‘Šæ‘˜è¦
                    if (deepSurveyData.survey_report?.abstract) {
                        html += `
                            <div style="margin-top:20px; padding:15px; background:#e8f4f8; border-left:4px solid #3498DB; border-radius:5px;">
                                <h4 style="margin:0 0 10px 0; color:#2c3e50;">ğŸ“ ç»¼è¿°æ‘˜è¦</h4>
                                <p style="line-height:1.6; color:#333; margin:0;">${deepSurveyData.survey_report.abstract}</p>
                            </div>
                        `;
                    }

                    html += '</div>';
                    surveyTab.innerHTML = html;
                }

                // æ¸²æŸ“ç§‘ç ”åˆ›æ„
                function renderResearchIdeas() {
                    const ideasTab = document.getElementById('ideas-tab');

                    if (!researchIdeasData || Object.keys(researchIdeasData).length === 0) {
                        ideasTab.innerHTML = '<div class="placeholder">æš‚æ— ç§‘ç ”åˆ›æ„æ•°æ®</div>';
                        return;
                    }

                    let html = '<div style="padding:20px;">';

                    // ç»Ÿè®¡ä¿¡æ¯
                    html += `
                        <div class="stats">
                            <h4 style="margin-top:0;">ğŸ’¡ åˆ›æ„ç”Ÿæˆç»Ÿè®¡</h4>
                            <div class="stat-item"><span>æ€»åˆ›æ„æ•°:</span><span>${researchIdeasData.total_ideas || 0}</span></div>
                            <div class="stat-item"><span>å¯è¡Œåˆ›æ„:</span><span>${researchIdeasData.successful_ideas || 0}</span></div>
                            ${researchIdeasData.pools ? `
                                <div class="stat-item"><span>æœªè§£å†³é™åˆ¶:</span><span>${researchIdeasData.pools.unsolved_limitations || 0}</span></div>
                                <div class="stat-item"><span>å€™é€‰æ–¹æ³•:</span><span>${researchIdeasData.pools.candidate_methods || 0}</span></div>
                            ` : ''}
                        </div>
                    `;

                    // åˆ›æ„åˆ—è¡¨
                    if (researchIdeasData.ideas && researchIdeasData.ideas.length > 0) {
                        html += '<h3 style="color:#2c3e50; margin-top:20px;">ğŸ’¡ ç ”ç©¶åˆ›æ„åˆ—è¡¨</h3>';
                        researchIdeasData.ideas.forEach((idea, index) => {
                            const statusClass = idea.status === 'SUCCESS' ? 'status-success' : 'status-incompatible';
                            const statusText = idea.status === 'SUCCESS' ? 'âœ“ å¯è¡Œ' : 'âœ— ä¸å…¼å®¹';

                            html += `
                                <div class="idea-card">
                                    <h4>
                                        åˆ›æ„ ${index + 1}: ${idea.title || 'æœªå‘½ååˆ›æ„'}
                                        <span class="status-badge ${statusClass}">${statusText}</span>
                                    </h4>
                                    ${idea.abstract ? `
                                        <p style="margin:10px 0; line-height:1.6; color:#444;">
                                            <strong>æ‘˜è¦:</strong> ${idea.abstract}
                                        </p>
                                    ` : ''}
                                    ${idea.modification ? `
                                        <p style="margin:8px 0; padding:10px; background:#f8f9fa; border-radius:4px;">
                                            <strong>ğŸ”§ å…³é”®åˆ›æ–°:</strong> ${idea.modification}
                                        </p>
                                    ` : ''}
                                    ${idea.reasoning ? `
                                        <details style="margin-top:10px;">
                                            <summary style="cursor:pointer; color:#3498DB;"><strong>æŸ¥çœ‹æ¨ç†è¿‡ç¨‹</strong></summary>
                                            <p style="margin-top:8px; font-size:12px; color:#666; white-space:pre-wrap;">${idea.reasoning}</p>
                                        </details>
                                    ` : ''}
                                </div>
                            `;
                        });
                    }

                    html += '</div>';
                    ideasTab.innerHTML = html;
                }

                function showPaperDetails(node) {
                    const authorsText = node.authors.slice(0, 5).join(', ') +
                                      (node.authors.length > 5 ? ' ç­‰' : '');

                    // æ„å»ºRAGåˆ†æéƒ¨åˆ†çš„HTML
                    let ragAnalysisHTML = '';
                    if (node.rag_problem || node.rag_method || node.rag_limitation || node.rag_future_work) {
                        ragAnalysisHTML = `
                            <div class="paper-info" style="background:#e8f4f8; padding:15px; border-radius:8px; margin-top:15px;">
                                <h4 style="margin:0 0 10px 0; color:#1a73e8; font-size:15px;">ğŸ§  å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ·±åº¦åˆ†æ</h4>
                                ${node.analysis_method ? `<p style="font-size:12px; color:#666; margin-bottom:10px;"><strong>åˆ†ææ–¹æ³•:</strong> ${node.analysis_method.toUpperCase()}</p>` : ''}
                                ${node.sections_extracted ? `<p style="font-size:12px; color:#666; margin-bottom:10px;"><strong>æå–ç« èŠ‚:</strong> ${node.sections_extracted} ä¸ª</p>` : ''}
                            </div>
                            ${node.rag_problem ? `
                            <div class="paper-info" style="border-left:3px solid #FF6B6B; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#FF6B6B; font-size:14px;">ğŸ“‹ ç ”ç©¶é—®é¢˜ (Problem)</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_problem}</p>
                            </div>
                            ` : ''}
                            ${node.rag_method ? `
                            <div class="paper-info" style="border-left:3px solid #4ECDC4; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#4ECDC4; font-size:14px;">ğŸ’¡ æ ¸å¿ƒæ–¹æ³• (Method)</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_method}</p>
                            </div>
                            ` : ''}
                            ${node.rag_limitation ? `
                            <div class="paper-info" style="border-left:3px solid #FFA500; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#FFA500; font-size:14px;">âš ï¸ å±€é™æ€§ (Limitation)</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_limitation}</p>
                            </div>
                            ` : ''}
                            ${node.rag_future_work ? `
                            <div class="paper-info" style="border-left:3px solid #9B59B6; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#9B59B6; font-size:14px;">ğŸ”® æœªæ¥å·¥ä½œ (Future Work)</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_future_work}</p>
                            </div>
                            ` : ''}
                        `;
                    }

                    document.getElementById('paper-tab').innerHTML = `
                        <div class="stats">
                            <h4 style="margin-top:0;">å›¾è°±ç»Ÿè®¡</h4>
                            <div class="stat-item"><span>è®ºæ–‡æ€»æ•°:</span><span>86</span></div>
                            <div class="stat-item"><span>å¼•ç”¨å…³ç³»:</span><span>99</span></div>
                            <div class="stat-item"><span>æ—¶é—´è·¨åº¦:</span><span>1972 - 2026</span></div>
                        </div>
                        <div class="paper-info">
                            <h3>${node.title}</h3>
                            <p><strong>ä½œè€…:</strong> ${authorsText}</p>
                            <p><strong>å¹´ä»½:</strong> ${node.year}</p>
                            <p><strong>å¼•ç”¨æ•°:</strong> ${node.cited_by_count}</p>
                            <p><strong>æœŸåˆŠ/ä¼šè®®:</strong> ${node.venue || 'æœªçŸ¥'}</p>
                            <p><strong>è®ºæ–‡ID:</strong> ${node.id}</p>
                        </div>
                        ${ragAnalysisHTML}
                    `;
                }

                function highlightClickedNodeAndEdges(nodeIndex, node) {
                    // åªæ”¹å˜èŠ‚ç‚¹é¢œè‰² - è¢«ç‚¹å‡»çš„èŠ‚ç‚¹é«˜äº®
                    const nodeColors = nodesData.map((n, i) =>
                        i === nodeIndex ? '#FF4444' : n.color);

                    // è¾¹ä¿æŒåŸå§‹æ ·å¼ä¸å˜
                    updateGraph(initialEdgeTraces, nodeColors);
                }

                // ========== æ¼”åŒ–æ•…äº‹çº¿é«˜äº®åŠŸèƒ½ ==========
                function highlightThread(threadIndex, highlightColor) {
                    // è·å–çº¿ç¨‹æ•°æ®
                    const threads = deepSurveyData.survey_report?.threads || deepSurveyData.evolutionary_paths || [];
                    if (threadIndex >= threads.length) return;

                    const thread = threads[threadIndex];
                    const threadPaperIds = new Set(thread.papers?.map(p => p.paper_id) || []);

                    console.log(`é«˜äº® Thread ${threadIndex}: ${thread.title}, åŒ…å« ${threadPaperIds.size} ç¯‡è®ºæ–‡`);

                    // æ›´æ–°èŠ‚ç‚¹é¢œè‰²å’Œå¤§å°
                    const newColors = [];
                    const newSizes = [];
                    const newLineStyles = [];

                    nodesData.forEach((node, index) => {
                        if (threadPaperIds.has(node.id)) {
                            // é«˜äº®æ˜¾ç¤ºï¼šä¿æŒèŠ‚ç‚¹åŸæœ¬é¢œè‰²ï¼Œæ”¾å¤§1.5å€
                            newColors.push(node.color);
                            newSizes.push(node.size * 1.5);
                            newLineStyles.push({ width: 3, color: node.color });
                        } else {
                            // å…¶ä»–èŠ‚ç‚¹ï¼šå˜ç°ï¼Œç¼©å°åˆ°0.5å€
                            newColors.push('#D3D3D3');
                            newSizes.push(node.size * 0.5);
                            newLineStyles.push({ width: 1, color: '#CCCCCC' });
                        }
                    });

                    // æ›´æ–°å›¾è°±
                    const highlightedNodeTrace = {
                        ...nodeTrace,
                        marker: {
                            ...nodeTrace.marker,
                            size: newSizes,
                            color: newColors,
                            line: newLineStyles
                        }
                    };

                    // è¾¹ä¹Ÿè°ƒæ•´é€æ˜åº¦ï¼ˆé«˜äº®æ•…äº‹çº¿å†…çš„è¾¹ï¼‰
                    const highlightedEdgeTraces = createEdgeTracesWithHighlight(threadPaperIds, highlightColor);

                    Plotly.react('graph', [...highlightedEdgeTraces, highlightedNodeTrace], layout);

                    // æ»šåŠ¨å›¾è°±åˆ°é«˜äº®åŒºåŸŸ
                    document.getElementById('graph').scrollIntoView({ behavior: 'smooth', block: 'center' });
                }

                function createEdgeTracesWithHighlight(highlightedNodeIds, highlightColor) {
                    const traces = [];
                    Object.keys(edgesByType).forEach(type => {
                        const edges = edgesByType[type];
                        const style = initialEdgeStyle.get(type) || {
                            color: edges[0].color,
                            width: edges[0].width,
                            dash: 'solid'
                        };

                        // åˆ†ç¦»é«˜äº®è¾¹å’Œéé«˜äº®è¾¹
                        const highlightedEdgeX = [];
                        const highlightedEdgeY = [];
                        const dimmedEdgeX = [];
                        const dimmedEdgeY = [];

                        edges.forEach(edge => {
                            const fromNode = nodesData.find(n => n.id === edge.from);
                            const toNode = nodesData.find(n => n.id === edge.to);
                            if (fromNode && toNode) {
                                // æ£€æŸ¥æ˜¯å¦æ˜¯é«˜äº®æ•…äº‹çº¿çš„è¾¹
                                const isHighlighted = highlightedNodeIds.has(edge.from) && highlightedNodeIds.has(edge.to);

                                if (isHighlighted) {
                                    highlightedEdgeX.push(fromNode.x, toNode.x, null);
                                    highlightedEdgeY.push(fromNode.y, toNode.y, null);
                                } else {
                                    dimmedEdgeX.push(fromNode.x, toNode.x, null);
                                    dimmedEdgeY.push(fromNode.y, toNode.y, null);
                                }
                            }
                        });

                        // æ·»åŠ é«˜äº®è¾¹traceï¼ˆä¿æŒåŸå§‹é¢œè‰²ï¼ŒåªåŠ ç²—ï¼‰
                        if (highlightedEdgeX.length > 0) {
                            traces.push({
                                x: highlightedEdgeX,
                                y: highlightedEdgeY,
                                mode: 'lines',
                                line: {
                                    width: style.width * 1.8,
                                    color: style.color,  // ä½¿ç”¨åŸå§‹é¢œè‰²ï¼Œä¸æ”¹å˜
                                    dash: style.dash
                                },
                                opacity: 1.0,
                                hoverinfo: 'none',
                                showlegend: false,
                                type: 'scatter'
                            });
                        }

                        // æ·»åŠ å˜ç°è¾¹trace
                        if (dimmedEdgeX.length > 0) {
                            traces.push({
                                x: dimmedEdgeX,
                                y: dimmedEdgeY,
                                mode: 'lines',
                                line: {
                                    width: style.width * 0.4,
                                    color: '#E0E0E0',
                                    dash: style.dash
                                },
                                opacity: 0.2,
                                hoverinfo: 'none',
                                showlegend: false,
                                type: 'scatter'
                            });
                        }
                    });
                    return traces;
                }

                function resetGraphHighlight() {
                    console.log('é‡ç½®å›¾è°±é«˜äº®');
                    // æ¢å¤åŸå§‹é¢œè‰²å’Œå¤§å°
                    updateGraph(initialEdgeTraces, nodesData.map(n => n.color));
                }

                // ä»æ·±åº¦ç»¼è¿°çš„è®ºæ–‡åˆ—è¡¨ä¸­ç‚¹å‡»è®ºæ–‡ï¼Œæ˜¾ç¤ºè¯¦æƒ…å¹¶é«˜äº®
                function showPaperFromThread(paperId) {
                    console.log('ä»æ·±åº¦ç»¼è¿°ç‚¹å‡»è®ºæ–‡:', paperId);

                    // æŸ¥æ‰¾å¯¹åº”çš„èŠ‚ç‚¹ç´¢å¼•
                    const nodeIndex = nodesData.findIndex(n => n.id === paperId);

                    if (nodeIndex === -1) {
                        console.warn('æœªåœ¨å›¾è°±ä¸­æ‰¾åˆ°è®ºæ–‡:', paperId);
                        alert('è¯¥è®ºæ–‡ä¸åœ¨å½“å‰æ˜¾ç¤ºçš„å›¾è°±èŠ‚ç‚¹ä¸­');
                        return;
                    }

                    const node = nodesData[nodeIndex];

                    // åˆ‡æ¢åˆ°è®ºæ–‡è¯¦æƒ…æ ‡ç­¾é¡µ
                    const paperTab = document.querySelector('.tab[onclick*="paper-tab"]');
                    if (paperTab) {
                        paperTab.click();
                    }

                    // æ˜¾ç¤ºè®ºæ–‡è¯¦æƒ…
                    showPaperDetails(node);

                    // æŸ¥æ‰¾è¯¥è®ºæ–‡æ‰€å±çš„æ¼”åŒ–æ•…äº‹çº¿
                    const threads = deepSurveyData.survey_report?.threads || deepSurveyData.evolutionary_paths || [];
                    let threadIndex = -1;

                    for (let i = 0; i < threads.length; i++) {
                        const thread = threads[i];
                        const paperIds = thread.papers?.map(p => p.paper_id) || [];
                        if (paperIds.includes(paperId)) {
                            threadIndex = i;
                            break;
                        }
                    }

                    // é«˜äº®æ•´ä¸ªæ•…äº‹çº¿ï¼ˆèŠ‚ç‚¹ä¿æŒåŸè‰²ï¼‰
                    if (threadIndex !== -1) {
                        console.log(`è¯¥è®ºæ–‡å±äºæ•…äº‹çº¿ ${threadIndex}ï¼Œå°†é«˜äº®æ•´ä¸ªæ•…äº‹çº¿`);
                        highlightThread(threadIndex, null);  // ä¼ nullå› ä¸ºä¸å†éœ€è¦highlightColor
                    } else {
                        // å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•…äº‹çº¿ï¼Œå›é€€åˆ°å•èŠ‚ç‚¹é«˜äº®
                        console.log('è¯¥è®ºæ–‡ä¸å±äºä»»ä½•æ•…äº‹çº¿ï¼Œåªé«˜äº®å•ä¸ªèŠ‚ç‚¹');
                        highlightClickedNodeAndEdges(nodeIndex, node);
                    }

                    console.log('å·²æ˜¾ç¤ºè®ºæ–‡è¯¦æƒ…å¹¶é«˜äº®èŠ‚ç‚¹:', node.title);
                }


                function updateHoverPosition(event) {
                    const hoverDiv = document.getElementById('hoverTitle');
                    if (hoverDiv) {
                        hoverDiv.style.left = (event.clientX + 10) + 'px';
                        hoverDiv.style.top = (event.clientY - 30) + 'px';
                    }
                }
            </script>
            
        </body>
        </html>
        