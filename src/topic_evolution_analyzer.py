"""
ä¸»é¢˜å‘å±•è„‰ç»œåˆ†ææ¨¡å—
Topic Evolution Analyzer

è´Ÿè´£åˆ†æç ”ç©¶ä¸»é¢˜åœ¨çŸ¥è¯†å›¾è°±ä¸­çš„å‘å±•è„‰ç»œï¼ŒåŒ…æ‹¬ï¼š
1. æ—¶é—´æ¼”åŒ–åˆ†æ
2. å…³é”®èŠ‚ç‚¹è¯†åˆ«ï¼ˆé‡Œç¨‹ç¢‘è®ºæ–‡ï¼‰
3. ç ”ç©¶åˆ†æ”¯åˆ†æï¼ˆç¤¾åŒºæ£€æµ‹ï¼‰
4. å¼•ç”¨é“¾è·¯åˆ†æ
5. åˆ›æ–°æ¨¡å¼åˆ†æ
6. å…³é”®è¿›åŒ–è·¯å¾„æå–ï¼ˆCritical Evolutionary Path Extractionï¼‰
7. æŠ€æœ¯åˆ†æ­§ç‚¹æ£€æµ‹ï¼ˆTechnical Bifurcation Detectionï¼‰
8. æœªé—­åˆå‰æ²¿æ¢æµ‹ï¼ˆOpen Frontier Detectionï¼‰
"""

import logging
from typing import Dict, List, Optional, Tuple, Set
from collections import defaultdict, Counter
from datetime import datetime
import re

try:
    import networkx as nx
except ImportError:
    raise ImportError("éœ€è¦å®‰è£…networkx: pip install networkx")

logger = logging.getLogger(__name__)


# è¿›åŒ–åŠ¨é‡æƒé‡å®šä¹‰ (Evolutionary Momentum Scores)
EVOLUTIONARY_WEIGHTS = {
    'Overcomes': 3.0,      # è´¨å˜ï¼šè§£å†³äº†å‰äººçš„ç¼ºé™·
    'Realizes': 2.5,       # å¡«å‘ï¼šå®ç°äº†å‰äººçš„Future Work
    'Extends': 1.0,        # é‡å˜ï¼šæ€§èƒ½æå‡
    'Alternative': 1.0,    # æ—æ”¯ï¼šå¦è¾Ÿè¹Šå¾„
    'Adapts_to': 0.5,      # è¿ç§»ï¼šæ¨ªå‘æ‰©æ•£
    'Baselines': 0.1,      # èƒŒæ™¯å™ªéŸ³ï¼šå‡ ä¹å¿½ç•¥
    'Unknown': 0.3         # æœªçŸ¥ç±»å‹ï¼šä½æƒé‡
}


class TopicEvolutionAnalyzer:
    """
    ä¸»é¢˜å‘å±•è„‰ç»œåˆ†æå™¨

    åŸºäºçŸ¥è¯†å›¾è°±ï¼ˆNetworkX Graphï¼‰åˆ†æç ”ç©¶ä¸»é¢˜çš„æ¼”åŒ–è§„å¾‹
    """

    def __init__(self, config: Dict = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«topic_evolutionç›¸å…³é…ç½®
        """
        # é»˜è®¤é…ç½®
        self.config = config or {}

        # æå–topic_evolutioné…ç½®
        evolution_config = self.config.get('topic_evolution', {})

        # é‡Œç¨‹ç¢‘è®ºæ–‡é…ç½®
        milestone_config = evolution_config.get('milestone', {})
        self.milestone_top_count = milestone_config.get('top_count', 10)
        self.milestone_citation_weight = milestone_config.get('citation_weight', 0.5)
        self.milestone_pagerank_weight = milestone_config.get('pagerank_weight', 1000)
        self.milestone_betweenness_weight = milestone_config.get('betweenness_weight', 500)
        self.milestone_display_count = milestone_config.get('display_count', 3)

        # ç ”ç©¶åˆ†æ”¯é…ç½®
        branch_config = evolution_config.get('branch', {})
        self.branch_min_size = branch_config.get('min_size', 3)
        self.branch_top_keywords = branch_config.get('top_keywords', 5)
        self.branch_display_count = branch_config.get('display_count', 3)
        self.branch_min_avg_citations = branch_config.get('min_avg_citations', 0)

        # å¼•ç”¨é“¾è·¯é…ç½®
        chain_config = evolution_config.get('citation_chain', {})
        self.chain_max_chains = chain_config.get('max_chains', 5)
        self.chain_min_length = chain_config.get('min_length', 3)
        self.chain_start_from_top = chain_config.get('start_from_top_milestones', 5)

        # æ—¶é—´æ¼”åŒ–é…ç½®
        time_config = evolution_config.get('time_evolution', {})
        self.time_top_papers_per_year = time_config.get('top_papers_per_year', 3)
        self.time_include_citation_types = time_config.get('include_citation_types', True)

        # åˆ›æ–°æ¨¡å¼é…ç½®
        pattern_config = evolution_config.get('innovation_pattern', {})
        self.pattern_examples_per_type = pattern_config.get('examples_per_type', 3)
        self.pattern_sort_by_count = pattern_config.get('sort_by_count', True)

        # å…³é”®è¯æå–é…ç½®
        keyword_config = evolution_config.get('keyword_extraction', {})
        self.keyword_min_length = keyword_config.get('min_word_length', 3)
        self.keyword_remove_stopwords = keyword_config.get('remove_stopwords', True)
        self.keyword_case_sensitive = keyword_config.get('case_sensitive', False)

        # å…³é”®è¿›åŒ–è·¯å¾„é…ç½®
        evolutionary_path_config = evolution_config.get('evolutionary_path', {})
        self.evol_enabled = evolutionary_path_config.get('enabled', True)
        self.evol_max_paths = evolutionary_path_config.get('max_paths', 3)
        self.evol_min_weight = evolutionary_path_config.get('min_total_weight', 3.0)
        self.evol_time_window_years = evolutionary_path_config.get('time_window_years', None)
        self.evol_custom_weights = evolutionary_path_config.get('custom_weights', {}) or {}

        # åˆå¹¶è‡ªå®šä¹‰æƒé‡
        self.evolutionary_weights = EVOLUTIONARY_WEIGHTS.copy()
        if self.evol_custom_weights:
            self.evolutionary_weights.update(self.evol_custom_weights)

        # æŠ€æœ¯åˆ†æ­§ç‚¹æ£€æµ‹é…ç½®
        bifurcation_config = evolution_config.get('bifurcation', {})
        self.bifur_enabled = bifurcation_config.get('enabled', True)
        self.bifur_max_bifurcations = bifurcation_config.get('max_bifurcations', 5)
        self.bifur_fork_edge_types = bifurcation_config.get('fork_edge_types', ['Alternative', 'Extends'])
        self.bifur_min_children = bifurcation_config.get('min_children', 2)
        self.bifur_method_sim_threshold = bifurcation_config.get('method_similarity_threshold', 0.3)
        self.bifur_problem_sim_threshold = bifurcation_config.get('problem_similarity_threshold', 0.6)
        self.bifur_use_cosine = bifurcation_config.get('use_cosine_similarity', True)

        # æœªé—­åˆå‰æ²¿æ¢æµ‹é…ç½®
        frontier_config = evolution_config.get('open_frontier', {})
        self.frontier_enabled = frontier_config.get('enabled', True)
        self.frontier_recent_years = frontier_config.get('recent_years', 2)
        self.frontier_max_open_problems = frontier_config.get('max_open_problems', 10)
        self.frontier_max_ideas = frontier_config.get('max_cross_domain_ideas', 5)
        self.frontier_lim_sim_threshold = frontier_config.get('limitation_similarity_threshold', 0.5)
        self.frontier_min_contrib_score = frontier_config.get('min_contribution_score', 0.3)

        logger.info(f"TopicEvolutionAnalyzeråˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  é‡Œç¨‹ç¢‘è®ºæ–‡: Top {self.milestone_top_count}, æ˜¾ç¤º {self.milestone_display_count}")
        logger.info(f"  ç ”ç©¶åˆ†æ”¯: æœ€å°è§„æ¨¡ {self.branch_min_size}, å…³é”®è¯ {self.branch_top_keywords}")
        logger.info(f"  å¼•ç”¨é“¾è·¯: æœ€å¤§ {self.chain_max_chains} æ¡, æœ€å°é•¿åº¦ {self.chain_min_length}")
        logger.info(f"  å…³é”®è¿›åŒ–è·¯å¾„: {'å¯ç”¨' if self.evol_enabled else 'ç¦ç”¨'}, æœ€å¤š {self.evol_max_paths} æ¡")
        logger.info(f"  æŠ€æœ¯åˆ†æ­§ç‚¹æ£€æµ‹: {'å¯ç”¨' if self.bifur_enabled else 'ç¦ç”¨'}, æœ€å¤š {self.bifur_max_bifurcations} ä¸ª")
        logger.info(f"  æœªé—­åˆå‰æ²¿æ¢æµ‹: {'å¯ç”¨' if self.frontier_enabled else 'ç¦ç”¨'}, æœ€å¤š {self.frontier_max_open_problems} ä¸ªé—®é¢˜")

    def analyze(self, graph: nx.DiGraph, topic: str) -> Dict:
        """
        æ‰§è¡Œå®Œæ•´çš„ä¸»é¢˜å‘å±•è„‰ç»œåˆ†æï¼ˆåŒæ ¸å¿ƒæ–¹å‘ï¼‰

        æ ¸å¿ƒæ–¹å‘1: å›æº¯è„‰ç»œ (Retrospective Analysis)
        æ ¸å¿ƒæ–¹å‘2: é¢„æµ‹æœªæ¥ (Future Prediction)

        Args:
            graph: NetworkXæœ‰å‘å›¾ï¼ŒèŠ‚ç‚¹åŒ…å«è®ºæ–‡ä¿¡æ¯
            topic: ç ”ç©¶ä¸»é¢˜åç§°

        Returns:
            åˆ†ææŠ¥å‘Šå­—å…¸
        """
        if len(graph.nodes()) == 0:
            logger.warning("çŸ¥è¯†å›¾è°±ä¸ºç©ºï¼Œè·³è¿‡ä¸»é¢˜å‘å±•è„‰ç»œåˆ†æ")
            return {}

        logger.info(f"å¼€å§‹åˆ†æä¸»é¢˜å‘å±•è„‰ç»œ: '{topic}'")
        logger.info(f"  å›¾è°±è§„æ¨¡: {len(graph.nodes())} èŠ‚ç‚¹, {len(graph.edges())} è¾¹")

        # åŸºç¡€åˆ†æ
        year_stats = self._analyze_time_evolution(graph)
        milestone_papers = self._identify_milestone_papers(graph)

        # ========== æ ¸å¿ƒæ–¹å‘1: å›æº¯è„‰ç»œ ==========
        logger.info("\n  ğŸ”™ æ ¸å¿ƒæ–¹å‘1: å›æº¯è„‰ç»œåˆ†æ...")

        # 1.1 è¯†åˆ«è¿›åŒ–ä¸»å¹² vs æ—æ”¯ä¿®è¡¥
        logger.info("    ğŸ“ è¯†åˆ«è¿›åŒ–ä¸»å¹² vs æ—æ”¯ä¿®è¡¥...")
        backbone_analysis = self._analyze_backbone_vs_incremental(graph)

        # 1.2 è¯†åˆ«æŠ€æœ¯åˆ†å‰å£
        logger.info("    ğŸ”€ è¯†åˆ«æŠ€æœ¯åˆ†å‰å£...")
        bifurcations = self._detect_technical_bifurcations(graph) if self.bifur_enabled else []

        # 1.3 è¯†åˆ«è·¨ç•Œå…¥ä¾µ
        logger.info("    ğŸŒ è¯†åˆ«è·¨ç•Œå…¥ä¾µ...")
        cross_domain_invasions = self._detect_cross_domain_invasions(graph)

        # ========== æ ¸å¿ƒæ–¹å‘2: é¢„æµ‹æœªæ¥ ==========
        logger.info("\n  ğŸ”® æ ¸å¿ƒæ–¹å‘2: é¢„æµ‹æœªæ¥...")

        # 2.1 ç­‰çº§ä¸€ï¼šæ¡æ¼å‹Ideaï¼ˆæœªå®ç°çš„Future Workï¼‰
        logger.info("    ğŸ’¡ ç­‰çº§ä¸€ï¼šæ¡æ¼å‹Idea...")
        low_hanging_fruits = self._detect_low_hanging_fruits(graph, year_stats)

        # 2.2 ç­‰çº§äºŒï¼šæ”»åšå‹Ideaï¼ˆæœªè§£å†³çš„Limitationï¼‰
        logger.info("    ğŸ”¨ ç­‰çº§äºŒï¼šæ”»åšå‹Idea...")
        hard_nuts = self._detect_hard_nuts(graph, milestone_papers)

        # 2.3 ç­‰çº§ä¸‰ï¼šåˆ›æ–°å‹Ideaï¼ˆè·¨åŸŸè¿ç§»/ç»„åˆæ‹³ï¼‰
        logger.info("    ğŸš€ ç­‰çº§ä¸‰ï¼šåˆ›æ–°å‹Idea...")
        innovative_ideas = self._generate_innovative_ideas(graph)

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'topic': topic,
            'analysis_time': datetime.now().isoformat(),
            'graph_overview': {
                'total_papers': len(graph.nodes()),
                'total_citations': len(graph.edges()),
                'year_range': f"{min(year_stats.keys())}-{max(year_stats.keys())}" if year_stats else "Unknown"
            },

            # æ ¸å¿ƒæ–¹å‘1: å›æº¯è„‰ç»œ
            'retrospective_analysis': {
                'backbone_vs_incremental': backbone_analysis,
                'technical_bifurcations': bifurcations,
                'cross_domain_invasions': cross_domain_invasions
            },

            # æ ¸å¿ƒæ–¹å‘2: é¢„æµ‹æœªæ¥
            'future_prediction': {
                'level_1_low_hanging_fruits': low_hanging_fruits,
                'level_2_hard_nuts': hard_nuts,
                'level_3_innovative_ideas': innovative_ideas
            },

            # ä¿ç•™åŸæœ‰åˆ†æï¼ˆå…¼å®¹æ€§ï¼‰
            'milestone_papers': milestone_papers,
            'time_evolution': dict(sorted(year_stats.items()))
        }

        # è¾“å‡ºæ¦‚è¦ä¿¡æ¯
        self._log_summary(report)

        return report

    def _analyze_time_evolution(self, graph: nx.DiGraph) -> Dict:
        """
        åˆ†ææ—¶é—´æ¼”åŒ–

        Returns:
            å¹´ä»½ç»Ÿè®¡å­—å…¸
        """
        year_stats = defaultdict(lambda: {
            'papers': [],
            'citation_types': defaultdict(int),
            'avg_citations': 0
        })

        # æ”¶é›†æ¯å¹´çš„è®ºæ–‡
        for node_id, node_data in graph.nodes(data=True):
            year = node_data.get('year')
            if year:
                year_stats[year]['papers'].append({
                    'id': node_id,
                    'title': node_data.get('title', ''),
                    'cited_by_count': node_data.get('cited_by_count', 0)
                })

        # è®¡ç®—æ¯å¹´çš„ç»Ÿè®¡ä¿¡æ¯
        for year, stats in year_stats.items():
            if stats['papers']:
                # å¹³å‡å¼•ç”¨æ•°
                stats['avg_citations'] = sum(
                    p['cited_by_count'] for p in stats['papers']
                ) / len(stats['papers'])

                # æŒ‰å¼•ç”¨æ•°æ’åºï¼Œåªä¿ç•™top N
                stats['papers'] = sorted(
                    stats['papers'],
                    key=lambda x: x['cited_by_count'],
                    reverse=True
                )[:self.time_top_papers_per_year]

        # ç»Ÿè®¡æ¯å¹´çš„å¼•ç”¨ç±»å‹åˆ†å¸ƒ
        if self.time_include_citation_types:
            for source, target, edge_data in graph.edges(data=True):
                source_year = graph.nodes[source].get('year')
                edge_type = edge_data.get('edge_type', 'Unknown')
                if source_year:
                    year_stats[source_year]['citation_types'][edge_type] += 1

        return year_stats

    def _identify_milestone_papers(self, graph: nx.DiGraph) -> List[Dict]:
        """
        è¯†åˆ«é‡Œç¨‹ç¢‘è®ºæ–‡

        ä½¿ç”¨ç»¼åˆè¯„åˆ†ï¼šå¼•ç”¨æ•° + PageRank + ä¸­ä»‹ä¸­å¿ƒæ€§

        Returns:
            é‡Œç¨‹ç¢‘è®ºæ–‡åˆ—è¡¨
        """
        # è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§æŒ‡æ ‡
        try:
            pagerank = nx.pagerank(graph, alpha=0.85)
            betweenness = nx.betweenness_centrality(graph)
        except Exception as e:
            logger.warning(f"è®¡ç®—å›¾æŒ‡æ ‡å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            pagerank = {node: 0 for node in graph.nodes()}
            betweenness = {node: 0 for node in graph.nodes()}

        # ç»¼åˆè¯„åˆ†
        milestone_papers = []
        for node_id in graph.nodes():
            node_data = graph.nodes[node_id]
            score = (
                node_data.get('cited_by_count', 0) * self.milestone_citation_weight +
                pagerank.get(node_id, 0) * self.milestone_pagerank_weight +
                betweenness.get(node_id, 0) * self.milestone_betweenness_weight
            )
            milestone_papers.append({
                'id': node_id,
                'title': node_data.get('title', ''),
                'year': node_data.get('year'),
                'cited_by_count': node_data.get('cited_by_count', 0),
                'pagerank': pagerank.get(node_id, 0),
                'betweenness': betweenness.get(node_id, 0),
                'score': score
            })

        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        milestone_papers = sorted(
            milestone_papers,
            key=lambda x: x['score'],
            reverse=True
        )[:self.milestone_top_count]

        return milestone_papers

    def _analyze_research_branches(self, graph: nx.DiGraph) -> List[Dict]:
        """
        åˆ†æç ”ç©¶åˆ†æ”¯ï¼ˆç¤¾åŒºæ£€æµ‹ï¼‰

        ä½¿ç”¨Louvainç®—æ³•è¿›è¡Œç¤¾åŒºæ£€æµ‹

        Returns:
            ç ”ç©¶åˆ†æ”¯åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨Louvainç®—æ³•è¿›è¡Œç¤¾åŒºæ£€æµ‹
            communities = nx.community.louvain_communities(graph.to_undirected())

            research_branches = []
            for i, community in enumerate(communities):
                if len(community) < self.branch_min_size:
                    continue

                # åˆ†æè¯¥åˆ†æ”¯çš„ç‰¹å¾
                branch_papers = []
                branch_years = []
                branch_citations = []

                for node_id in community:
                    node_data = graph.nodes[node_id]
                    branch_papers.append({
                        'id': node_id,
                        'title': node_data.get('title', ''),
                        'year': node_data.get('year')
                    })
                    if node_data.get('year'):
                        branch_years.append(node_data.get('year'))
                    branch_citations.append(node_data.get('cited_by_count', 0))

                # è®¡ç®—å¹³å‡å¼•ç”¨æ•°
                avg_citations = sum(branch_citations) / len(branch_citations) if branch_citations else 0

                # è¿‡æ»¤ä½è´¨é‡åˆ†æ”¯
                if avg_citations < self.branch_min_avg_citations:
                    continue

                # è¯†åˆ«åˆ†æ”¯çš„å…³é”®è¯
                branch_keywords = self._extract_keywords(
                    [p['title'] for p in branch_papers],
                    top_k=self.branch_top_keywords
                )

                research_branches.append({
                    'branch_id': i + 1,
                    'size': len(community),
                    'papers': sorted(branch_papers, key=lambda x: x.get('year', 0))[:5],
                    'year_range': f"{min(branch_years)}-{max(branch_years)}" if branch_years else "Unknown",
                    'avg_citations': avg_citations,
                    'keywords': branch_keywords
                })

            # æŒ‰è§„æ¨¡æ’åº
            research_branches = sorted(
                research_branches,
                key=lambda x: x['size'],
                reverse=True
            )

            return research_branches

        except Exception as e:
            logger.warning(f"ç¤¾åŒºæ£€æµ‹å¤±è´¥: {e}")
            return []

    def _analyze_citation_chains(
        self,
        graph: nx.DiGraph,
        milestone_papers: List[Dict]
    ) -> List[Dict]:
        """
        åˆ†æå¼•ç”¨é“¾è·¯ï¼ˆå¼•ç”¨ä¼ æ‰¿è·¯å¾„ï¼‰

        ä»é‡Œç¨‹ç¢‘è®ºæ–‡å¼€å§‹ï¼Œæ‰¾å‡ºæœ€é•¿çš„å¼•ç”¨é“¾

        Args:
            graph: çŸ¥è¯†å›¾è°±
            milestone_papers: é‡Œç¨‹ç¢‘è®ºæ–‡åˆ—è¡¨

        Returns:
            å¼•ç”¨é“¾è·¯åˆ—è¡¨
        """
        citation_chains = []

        try:
            # ä»top Né‡Œç¨‹ç¢‘è®ºæ–‡å¼€å§‹è¿½è¸ª
            for start_node in milestone_papers[:self.chain_start_from_top]:
                start_id = start_node['id']
                if start_id not in graph:
                    continue

                # æ‰¾å‡ºä»è¯¥èŠ‚ç‚¹å‡ºå‘çš„æœ€é•¿è·¯å¾„
                lengths = nx.single_source_shortest_path_length(graph, start_id)
                if not lengths:
                    continue

                farthest_node = max(lengths.items(), key=lambda x: x[1])

                # æ£€æŸ¥è·¯å¾„é•¿åº¦
                if farthest_node[1] < self.chain_min_length - 1:  # -1 å› ä¸ºé•¿åº¦æ˜¯è¾¹æ•°
                    continue

                # è·å–è·¯å¾„
                path = nx.shortest_path(graph, start_id, farthest_node[0])
                if len(path) < self.chain_min_length:
                    continue

                # æ„å»ºé“¾è·¯ä¿¡æ¯
                chain_info = []
                for node in path:
                    node_data = graph.nodes[node]
                    chain_info.append({
                        'id': node,
                        'title': node_data.get('title', '')[:60],
                        'year': node_data.get('year')
                    })

                citation_chains.append({
                    'length': len(path),
                    'chain': chain_info
                })

        except Exception as e:
            logger.warning(f"å¼•ç”¨é“¾è·¯åˆ†æå¤±è´¥: {e}")

        # æŒ‰é•¿åº¦æ’åºï¼Œå–top N
        citation_chains = sorted(
            citation_chains,
            key=lambda x: x['length'],
            reverse=True
        )[:self.chain_max_chains]

        return citation_chains

    def _analyze_innovation_patterns(self, graph: nx.DiGraph) -> Dict:
        """
        åˆ†æåˆ›æ–°æ¨¡å¼ï¼ˆå¼•ç”¨ç±»å‹ç»Ÿè®¡ï¼‰

        ç»Ÿè®¡ä¸åŒå¼•ç”¨ç±»å‹çš„åˆ†å¸ƒå’Œç¤ºä¾‹

        Returns:
            åˆ›æ–°æ¨¡å¼å­—å…¸
        """
        innovation_patterns = defaultdict(lambda: {
            'count': 0,
            'examples': []
        })

        for source, target, edge_data in graph.edges(data=True):
            edge_type = edge_data.get('edge_type', 'Unknown')
            innovation_patterns[edge_type]['count'] += 1

            # ä¿å­˜ç¤ºä¾‹
            if len(innovation_patterns[edge_type]['examples']) < self.pattern_examples_per_type:
                source_data = graph.nodes[source]
                target_data = graph.nodes[target]
                innovation_patterns[edge_type]['examples'].append({
                    'from': source_data.get('title', '')[:50],
                    'to': target_data.get('title', '')[:50],
                    'from_year': source_data.get('year'),
                    'to_year': target_data.get('year')
                })

        # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
        return {k: dict(v) for k, v in innovation_patterns.items()}

    def _extract_keywords(self, titles: List[str], top_k: int = 5) -> List[str]:
        """
        ä»æ ‡é¢˜åˆ—è¡¨ä¸­æå–å…³é”®è¯

        Args:
            titles: æ ‡é¢˜åˆ—è¡¨
            top_k: è¿”å›top kå…³é”®è¯

        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # åœç”¨è¯
        stopwords = {
            'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
            'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'could', 'should', 'may', 'might', 'can', 'using', 'based', 'via',
            'through', 'into', 'over', 'after', 'before', 'between', 'under'
        } if self.keyword_remove_stopwords else set()

        # æå–æ‰€æœ‰å•è¯
        words = []
        for title in titles:
            # è½¬å°å†™ï¼ˆå¦‚æœä¸åŒºåˆ†å¤§å°å†™ï¼‰
            title_processed = title if self.keyword_case_sensitive else title.lower()

            # æå–å•è¯ï¼ˆæœ€å°é•¿åº¦ï¼‰
            pattern = rf'\b[a-zA-Z]{{{self.keyword_min_length},}}\b'
            if not self.keyword_case_sensitive:
                pattern = rf'\b[a-z]{{{self.keyword_min_length},}}\b'

            words_in_title = re.findall(pattern, title_processed)
            words.extend([w for w in words_in_title if w not in stopwords])

        # ç»Ÿè®¡é¢‘æ¬¡
        word_counts = Counter(words)

        # è¿”å›top kå…³é”®è¯
        return [word for word, count in word_counts.most_common(top_k)]

    def _analyze_backbone_vs_incremental(self, graph: nx.DiGraph) -> Dict:
        """
        è¯†åˆ«è¿›åŒ–ä¸»å¹² vs æ—æ”¯ä¿®è¡¥ï¼ˆå¢å¼ºç‰ˆï¼‰

        æ ¸å¿ƒæ´å¯Ÿï¼š
        1. ä¸»å¹²è·¯å¾„ï¼ˆBackboneï¼‰ï¼šåªä¿ç•™ Overcomes å’Œ Realizes ç±»å‹çš„è¿æ¥
           - å«é‡‘é‡æœ€é«˜çš„æ¼”åŒ–çº¿
           - æ¯ä¸ªèŠ‚ç‚¹éƒ½åœ¨è§£å†³å‰äººçš„è‡´å‘½ç¼ºé™·

        2. æ¸è¿›è·¯å¾„ï¼ˆIncrementalï¼‰ï¼šåªä¿ç•™ Extends ç±»å‹çš„è¿æ¥
           - å†…å·/åˆ·æ¦œçš„æ¼”åŒ–çº¿
           - åŒä¸€æ–¹æ³•è®ºä¸‹çš„å¾®è°ƒä¼˜åŒ–

        3. ç ´å±€ç‚¹ï¼ˆBreakthroughï¼‰ï¼šä» Extends å†…å·ä¸­çªç„¶è·³å‡º Overcomes
           - è¿™æ˜¯æœ€æœ‰ä»·å€¼çš„åˆ›æ–°ç‚¹
           - ä»£è¡¨æ–¹æ³•è®ºçš„çªç ´

        Returns:
            è¯¦ç»†çš„ä¸»å¹²vsæ—æ”¯åˆ†ææŠ¥å‘Š
        """
        backbone_paths = []
        incremental_paths = []
        breakthrough_points = []

        # ç»Ÿè®¡æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥è¾“å‡ºè¾¹ç±»å‹
        node_stats = {}

        for node in graph.nodes():
            in_edges = list(graph.in_edges(node, data=True))
            out_edges = list(graph.out_edges(node, data=True))

            in_overcomes = sum(1 for _, _, d in in_edges if d.get('edge_type') == 'Overcomes')
            in_realizes = sum(1 for _, _, d in in_edges if d.get('edge_type') == 'Realizes')
            in_extends = sum(1 for _, _, d in in_edges if d.get('edge_type') == 'Extends')

            out_overcomes = sum(1 for _, _, d in out_edges if d.get('edge_type') == 'Overcomes')
            out_realizes = sum(1 for _, _, d in out_edges if d.get('edge_type') == 'Realizes')
            out_extends = sum(1 for _, _, d in out_edges if d.get('edge_type') == 'Extends')

            node_stats[node] = {
                'in_overcomes': in_overcomes,
                'in_realizes': in_realizes,
                'in_extends': in_extends,
                'out_overcomes': out_overcomes,
                'out_realizes': out_realizes,
                'out_extends': out_extends
            }

        # æ„å»ºä¸»å¹²è·¯å¾„å’Œæ¸è¿›è·¯å¾„
        for source, target, data in graph.edges(data=True):
            edge_type = data.get('edge_type', 'Unknown')

            if edge_type in ['Overcomes', 'Realizes']:
                backbone_paths.append({
                    'from': {
                        'id': source,
                        'title': graph.nodes[source].get('title', '')[:60],
                        'year': graph.nodes[source].get('year')
                    },
                    'to': {
                        'id': target,
                        'title': graph.nodes[target].get('title', '')[:60],
                        'year': graph.nodes[target].get('year')
                    },
                    'type': edge_type
                })
            elif edge_type == 'Extends':
                incremental_paths.append({
                    'from': {
                        'id': source,
                        'title': graph.nodes[source].get('title', '')[:60],
                        'year': graph.nodes[source].get('year')
                    },
                    'to': {
                        'id': target,
                        'title': graph.nodes[target].get('title', '')[:60],
                        'year': graph.nodes[target].get('year')
                    }
                })

        # è¯†åˆ«ç ´å±€ç‚¹ï¼šä» Extends å†…å·ä¸­è·³å‡º Overcomes
        # æ ‡å‡†ï¼šè¾“å…¥æœ‰å¤šä¸ª Extendsï¼Œè¾“å‡ºæœ‰ Overcomes/Realizes
        for node, stats in node_stats.items():
            # åˆ¤æ–­æ˜¯å¦ä¸ºç ´å±€ç‚¹
            is_breakthrough = False
            breakthrough_score = 0

            # æ¨¡å¼1ï¼šä»Extendså†…å·ä¸­è·³å‡ºOvercomesï¼ˆæœ€ç»å…¸ï¼‰
            if stats['in_extends'] >= 2 and stats['out_overcomes'] >= 1:
                is_breakthrough = True
                breakthrough_score = stats['in_extends'] * 1.0 + stats['out_overcomes'] * 3.0

            # æ¨¡å¼2ï¼šä»çº¯Extendså˜ä¸ºRealizesï¼ˆå¡«å‘å‹çªç ´ï¼‰
            elif stats['in_extends'] >= 1 and stats['out_realizes'] >= 1 and stats['out_overcomes'] == 0:
                is_breakthrough = True
                breakthrough_score = stats['in_extends'] * 0.5 + stats['out_realizes'] * 2.0

            if is_breakthrough:
                node_data = graph.nodes[node]
                breakthrough_points.append({
                    'node': node,
                    'title': node_data.get('title', '')[:80],
                    'year': node_data.get('year'),
                    'cited_by_count': node_data.get('cited_by_count', 0),
                    'in_extends': stats['in_extends'],
                    'out_overcomes': stats['out_overcomes'],
                    'out_realizes': stats['out_realizes'],
                    'breakthrough_score': breakthrough_score,
                    'breakthrough_type': 'Overcomesçªç ´' if stats['out_overcomes'] > 0 else 'Realizeså¡«å‘'
                })

        # æŒ‰çªç ´åˆ†æ•°æ’åº
        breakthrough_points = sorted(
            breakthrough_points,
            key=lambda x: x['breakthrough_score'],
            reverse=True
        )[:10]

        # åˆ†æä¸»å¹²è·¯å¾„çš„è¿è´¯æ€§
        backbone_chains = self._extract_backbone_chains(graph, backbone_paths)

        # åˆ†ææ¸è¿›è·¯å¾„çš„ç“¶é¢ˆ
        incremental_bottlenecks = self._analyze_incremental_bottlenecks(graph, incremental_paths)

        return {
            'summary': {
                'backbone_count': len(backbone_paths),
                'incremental_count': len(incremental_paths),
                'breakthrough_count': len(breakthrough_points),
                'ratio': len(backbone_paths) / len(incremental_paths) if len(incremental_paths) > 0 else 0
            },
            'backbone_paths': backbone_paths[:20],  # è¿”å›å‰20ä¸ªä¸»å¹²è·¯å¾„
            'incremental_paths': incremental_paths[:20],  # è¿”å›å‰20ä¸ªæ¸è¿›è·¯å¾„
            'breakthrough_points': breakthrough_points,
            'backbone_chains': backbone_chains,  # ä¸»å¹²è¿ç»­é“¾
            'incremental_bottlenecks': incremental_bottlenecks  # æ¸è¿›ç“¶é¢ˆ
        }

    def _extract_backbone_chains(self, graph: nx.DiGraph, backbone_paths: List[Dict]) -> List[Dict]:
        """
        æå–ä¸»å¹²è¿ç»­é“¾ï¼šè¿ç»­çš„Overcomes/Realizesè·¯å¾„

        è¿™äº›é“¾ä»£è¡¨äº†"ç¡¬æ ¸æ”»åš"çš„æ¼”åŒ–çº¿
        """
        # æ„å»ºåªåŒ…å«ä¸»å¹²è¾¹çš„å­å›¾
        backbone_graph = nx.DiGraph()

        for path in backbone_paths:
            source = path['from']['id']
            target = path['to']['id']
            edge_type = path['type']
            backbone_graph.add_edge(source, target, edge_type=edge_type)

            # æ·»åŠ èŠ‚ç‚¹å±æ€§
            for node_id in [source, target]:
                if node_id in graph.nodes():
                    node_data = graph.nodes[node_id]
                    backbone_graph.add_node(node_id, **node_data)

        # æ‰¾å‡ºæ‰€æœ‰ç®€å•è·¯å¾„ï¼ˆé•¿åº¦>=3ï¼‰
        chains = []

        # æ‰¾å‡ºèµ·ç‚¹ï¼ˆå…¥åº¦ä¸º0çš„èŠ‚ç‚¹ï¼‰
        source_nodes = [n for n in backbone_graph.nodes() if backbone_graph.in_degree(n) == 0]
        # æ‰¾å‡ºç»ˆç‚¹ï¼ˆå‡ºåº¦ä¸º0çš„èŠ‚ç‚¹ï¼‰
        target_nodes = [n for n in backbone_graph.nodes() if backbone_graph.out_degree(n) == 0]

        for source in source_nodes:
            for target in target_nodes:
                if source == target:
                    continue

                if nx.has_path(backbone_graph, source, target):
                    # æ‰¾æœ€é•¿è·¯å¾„
                    try:
                        path = nx.shortest_path(backbone_graph, source, target)

                        if len(path) >= 3:  # è‡³å°‘3ä¸ªèŠ‚ç‚¹
                            chain_info = []
                            edge_types = []

                            for i, node in enumerate(path):
                                node_data = graph.nodes[node]
                                chain_info.append({
                                    'id': node,
                                    'title': node_data.get('title', '')[:50],
                                    'year': node_data.get('year')
                                })

                                # è·å–è¾¹ç±»å‹
                                if i < len(path) - 1:
                                    edge_data = backbone_graph[path[i]][path[i+1]]
                                    edge_types.append(edge_data.get('edge_type', 'Unknown'))

                            chains.append({
                                'length': len(path),
                                'chain': chain_info,
                                'edge_types': edge_types,
                                'year_span': chain_info[-1]['year'] - chain_info[0]['year'] if chain_info[0].get('year') and chain_info[-1].get('year') else 0
                            })
                    except:
                        continue

        # æŒ‰é•¿åº¦æ’åº
        chains = sorted(chains, key=lambda x: x['length'], reverse=True)[:5]

        return chains

    def _analyze_incremental_bottlenecks(self, graph: nx.DiGraph, incremental_paths: List[Dict]) -> List[Dict]:
        """
        åˆ†ææ¸è¿›è·¯å¾„çš„ç“¶é¢ˆï¼šæ‰¾åˆ°é‚£äº›è¢«å¤§é‡Extendså¼•ç”¨ä½†æ²¡æœ‰åç»­çªç ´çš„è®ºæ–‡

        è¿™äº›è®ºæ–‡ä»£è¡¨äº†"å†…å·ç»ˆç‚¹"ï¼Œå¯èƒ½å·²ç»æ¥è¿‘ç“¶é¢ˆ
        """
        # æ„å»ºåªåŒ…å«Extendsè¾¹çš„å­å›¾
        extends_graph = nx.DiGraph()

        for path in incremental_paths:
            source = path['from']['id']
            target = path['to']['id']
            extends_graph.add_edge(source, target)

            # æ·»åŠ èŠ‚ç‚¹å±æ€§
            for node_id in [source, target]:
                if node_id in graph.nodes():
                    node_data = graph.nodes[node_id]
                    extends_graph.add_node(node_id, **node_data)

        bottlenecks = []

        for node in extends_graph.nodes():
            in_degree = extends_graph.in_degree(node)

            # è¢«å¤§é‡Extendså¼•ç”¨ï¼ˆè‡³å°‘3ä¸ªï¼‰
            if in_degree >= 3:
                # æ£€æŸ¥æ˜¯å¦æœ‰åç»­çªç ´ï¼ˆåœ¨åŸå›¾ä¸­ï¼‰
                has_breakthrough = False

                for pred in graph.predecessors(node):
                    edge_data = graph[pred][node]
                    edge_type = edge_data.get('edge_type', 'Unknown')

                    if edge_type in ['Overcomes', 'Realizes']:
                        has_breakthrough = True
                        break

                # æ²¡æœ‰çªç ´çš„æ‰æ˜¯ç“¶é¢ˆ
                if not has_breakthrough:
                    node_data = graph.nodes[node]
                    bottlenecks.append({
                        'node': node,
                        'title': node_data.get('title', '')[:60],
                        'year': node_data.get('year'),
                        'cited_by_count': node_data.get('cited_by_count', 0),
                        'extends_in_count': in_degree,
                        'reason': f'è¢«{in_degree}ä¸ªExtendså¼•ç”¨ä½†æ— åç»­çªç ´ï¼Œå¯èƒ½å·²è¾¾ç“¶é¢ˆ'
                    })

        # æŒ‰Extendså¼•ç”¨æ•°æ’åº
        bottlenecks = sorted(bottlenecks, key=lambda x: x['extends_in_count'], reverse=True)[:5]

        return bottlenecks

    def _detect_cross_domain_invasions(self, graph: nx.DiGraph) -> List[Dict]:
        """
        è¯†åˆ«è·¨ç•Œå…¥ä¾µï¼ˆCross-Domain Invasionï¼‰- å¢å¼ºç‰ˆ

        æ ¸å¿ƒæ€æƒ³ï¼š
        è¿½è¸ª Adapts_to ç±»å‹çš„è¿æ¥ï¼Œè¯†åˆ«æ–¹æ³•è®ºçš„è·¨åŸŸè¿ç§»ã€‚
        è¿™ç§èŠ‚ç‚¹é€šå¸¸æ„å‘³ç€æ–¹æ³•è®ºçš„é™ç»´æ‰“å‡»ï¼Œæ˜¯è¯¥Topicçš„ä¸€æ¬¡é‡è¦å¤–éƒ¨è¾“è¡€ã€‚

        å¢å¼ºåˆ†æï¼š
        1. è¿½è¸ªè¿™äº›èŠ‚ç‚¹çš„å‰èº«ï¼ˆåŸå§‹é¢†åŸŸï¼‰
        2. åˆ†æè¿ç§»çš„å½±å“åŠ›ï¼ˆåç»­å·¥ä½œæ•°é‡ï¼‰
        3. è¯†åˆ«æœ€æˆåŠŸçš„è·¨åŸŸè¿ç§»æ¡ˆä¾‹

        Returns:
            è·¨ç•Œå…¥ä¾µåˆ—è¡¨ï¼ˆåŒ…å«å½±å“åŠ›åˆ†æï¼‰
        """
        invasions = []

        for source, target, data in graph.edges(data=True):
            if data.get('edge_type') == 'Adapts_to':
                source_data = graph.nodes[source]
                target_data = graph.nodes[target]

                # åˆ†æè¿ç§»çš„å½±å“åŠ›
                # 1. ç»Ÿè®¡sourceçš„åç»­å·¥ä½œæ•°é‡ï¼ˆè¿ç§»åçš„å½±å“ï¼‰
                source_descendants = list(nx.descendants(graph, source)) if source in graph else []
                impact_count = len(source_descendants)

                # 2. ç»Ÿè®¡targetçš„åŸæœ‰å½±å“åŠ›ï¼ˆè¢«è¿ç§»çš„åŸºç¡€ï¼‰
                target_citations = target_data.get('cited_by_count', 0)

                # 3. åˆ¤æ–­è¿ç§»æˆåŠŸåº¦
                if impact_count > 10:
                    success_level = 'highly_successful'  # é«˜åº¦æˆåŠŸ
                elif impact_count > 5:
                    success_level = 'successful'  # æˆåŠŸ
                elif impact_count > 0:
                    success_level = 'moderate'  # ä¸€èˆ¬
                else:
                    success_level = 'limited'  # æœ‰é™

                # 4. å°è¯•è¯†åˆ«åŸå§‹é¢†åŸŸå’Œç›®æ ‡é¢†åŸŸï¼ˆåŸºäºå…³é”®è¯ï¼‰
                source_keywords = self._extract_domain_keywords(source_data.get('title', ''))
                target_keywords = self._extract_domain_keywords(target_data.get('title', ''))

                invasions.append({
                    'from': {
                        'id': source,
                        'title': source_data.get('title', '')[:80],
                        'year': source_data.get('year'),
                        'domain_keywords': source_keywords
                    },
                    'to': {
                        'id': target,
                        'title': target_data.get('title', '')[:80],
                        'year': target_data.get('year'),
                        'citations': target_citations,
                        'domain_keywords': target_keywords
                    },
                    'impact_analysis': {
                        'descendants_count': impact_count,
                        'success_level': success_level,
                        'year_gap': source_data.get('year', 0) - target_data.get('year', 0) if source_data.get('year') and target_data.get('year') else 0
                    },
                    'cross_domain_story': self._generate_invasion_story(
                        source_keywords,
                        target_keywords,
                        source_data.get('year'),
                        impact_count,
                        success_level
                    )
                })

        # æŒ‰å½±å“åŠ›æ’åº
        invasions = sorted(invasions, key=lambda x: x['impact_analysis']['descendants_count'], reverse=True)[:15]

        return invasions

    def _extract_domain_keywords(self, title: str) -> List[str]:
        """
        ä»æ ‡é¢˜ä¸­æå–é¢†åŸŸå…³é”®è¯

        Args:
            title: è®ºæ–‡æ ‡é¢˜

        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # å¸¸è§é¢†åŸŸå…³é”®è¯
        domain_keywords_dict = {
            'nlp': ['language', 'text', 'nlp', 'semantic', 'linguistic', 'dialogue', 'translation', 'sentiment'],
            'cv': ['image', 'vision', 'visual', 'object', 'detection', 'segmentation', 'recognition', 'video'],
            'rl': ['reinforcement', 'policy', 'reward', 'agent', 'environment', 'q-learning'],
            'graph': ['graph', 'node', 'edge', 'network', 'topology'],
            'audio': ['audio', 'speech', 'sound', 'acoustic', 'voice'],
            'generative': ['generation', 'generative', 'gan', 'diffusion', 'synthesis'],
            'representation': ['representation', 'embedding', 'feature', 'encoding']
        }

        title_lower = title.lower()
        found_domains = []

        for domain, keywords in domain_keywords_dict.items():
            for kw in keywords:
                if kw in title_lower:
                    found_domains.append(domain)
                    break  # æ‰¾åˆ°ä¸€ä¸ªå°±å¤Ÿäº†

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›ä¸€äº›ä»æ ‡é¢˜ä¸­æå–çš„é€šç”¨å…³é”®è¯
        if not found_domains:
            words = re.findall(r'\b[a-z]{4,}\b', title_lower)
            return words[:3] if words else ['unknown']

        return found_domains

    def _generate_invasion_story(
        self,
        source_keywords: List[str],
        target_keywords: List[str],
        year: Optional[int],
        impact: int,
        success: str
    ) -> str:
        """
        ç”Ÿæˆè·¨åŸŸè¿ç§»çš„æ•…äº‹æè¿°

        Args:
            source_keywords: æºé¢†åŸŸå…³é”®è¯
            target_keywords: ç›®æ ‡é¢†åŸŸå…³é”®è¯
            year: è¿ç§»å¹´ä»½
            impact: å½±å“åŠ›ï¼ˆåç»­å·¥ä½œæ•°ï¼‰
            success: æˆåŠŸç¨‹åº¦

        Returns:
            æè¿°æ–‡æœ¬
        """
        source_str = ', '.join(source_keywords[:2]) if source_keywords else 'æœªçŸ¥é¢†åŸŸ'
        target_str = ', '.join(target_keywords[:2]) if target_keywords else 'æœªçŸ¥é¢†åŸŸ'

        success_desc = {
            'highly_successful': f'äº§ç”Ÿäº†{impact}ä¸ªåç»­å·¥ä½œï¼Œæˆä¸ºè¯¥é¢†åŸŸçš„é‡è¦çªç ´',
            'successful': f'äº§ç”Ÿäº†{impact}ä¸ªåç»­å·¥ä½œï¼Œè·å¾—è¾ƒå¥½å‘å±•',
            'moderate': f'äº§ç”Ÿäº†{impact}ä¸ªåç»­å·¥ä½œï¼Œæœ‰ä¸€å®šå½±å“',
            'limited': 'åç»­å‘å±•æœ‰é™'
        }

        year_str = f"åœ¨{year}å¹´ï¼Œ" if year else ""

        return f"{year_str}å°†[{target_str}]é¢†åŸŸçš„æ–¹æ³•è¿ç§»åˆ°[{source_str}]é¢†åŸŸï¼Œ{success_desc.get(success, 'å½±å“æœªçŸ¥')}"

    def _detect_low_hanging_fruits(self, graph: nx.DiGraph, year_stats: Dict) -> List[Dict]:
        """
        ç­‰çº§ä¸€ï¼šæ¡æ¼å‹Idea - å¯»æ‰¾æœªè¢«Realizedçš„Future Workï¼ˆå¢å¼ºç‰ˆï¼‰

        ç®—æ³•é€»è¾‘ï¼š
        1. æå–æœ€è¿‘3å¹´å‘è¡¨çš„è®ºæ–‡ï¼ˆ"å‰æ²¿"ï¼‰
        2. æå–å®ƒä»¬æ–‡æœ¬ä¸­çš„ Future_Work éƒ¨åˆ†
        3. ç¡®è®¤æ˜¯å¦å·²ç»æœ‰åç»­è®ºæ–‡é€šè¿‡ Realizes è¿æ¥åˆ°å®ƒ
        4. å¦‚æœæ²¡æœ‰ï¼Œé‚£ä¹ˆ"æŠŠè¿™ç¯‡è®ºæ–‡çš„Future Workåšå‡ºæ¥"å°±æ˜¯ä¸€ä¸ªç°æˆçš„Idea

        å¢å¼ºï¼š
        - æŒ‰è®ºæ–‡å½±å“åŠ›ï¼ˆå¼•ç”¨æ•°ï¼‰æ’åºï¼Œä¼˜å…ˆæ¨èé«˜å½±å“åŠ›è®ºæ–‡çš„Future Work
        - åˆ†æFuture Workçš„å¯è¡Œæ€§ï¼ˆé•¿åº¦ã€å…·ä½“æ€§ï¼‰
        - æä¾›å®ç°éš¾åº¦è¯„ä¼°

        Returns:
            æ¡æ¼å‹Ideaåˆ—è¡¨
        """
        if not year_stats:
            return []

        years = sorted(year_stats.keys())
        recent_cutoff = years[-1] - 3 if len(years) > 3 else years[0]

        low_hanging = []

        for node in graph.nodes():
            node_data = graph.nodes[node]
            year = node_data.get('year')

            if not year or year < recent_cutoff:
                continue

            # ä»deep_analysisç»“æ„ä¸­è·å–future_work
            deep_analysis = node_data.get('deep_analysis', {})
            future_work = deep_analysis.get('future_work', {}).get('content', '')

            if not future_work or len(future_work) < 20:
                continue

            # æ£€æŸ¥æ˜¯å¦æœ‰åç»­å·¥ä½œé€šè¿‡Realizeså®ç°
            has_realization = False
            for pred in graph.predecessors(node):
                edge_data = graph[pred][node]
                if edge_data.get('edge_type') == 'Realizes':
                    has_realization = True
                    break

            if not has_realization:
                # è¯„ä¼°å¯è¡Œæ€§ï¼ˆåŸºäºæè¿°é•¿åº¦å’Œå…·ä½“æ€§ï¼‰
                feasibility_score = self._assess_idea_feasibility(future_work)

                # è¯„ä¼°éš¾åº¦
                difficulty = self._assess_implementation_difficulty(future_work, node_data)

                low_hanging.append({
                    'paper': {
                        'id': node,
                        'title': node_data.get('title', '')[:80],
                        'year': year,
                        'cited_by_count': node_data.get('cited_by_count', 0)
                    },
                    'future_work': future_work[:300],
                    'feasibility_score': feasibility_score,
                    'difficulty': difficulty,
                    'priority': node_data.get('cited_by_count', 0) * feasibility_score,  # ç»¼åˆä¼˜å…ˆçº§
                    'recommendation': self._generate_implementation_recommendation(future_work, difficulty)
                })

        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆå¼•ç”¨æ•° * å¯è¡Œæ€§ï¼‰
        low_hanging = sorted(low_hanging, key=lambda x: x['priority'], reverse=True)[:15]

        return low_hanging

    def _assess_idea_feasibility(self, future_work: str) -> float:
        """
        è¯„ä¼°Future Workçš„å¯è¡Œæ€§

        Args:
            future_work: Future Workæè¿°

        Returns:
            å¯è¡Œæ€§åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        score = 0.5  # åŸºç¡€åˆ†

        # é•¿åº¦è¶Šé•¿è¶Šå…·ä½“
        if len(future_work) > 100:
            score += 0.2
        elif len(future_work) > 200:
            score += 0.3

        # åŒ…å«å…·ä½“å…³é”®è¯
        action_keywords = ['apply', 'extend', 'improve', 'combine', 'test', 'evaluate', 'implement']
        for kw in action_keywords:
            if kw in future_work.lower():
                score += 0.1
                break

        # åŒ…å«å…·ä½“æ–¹æ³•æˆ–æ•°æ®é›†
        specific_keywords = ['dataset', 'benchmark', 'algorithm', 'model', 'framework']
        for kw in specific_keywords:
            if kw in future_work.lower():
                score += 0.1
                break

        return min(score, 1.0)

    def _assess_implementation_difficulty(self, future_work: str, paper_data: Dict) -> str:
        """
        è¯„ä¼°å®ç°éš¾åº¦

        Args:
            future_work: Future Workæè¿°
            paper_data: è®ºæ–‡æ•°æ®

        Returns:
            éš¾åº¦ç­‰çº§ï¼š'easy', 'medium', 'hard'
        """
        # åŸºäºå…³é”®è¯åˆ¤æ–­éš¾åº¦
        easy_keywords = ['extend', 'apply', 'test', 'evaluate', 'additional']
        medium_keywords = ['improve', 'enhance', 'combine', 'integrate']
        hard_keywords = ['novel', 'new', 'develop', 'design', 'fundamental', 'theoretical']

        future_lower = future_work.lower()

        hard_count = sum(1 for kw in hard_keywords if kw in future_lower)
        medium_count = sum(1 for kw in medium_keywords if kw in future_lower)
        easy_count = sum(1 for kw in easy_keywords if kw in future_lower)

        if hard_count > medium_count and hard_count > easy_count:
            return 'hard'
        elif medium_count > easy_count:
            return 'medium'
        else:
            return 'easy'

    def _generate_implementation_recommendation(self, future_work: str, difficulty: str) -> str:
        """
        ç”Ÿæˆå®ç°å»ºè®®

        Args:
            future_work: Future Workæè¿°
            difficulty: éš¾åº¦ç­‰çº§

        Returns:
            å®ç°å»ºè®®æ–‡æœ¬
        """
        difficulty_desc = {
            'easy': 'éš¾åº¦è¾ƒä½ï¼Œå¯å¿«é€ŸéªŒè¯',
            'medium': 'éš¾åº¦ä¸­ç­‰ï¼Œéœ€è¦ä¸€å®šå·¥ç¨‹å®ç°',
            'hard': 'éš¾åº¦è¾ƒé«˜ï¼Œéœ€è¦æ·±å…¥ç ”ç©¶å’Œåˆ›æ–°'
        }

        return f"{difficulty_desc.get(difficulty, 'éš¾åº¦æœªçŸ¥')}ã€‚å»ºè®®ä¼˜å…ˆé˜…è¯»åŸè®ºæ–‡ï¼Œç†è§£å…¶æ ¸å¿ƒæ–¹æ³•åè¿›è¡Œæ‰©å±•ã€‚"

    def _detect_hard_nuts(self, graph: nx.DiGraph, milestone_papers: List[Dict]) -> List[Dict]:
        """
        ç­‰çº§äºŒï¼šæ”»åšå‹Idea - å¯»æ‰¾æœªè¢«Overcomesçš„Limitationï¼ˆå¢å¼ºç‰ˆï¼‰

        ç®—æ³•é€»è¾‘ï¼š
        1. æ‰¾åˆ°è¯¥é¢†åŸŸå¼•ç”¨é‡æœ€é«˜ã€ä½†å‘å¸ƒæ—¶é—´è¾ƒè¿‘çš„å‡ ç¯‡"åŸºçŸ³è®ºæ–‡"
        2. æå–å®ƒä»¬çš„ Limitation
        3. æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ–°è®ºæ–‡é€šè¿‡ Overcomes è¿æ¥å®ƒ
        4. å¦‚æœè¿˜æ²¡æœ‰ï¼Œè¯´æ˜å¤§å®¶è™½ç„¶éƒ½åœ¨å¼•ç”¨å®ƒï¼Œä½†å®ƒçš„æ ¸å¿ƒç¼ºé™·ä¾ç„¶å­˜åœ¨

        å¢å¼ºï¼š
        - è¯„ä¼°Limitationçš„ä¸¥é‡æ€§å’Œå½±å“
        - åˆ†æä¸ºä½•è‡³ä»Šæœªè¢«è§£å†³ï¼ˆæŠ€æœ¯éš¾åº¦ã€èµ„æºè¦æ±‚ç­‰ï¼‰
        - æä¾›æ”»åšå»ºè®®

        ä½ çš„Ideaï¼šä¸“é—¨é’ˆå¯¹è¿™ä¸ªæœªè§£çš„Limitationæå‡ºæ–°çš„Method

        Returns:
            æ”»åšå‹Ideaåˆ—è¡¨
        """
        hard_nuts = []

        for milestone in milestone_papers[:15]:  # æ‰©å±•åˆ°å‰15ç¯‡
            node_id = milestone['id']
            if node_id not in graph:
                continue

            node_data = graph.nodes[node_id]

            # ä»deep_analysisç»“æ„ä¸­è·å–limitation
            deep_analysis = node_data.get('deep_analysis', {})
            limitation = deep_analysis.get('limitation', {}).get('content', '')

            if not limitation or len(limitation) < 20:
                continue

            # æ£€æŸ¥æ˜¯å¦æœ‰åç»­å·¥ä½œé€šè¿‡Overcomesè§£å†³
            has_overcome = False
            overcome_attempts = []  # è®°å½•å°è¯•è§£å†³çš„å·¥ä½œ

            for pred in graph.predecessors(node_id):
                edge_data = graph[pred][node_id]
                edge_type = edge_data.get('edge_type', 'Unknown')

                if edge_type == 'Overcomes':
                    has_overcome = True
                    break
                elif edge_type in ['Extends', 'Realizes']:
                    # è™½ç„¶ä¸æ˜¯Overcomesï¼Œä½†æœ‰äººåœ¨å°è¯•æ”¹è¿›
                    overcome_attempts.append({
                        'id': pred,
                        'title': graph.nodes[pred].get('title', '')[:60],
                        'type': edge_type
                    })

            if not has_overcome:
                # è¯„ä¼°Limitationçš„ä¸¥é‡æ€§
                severity = self._assess_limitation_severity(limitation)

                # åˆ†æä¸ºä½•æœªè¢«è§£å†³
                unsolved_reason = self._analyze_unsolved_reason(
                    limitation,
                    len(overcome_attempts),
                    node_data
                )

                # è¯„ä¼°æ”»åšéš¾åº¦
                attack_difficulty = self._assess_attack_difficulty(
                    limitation,
                    node_data.get('cited_by_count', 0),
                    len(overcome_attempts)
                )

                hard_nuts.append({
                    'paper': {
                        'id': node_id,
                        'title': node_data.get('title', '')[:80],
                        'year': node_data.get('year'),
                        'cited_by_count': node_data.get('cited_by_count', 0)
                    },
                    'limitation': limitation[:300],
                    'severity': severity,
                    'unsolved_reason': unsolved_reason,
                    'attack_difficulty': attack_difficulty,
                    'overcome_attempts': overcome_attempts[:3],  # æ˜¾ç¤ºå‰3ä¸ªå°è¯•
                    'impact_potential': node_data.get('cited_by_count', 0) * severity,  # è§£å†³åçš„æ½œåœ¨å½±å“
                    'research_direction': self._suggest_research_direction(limitation, attack_difficulty)
                })

        # æŒ‰æ½œåœ¨å½±å“æ’åº
        hard_nuts = sorted(hard_nuts, key=lambda x: x['impact_potential'], reverse=True)[:12]

        return hard_nuts

    def _assess_limitation_severity(self, limitation: str) -> float:
        """
        è¯„ä¼°Limitationçš„ä¸¥é‡æ€§

        Args:
            limitation: Limitationæè¿°

        Returns:
            ä¸¥é‡æ€§åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        score = 0.5  # åŸºç¡€åˆ†

        # ä¸¥é‡æ€§å…³é”®è¯
        critical_keywords = ['critical', 'major', 'significant', 'fundamental', 'serious']
        moderate_keywords = ['important', 'notable', 'considerable']
        minor_keywords = ['minor', 'small', 'slight']

        limitation_lower = limitation.lower()

        # æ£€æŸ¥ä¸¥é‡æ€§
        if any(kw in limitation_lower for kw in critical_keywords):
            score += 0.4
        elif any(kw in limitation_lower for kw in moderate_keywords):
            score += 0.2
        elif any(kw in limitation_lower for kw in minor_keywords):
            score -= 0.2

        # æ£€æŸ¥å½±å“èŒƒå›´
        scope_keywords = ['all', 'general', 'common', 'widespread', 'universal']
        if any(kw in limitation_lower for kw in scope_keywords):
            score += 0.2

        return min(max(score, 0.1), 1.0)

    def _analyze_unsolved_reason(self, limitation: str, attempt_count: int, paper_data: Dict) -> str:
        """
        åˆ†æLimitationä¸ºä½•è‡³ä»Šæœªè¢«è§£å†³

        Args:
            limitation: Limitationæè¿°
            attempt_count: å°è¯•è§£å†³çš„å·¥ä½œæ•°é‡
            paper_data: è®ºæ–‡æ•°æ®

        Returns:
            åˆ†æç»“è®º
        """
        limitation_lower = limitation.lower()

        # æŠ€æœ¯éš¾åº¦é«˜
        if any(kw in limitation_lower for kw in ['complex', 'difficult', 'challenging', 'non-trivial']):
            if attempt_count > 0:
                return f"æŠ€æœ¯éš¾åº¦æé«˜ï¼Œå·²æœ‰{attempt_count}ä¸ªå·¥ä½œå°è¯•æ”¹è¿›ä½†æœªèƒ½ä»æ ¹æœ¬ä¸Šè§£å†³"
            else:
                return "æŠ€æœ¯éš¾åº¦æé«˜ï¼Œè‡³ä»Šæ— äººæ•¢äºæŒ‘æˆ˜"

        # èµ„æºè¦æ±‚é«˜
        if any(kw in limitation_lower for kw in ['expensive', 'costly', 'large-scale', 'computation']):
            return "éœ€è¦å¤§é‡è®¡ç®—èµ„æºæˆ–æ•°æ®ï¼Œå¯¹ç ”ç©¶è€…è¦æ±‚è¾ƒé«˜"

        # ç†è®ºåŸºç¡€é—®é¢˜
        if any(kw in limitation_lower for kw in ['theoretical', 'fundamental', 'framework']):
            return "æ¶‰åŠç†è®ºåŸºç¡€é—®é¢˜ï¼Œéœ€è¦æ–¹æ³•è®ºçªç ´"

        # å…¶ä»–
        if attempt_count > 0:
            return f"å·²æœ‰{attempt_count}ä¸ªå·¥ä½œå°è¯•æ”¹è¿›ï¼Œä½†æ ¸å¿ƒé—®é¢˜ä»æœªè§£å†³"
        else:
            return "å°šæœªå¼•èµ·è¶³å¤Ÿé‡è§†ï¼Œæˆ–éœ€è¦è·¨å­¦ç§‘çŸ¥è¯†"

    def _assess_attack_difficulty(self, limitation: str, citations: int, attempts: int) -> str:
        """
        è¯„ä¼°æ”»åšéš¾åº¦

        Args:
            limitation: Limitationæè¿°
            citations: è®ºæ–‡å¼•ç”¨æ•°
            attempts: å°è¯•è§£å†³çš„å·¥ä½œæ•°é‡

        Returns:
            éš¾åº¦ç­‰çº§ï¼š'very_hard', 'hard', 'medium'
        """
        # é«˜å¼•ç”¨ä½†æ— äººè§£å†³ = æéš¾
        if citations > 1000 and attempts == 0:
            return 'very_hard'

        # æœ‰äººå°è¯•ä½†å¤±è´¥ = éš¾
        if attempts > 0:
            return 'hard'

        # å…¶ä»–
        if citations > 500:
            return 'hard'
        else:
            return 'medium'

    def _suggest_research_direction(self, limitation: str, difficulty: str) -> str:
        """
        å»ºè®®ç ”ç©¶æ–¹å‘

        Args:
            limitation: Limitationæè¿°
            difficulty: éš¾åº¦ç­‰çº§

        Returns:
            ç ”ç©¶æ–¹å‘å»ºè®®
        """
        limitation_lower = limitation.lower()

        # åŸºäºLimitationç±»å‹ç»™å‡ºå»ºè®®
        if 'scalability' in limitation_lower or 'scale' in limitation_lower:
            direction = "è€ƒè™‘åˆ†å¸ƒå¼æ–¹æ³•ã€è¿‘ä¼¼ç®—æ³•æˆ–æ¨¡å‹å‹ç¼©æŠ€æœ¯"
        elif 'generalization' in limitation_lower or 'generalize' in limitation_lower:
            direction = "æ¢ç´¢å…ƒå­¦ä¹ ã€åŸŸé€‚åº”æˆ–è¿ç§»å­¦ä¹ æ–¹æ³•"
        elif 'efficiency' in limitation_lower or 'computation' in limitation_lower:
            direction = "ç ”ç©¶åŠ é€Ÿç®—æ³•ã€æ¨¡å‹è’¸é¦æˆ–ç¡¬ä»¶ä¼˜åŒ–"
        elif 'interpretability' in limitation_lower or 'explainability' in limitation_lower:
            direction = "å¼•å…¥å¯è§£é‡Šæ€§æ–¹æ³•ã€æ³¨æ„åŠ›æœºåˆ¶æˆ–å› æœæ¨ç†"
        elif 'data' in limitation_lower and 'require' in limitation_lower:
            direction = "æ¢ç´¢å°‘æ ·æœ¬å­¦ä¹ ã€æ•°æ®å¢å¼ºæˆ–æ— ç›‘ç£æ–¹æ³•"
        else:
            direction = "å»ºè®®ä»æ–¹æ³•è®ºåˆ›æ–°æˆ–è·¨é¢†åŸŸè¿ç§»è§’åº¦å…¥æ‰‹"

        difficulty_prefix = {
            'very_hard': "âš ï¸ æé«˜éš¾åº¦é¡¹ç›®ï¼Œ",
            'hard': "ğŸ”¥ é«˜éš¾åº¦é¡¹ç›®ï¼Œ",
            'medium': "ğŸ’ª ä¸­ç­‰éš¾åº¦é¡¹ç›®ï¼Œ"
        }

        return f"{difficulty_prefix.get(difficulty, '')} {direction}"

    def _generate_innovative_ideas(self, graph: nx.DiGraph) -> Dict:
        """
        ç­‰çº§ä¸‰ï¼šåˆ›æ–°å‹Idea - è·¨åŸŸè¿ç§»å’Œç»„åˆæ‹³ï¼ˆå®Œå…¨é‡æ„ï¼‰

        æ ¸å¿ƒæ€æƒ³ï¼š
        åˆ©ç”¨ Adapts_to æˆ– Alternative çš„ä¼ é€’æ€§è¿›è¡Œæ¨ç†

        æ¨¡å¼ Aï¼ˆå€Ÿå°¸è¿˜é­‚ï¼‰ï¼š
        1. æ‰¾åˆ°ä¸€ä¸ªåœ¨åˆ†æ”¯ A ä¸­éå¸¸æˆåŠŸï¼ˆè¢«å¤§é‡ Extendsï¼‰çš„ Method_X
        2. æ‰¾åˆ°åˆ†æ”¯ B ä¸­ç›®å‰é¢ä¸´çš„ä¸€ä¸ª Problem_Yï¼ˆæœ‰å¾ˆå¤š Limitation æ²¡è¢«è§£å†³ï¼‰
        3. é¢„æµ‹ï¼šå°è¯•è®¡ç®— TextSimilarity(Method_X, Problem_Y's context)
        4. å¦‚æœé€»è¾‘ä¸Šå¯è¡Œï¼Œå°† Method_X è¿ç§»è¿‡æ¥è§£å†³ Problem_Y å°±æ˜¯ä¸€ä¸ªå…¸å‹çš„ Adapts_to åˆ›æ–°

        æ¨¡å¼ Bï¼ˆç»„åˆæ‹³ï¼‰ï¼š
        1. å¦‚æœè®ºæ–‡ A å’Œè®ºæ–‡ B æ˜¯ Alternative å…³ç³»ï¼ˆè§£å†³åŒä¸€é—®é¢˜ï¼Œæ–¹æ³•ä¸åŒï¼‰
        2. æ£€æŸ¥ A çš„ Limitation æ˜¯å¦æ­£å¥½æ˜¯ B çš„ä¼˜åŠ¿ï¼Œåä¹‹äº¦ç„¶
        3. é¢„æµ‹ï¼šæå‡ºä¸€ä¸ª Hybrid Methodï¼Œç»“åˆ A å’Œ B çš„ä¼˜ç‚¹ï¼Œè¿™é€šå¸¸èƒ½ç”Ÿæˆä¸€ç¯‡å¼ºæœ‰åŠ›çš„ Overcomes è®ºæ–‡

        Returns:
            åˆ›æ–°å‹Ideaå­—å…¸ï¼ŒåŒ…å«cross_domain_transferå’Œhybrid_methodsä¸¤ç§ç±»å‹
        """
        # æ¨¡å¼Aï¼šå€Ÿå°¸è¿˜é­‚ï¼ˆè·¨åŸŸè¿ç§»ï¼‰
        cross_domain_ideas = self._generate_cross_domain_transfer_ideas(graph)

        # æ¨¡å¼Bï¼šç»„åˆæ‹³ï¼ˆæ··åˆæ–¹æ³•ï¼‰
        hybrid_ideas = self._generate_hybrid_method_ideas(graph)

        return {
            'cross_domain_transfer': cross_domain_ideas,
            'hybrid_methods': hybrid_ideas,
            'summary': {
                'cross_domain_count': len(cross_domain_ideas),
                'hybrid_count': len(hybrid_ideas),
                'total_ideas': len(cross_domain_ideas) + len(hybrid_ideas)
            }
        }

    def _generate_cross_domain_transfer_ideas(self, graph: nx.DiGraph) -> List[Dict]:
        """
        æ¨¡å¼Aï¼šå€Ÿå°¸è¿˜é­‚ - ç”Ÿæˆè·¨åŸŸè¿ç§»Idea

        ç®—æ³•æ­¥éª¤ï¼š
        1. æ‰¾åˆ°è¢«å¤§é‡Extendså¼•ç”¨çš„æˆåŠŸMethodï¼ˆè¯æ˜æ–¹æ³•æœ‰æ•ˆï¼‰
        2. æ‰¾åˆ°æœ‰æœªè§£å†³Limitationçš„Problem
        3. è®¡ç®—Methodä¸Problemçš„è¯­ä¹‰åŒ¹é…åº¦
        4. æ¨èé«˜åŒ¹é…åº¦çš„è·¨åŸŸè¿ç§»æ–¹æ¡ˆ

        Returns:
            è·¨åŸŸè¿ç§»Ideaåˆ—è¡¨
        """
        ideas = []

        # Step 1: è¯†åˆ«æˆåŠŸçš„Methodï¼ˆè¢«å¤§é‡Extendsçš„èŠ‚ç‚¹ï¼‰
        successful_methods = []

        for node in graph.nodes():
            # ç»Ÿè®¡è¢«Extendså¼•ç”¨çš„æ¬¡æ•°
            in_extends = sum(
                1 for pred in graph.predecessors(node)
                if graph[pred][node].get('edge_type') == 'Extends'
            )

            if in_extends >= 3:  # è¢«è‡³å°‘3ä¸ªExtendså¼•ç”¨
                node_data = graph.nodes[node]

                # ä»deep_analysisç»“æ„ä¸­è·å–method
                deep_analysis = node_data.get('deep_analysis', {})
                method = deep_analysis.get('method', {}).get('content', '')
                if not method:
                    method = node_data.get('title', '')

                successful_methods.append({
                    'id': node,
                    'title': node_data.get('title', ''),
                    'year': node_data.get('year'),
                    'extends_count': in_extends,
                    'method': method,
                    'domain': self._extract_domain_keywords(node_data.get('title', ''))
                })

        # æŒ‰Extendsæ•°é‡æ’åº
        successful_methods = sorted(successful_methods, key=lambda x: x['extends_count'], reverse=True)[:10]

        # Step 2: è¯†åˆ«æœ‰æœªè§£å†³Limitationçš„Problem
        unsolved_problems = []

        for node in graph.nodes():
            node_data = graph.nodes[node]

            # ä»deep_analysisç»“æ„ä¸­è·å–limitation
            deep_analysis = node_data.get('deep_analysis', {})
            limitation = deep_analysis.get('limitation', {}).get('content', '')

            if not limitation or len(limitation) < 20:
                continue

            # æ£€æŸ¥æ˜¯å¦å·²è¢«Overcomesè§£å†³
            has_overcome = any(
                graph[pred][node].get('edge_type') == 'Overcomes'
                for pred in graph.predecessors(node)
            )

            if not has_overcome:
                # ä»deep_analysisè·å–problemå­—æ®µ
                deep_analysis = node_data.get('deep_analysis', {})
                problem = deep_analysis.get('problem', {}).get('content', '')
                if not problem:
                    problem = limitation  # å¦‚æœæ²¡æœ‰problemï¼Œä½¿ç”¨limitation

                unsolved_problems.append({
                    'id': node,
                    'title': node_data.get('title', ''),
                    'year': node_data.get('year'),
                    'limitation': limitation,
                    'problem': problem,
                    'domain': self._extract_domain_keywords(node_data.get('title', ''))
                })

        # Step 3: åŒ¹é…Methodä¸Problem
        for method in successful_methods[:8]:  # å‰8ä¸ªæˆåŠŸæ–¹æ³•
            for problem in unsolved_problems[:15]:  # å‰15ä¸ªæœªè§£å†³é—®é¢˜
                # é¿å…è‡ªå·±åŒ¹é…è‡ªå·±
                if method['id'] == problem['id']:
                    continue

                # é¿å…å·²æœ‰å¼•ç”¨å…³ç³»
                if graph.has_edge(method['id'], problem['id']) or graph.has_edge(problem['id'], method['id']):
                    continue

                # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
                similarity = self._calculate_text_similarity(
                    method['method'],
                    problem['problem']
                )

                # è¿‡æ»¤ä½ç›¸ä¼¼åº¦
                if similarity < 0.2:
                    continue

                # æ£€æŸ¥æ˜¯å¦è·¨åŸŸï¼ˆå¢åŠ åˆ›æ–°æ€§ï¼‰
                is_cross_domain = len(set(method['domain']) & set(problem['domain'])) == 0

                # è¯„ä¼°è¿ç§»å¯è¡Œæ€§
                feasibility = self._assess_transfer_feasibility(
                    method,
                    problem,
                    similarity,
                    is_cross_domain
                )

                ideas.append({
                    'type': 'cross_domain_transfer',
                    'method_paper': {
                        'id': method['id'],
                        'title': method['title'][:80],
                        'year': method['year'],
                        'extends_count': method['extends_count'],
                        'domain': method['domain'][:2]
                    },
                    'target_paper': {
                        'id': problem['id'],
                        'title': problem['title'][:80],
                        'year': problem['year'],
                        'domain': problem['domain'][:2]
                    },
                    'method_description': method['method'][:200],
                    'target_limitation': problem['limitation'][:200],
                    'similarity_score': similarity,
                    'is_cross_domain': is_cross_domain,
                    'feasibility': feasibility,
                    'innovation_story': self._generate_transfer_story(method, problem, similarity, is_cross_domain)
                })

        # æŒ‰å¯è¡Œæ€§å’Œç›¸ä¼¼åº¦æ’åº
        ideas = sorted(ideas, key=lambda x: x['feasibility'] * x['similarity_score'], reverse=True)[:8]

        return ideas

    def _generate_hybrid_method_ideas(self, graph: nx.DiGraph) -> List[Dict]:
        """
        æ¨¡å¼Bï¼šç»„åˆæ‹³ - ç”Ÿæˆæ··åˆæ–¹æ³•Idea

        ç®—æ³•æ­¥éª¤ï¼š
        1. æ‰¾åˆ°Alternativeå…³ç³»å¯¹ï¼ˆè§£å†³åŒä¸€é—®é¢˜ï¼Œæ–¹æ³•ä¸åŒï¼‰
        2. åˆ†æAçš„Limitationæ˜¯å¦æ˜¯Bçš„ä¼˜åŠ¿
        3. æ¨èç»“åˆä¸¤è€…ä¼˜ç‚¹çš„Hybrid Method

        Returns:
            æ··åˆæ–¹æ³•Ideaåˆ—è¡¨
        """
        ideas = []

        # æ”¶é›†æ‰€æœ‰Alternativeå…³ç³»
        alternative_pairs = []

        for source, target, data in graph.edges(data=True):
            if data.get('edge_type') == 'Alternative':
                source_data = graph.nodes[source]
                target_data = graph.nodes[target]

                # ä»deep_analysisè·å–å„å­—æ®µ
                source_deep = source_data.get('deep_analysis', {})
                target_deep = target_data.get('deep_analysis', {})

                alternative_pairs.append({
                    'paper_a': {
                        'id': source,
                        'title': source_data.get('title', ''),
                        'year': source_data.get('year'),
                        'method': source_deep.get('method', {}).get('content', '') or source_data.get('title', ''),
                        'limitation': source_deep.get('limitation', {}).get('content', '')
                    },
                    'paper_b': {
                        'id': target,
                        'title': target_data.get('title', ''),
                        'year': target_data.get('year'),
                        'method': target_deep.get('method', {}).get('content', '') or target_data.get('title', ''),
                        'limitation': target_deep.get('limitation', {}).get('content', '')
                    }
                })

        # åˆ†ææ¯å¯¹Alternativeå…³ç³»
        for pair in alternative_pairs[:10]:  # åˆ†æå‰10å¯¹
            paper_a = pair['paper_a']
            paper_b = pair['paper_b']

            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ä¿¡æ¯
            if not paper_a['limitation'] or not paper_b['method']:
                continue

            # æ£€æŸ¥Açš„Limitationä¸Bçš„Methodçš„äº’è¡¥æ€§
            complementarity_ab = self._calculate_text_similarity(
                paper_a['limitation'],
                paper_b['method']
            )

            complementarity_ba = self._calculate_text_similarity(
                paper_b.get('limitation', ''),
                paper_a.get('method', '')
            ) if paper_b.get('limitation') and paper_a.get('method') else 0

            # è‡³å°‘ä¸€æ–¹é¢å…·æœ‰äº’è¡¥æ€§
            if complementarity_ab < 0.3 and complementarity_ba < 0.3:
                continue

            # è¯„ä¼°æ··åˆæ–¹æ³•çš„å¯è¡Œæ€§
            hybrid_feasibility = self._assess_hybrid_feasibility(
                paper_a,
                paper_b,
                complementarity_ab,
                complementarity_ba
            )

            ideas.append({
                'type': 'hybrid_method',
                'paper_a': {
                    'id': paper_a['id'],
                    'title': paper_a['title'][:80],
                    'year': paper_a['year'],
                    'strength': paper_a.get('method', '')[:150],
                    'weakness': paper_a.get('limitation', '')[:150]
                },
                'paper_b': {
                    'id': paper_b['id'],
                    'title': paper_b['title'][:80],
                    'year': paper_b['year'],
                    'strength': paper_b.get('method', '')[:150],
                    'weakness': paper_b.get('limitation', '')[:150]
                },
                'complementarity_scores': {
                    'a_weakness_vs_b_strength': complementarity_ab,
                    'b_weakness_vs_a_strength': complementarity_ba,
                    'overall': max(complementarity_ab, complementarity_ba)
                },
                'hybrid_feasibility': hybrid_feasibility,
                'hybrid_strategy': self._suggest_hybrid_strategy(paper_a, paper_b, complementarity_ab, complementarity_ba)
            })

        # æŒ‰å¯è¡Œæ€§æ’åº
        ideas = sorted(ideas, key=lambda x: x['hybrid_feasibility'], reverse=True)[:6]

        return ideas

    def _assess_transfer_feasibility(
        self,
        method: Dict,
        problem: Dict,
        similarity: float,
        is_cross_domain: bool
    ) -> float:
        """
        è¯„ä¼°è·¨åŸŸè¿ç§»çš„å¯è¡Œæ€§

        Args:
            method: æ–¹æ³•ä¿¡æ¯
            problem: é—®é¢˜ä¿¡æ¯
            similarity: è¯­ä¹‰ç›¸ä¼¼åº¦
            is_cross_domain: æ˜¯å¦è·¨åŸŸ

        Returns:
            å¯è¡Œæ€§åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        feasibility = similarity  # åŸºç¡€åˆ†

        # æˆåŠŸæ–¹æ³•çš„Extendsæ•°é‡è¶Šå¤šï¼Œè¯´æ˜æ–¹æ³•è¶Šæˆç†Ÿ
        if method['extends_count'] >= 5:
            feasibility += 0.2
        elif method['extends_count'] >= 3:
            feasibility += 0.1

        # è·¨åŸŸè¿ç§»æ›´æœ‰åˆ›æ–°æ€§ï¼Œä½†å¯è¡Œæ€§ç•¥é™
        if is_cross_domain:
            feasibility += 0.1  # åˆ›æ–°åŠ åˆ†
            feasibility *= 0.9  # é£é™©æŠ˜æ‰£

        # æ—¶é—´å·®ä¸èƒ½å¤ªå¤§ï¼ˆæ–¹æ³•ä¸èƒ½å¤ªé™ˆæ—§ï¼‰
        year_gap = problem.get('year', 2024) - method.get('year', 2024)
        if year_gap < 0 or year_gap > 10:
            feasibility *= 0.8

        return min(feasibility, 1.0)

    def _assess_hybrid_feasibility(
        self,
        paper_a: Dict,
        paper_b: Dict,
        comp_ab: float,
        comp_ba: float
    ) -> float:
        """
        è¯„ä¼°æ··åˆæ–¹æ³•çš„å¯è¡Œæ€§

        Args:
            paper_a: è®ºæ–‡Aä¿¡æ¯
            paper_b: è®ºæ–‡Bä¿¡æ¯
            comp_ab: Açš„weaknessä¸Bçš„strengthçš„äº’è¡¥æ€§
            comp_ba: Bçš„weaknessä¸Açš„strengthçš„äº’è¡¥æ€§

        Returns:
            å¯è¡Œæ€§åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        # åŸºç¡€åˆ†ï¼šå–ä¸¤ä¸ªäº’è¡¥æ€§çš„æœ€å¤§å€¼
        feasibility = max(comp_ab, comp_ba)

        # å¦‚æœåŒå‘äº’è¡¥ï¼ŒåŠ åˆ†
        if comp_ab > 0.3 and comp_ba > 0.3:
            feasibility += 0.2

        # æ—¶é—´æ¥è¿‘æ€§ï¼ˆåŒæ—¶æœŸçš„æ–¹æ³•æ›´å®¹æ˜“ç»“åˆï¼‰
        year_gap = abs(paper_a.get('year', 2024) - paper_b.get('year', 2024))
        if year_gap <= 2:
            feasibility += 0.15
        elif year_gap <= 5:
            feasibility += 0.05

        return min(feasibility, 1.0)

    def _generate_transfer_story(
        self,
        method: Dict,
        problem: Dict,
        similarity: float,
        is_cross_domain: bool
    ) -> str:
        """
        ç”Ÿæˆè·¨åŸŸè¿ç§»çš„åˆ›æ–°æ•…äº‹

        Returns:
            æè¿°æ–‡æœ¬
        """
        method_domain = ', '.join(method['domain'][:2]) if method['domain'] else 'æŸé¢†åŸŸ'
        problem_domain = ', '.join(problem['domain'][:2]) if problem['domain'] else 'è¯¥é¢†åŸŸ'

        domain_desc = f"ä»[{method_domain}]è¿ç§»åˆ°[{problem_domain}]" if is_cross_domain else f"åœ¨[{problem_domain}]å†…éƒ¨åº”ç”¨"

        return (
            f"ğŸ’¡ åˆ›æ–°ç‚¹ï¼š{domain_desc}ã€‚"
            f"æ–¹æ³•æ¥æºäº{method['year']}å¹´çš„æˆåŠŸç»éªŒï¼ˆè¢«{method['extends_count']}ä¸ªå·¥ä½œExtendsï¼‰ï¼Œ"
            f"å¯ç”¨äºè§£å†³{problem['year']}å¹´è®ºæ–‡ä¸­çš„æœªè§£å†³é—®é¢˜ã€‚"
            f"åŒ¹é…åº¦ï¼š{similarity:.0%}"
        )

    def _suggest_hybrid_strategy(
        self,
        paper_a: Dict,
        paper_b: Dict,
        comp_ab: float,
        comp_ba: float
    ) -> str:
        """
        å»ºè®®æ··åˆæ–¹æ³•çš„ç­–ç•¥

        Returns:
            ç­–ç•¥æè¿°
        """
        if comp_ab > comp_ba:
            dominant = 'B'
            complementary = 'A'
            desc = f"ä»¥æ–¹æ³•Bä¸ºä¸»ï¼Œå¼•å…¥æ–¹æ³•Aæ¥å¼¥è¡¥Bçš„ä¸è¶³ï¼ˆåŒ¹é…åº¦{comp_ab:.0%}ï¼‰"
        else:
            dominant = 'A'
            complementary = 'B'
            desc = f"ä»¥æ–¹æ³•Aä¸ºä¸»ï¼Œå¼•å…¥æ–¹æ³•Bæ¥å¼¥è¡¥Açš„ä¸è¶³ï¼ˆåŒ¹é…åº¦{comp_ba:.0%}ï¼‰"

        return f"ğŸ”§ æ··åˆç­–ç•¥ï¼š{desc}ã€‚å»ºè®®åœ¨{dominant}çš„æ¡†æ¶ä¸‹ï¼Œé€‰æ‹©æ€§åœ°é›†æˆ{complementary}çš„ä¼˜åŠ¿ç»„ä»¶ã€‚"

    def _log_summary(self, report: Dict) -> None:
        """
        è¾“å‡ºåˆ†ææ¦‚è¦æ—¥å¿—ï¼ˆå¢å¼ºç‰ˆ - åŒæ ¸å¿ƒæ–¹å‘ï¼‰

        Args:
            report: åˆ†ææŠ¥å‘Š
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"âœ… ä¸»é¢˜å‘å±•è„‰ç»œåˆ†æå®Œæˆï¼ˆåŒæ ¸å¿ƒæ–¹å‘ï¼‰")
        logger.info(f"{'='*80}")

        logger.info(f"\nğŸ“Š å›¾è°±æ¦‚è§ˆ:")
        overview = report.get('graph_overview', {})
        logger.info(f"  â€¢ è®ºæ–‡æ€»æ•°: {overview.get('total_papers', 0)}")
        logger.info(f"  â€¢ å¼•ç”¨å…³ç³»: {overview.get('total_citations', 0)}")
        logger.info(f"  â€¢ æ—¶é—´è·¨åº¦: {overview.get('year_range', 'Unknown')}")

        # ======================== æ ¸å¿ƒæ–¹å‘1: å›æº¯è„‰ç»œ ========================
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ”™ ã€æ ¸å¿ƒæ–¹å‘1ã€‘å›æº¯è„‰ç»œåˆ†æ (Retrospective Analysis)")
        logger.info(f"{'='*80}")

        retro = report.get('retrospective_analysis', {})

        # 1.1 è¿›åŒ–ä¸»å¹² vs æ—æ”¯ä¿®è¡¥
        logger.info(f"\nğŸ“ 1.1 è¿›åŒ–ä¸»å¹² vs æ—æ”¯ä¿®è¡¥:")
        backbone = retro.get('backbone_vs_incremental', {})
        summary = backbone.get('summary', {})
        logger.info(f"  â€¢ ä¸»å¹²è·¯å¾„ï¼ˆOvercomes/Realizesï¼‰: {summary.get('backbone_count', 0)} æ¡")
        logger.info(f"  â€¢ æ¸è¿›è·¯å¾„ï¼ˆExtendsï¼‰: {summary.get('incremental_count', 0)} æ¡")
        logger.info(f"  â€¢ ä¸»å¹²/æ¸è¿›æ¯”ä¾‹: {summary.get('ratio', 0):.2f}")
        logger.info(f"  â€¢ ç ´å±€ç‚¹æ•°é‡: {summary.get('breakthrough_count', 0)} ä¸ª\n")

        # å±•ç¤ºç ´å±€ç‚¹
        breakthrough = backbone.get('breakthrough_points', [])
        if breakthrough:
            logger.info(f"  ğŸ¯ Top ç ´å±€ç‚¹ï¼ˆä»Extendså†…å·è·³å‡ºOvercomesï¼‰:")
            for i, bp in enumerate(breakthrough[:5], 1):
                logger.info(f"    [{i}] {bp['title']}")
                logger.info(f"        å¹´ä»½: {bp['year']}, å¼•ç”¨æ•°: {bp['cited_by_count']}")
                logger.info(f"        çªç ´æ¨¡å¼: {bp['breakthrough_type']}")
                logger.info(f"        è¾“å…¥: {bp['in_extends']}ä¸ªExtends â†’ è¾“å‡º: {bp['out_overcomes']}ä¸ªOvercomes, {bp['out_realizes']}ä¸ªRealizes")
                logger.info(f"        çªç ´åˆ†æ•°: {bp['breakthrough_score']:.1f}\n")

        # å±•ç¤ºä¸»å¹²è¿ç»­é“¾
        backbone_chains = backbone.get('backbone_chains', [])
        if backbone_chains:
            logger.info(f"  â›“ï¸  ä¸»å¹²è¿ç»­é“¾ï¼ˆç¡¬æ ¸æ”»åšæ¼”åŒ–çº¿ï¼‰:")
            for i, chain in enumerate(backbone_chains[:3], 1):
                logger.info(f"    [é“¾{i}] é•¿åº¦: {chain['length']}ç¯‡, æ—¶é—´è·¨åº¦: {chain['year_span']}å¹´")
                for j, paper in enumerate(chain['chain'][:4], 1):  # åªæ˜¾ç¤ºå‰4ç¯‡
                    logger.info(f"      {j}. {paper['title']} ({paper['year']})")
                    if j < len(chain['edge_types']):
                        logger.info(f"         â””â”€> [{chain['edge_types'][j-1]}]")
                logger.info("")

        # å±•ç¤ºæ¸è¿›ç“¶é¢ˆ
        bottlenecks = backbone.get('incremental_bottlenecks', [])
        if bottlenecks:
            logger.info(f"  âš ï¸  æ¸è¿›è·¯å¾„ç“¶é¢ˆï¼ˆå†…å·ç»ˆç‚¹ï¼‰:")
            for i, bn in enumerate(bottlenecks[:3], 1):
                logger.info(f"    [{i}] {bn['title']} ({bn['year']})")
                logger.info(f"        {bn['reason']}\n")

        # 1.2 æŠ€æœ¯åˆ†å‰å£
        logger.info(f"\nğŸ”€ 1.2 æŠ€æœ¯åˆ†å‰å£ï¼ˆTechnical Bifurcationï¼‰:")
        bifur = retro.get('technical_bifurcations', [])
        if bifur:
            logger.info(f"  å‘ç° {len(bifur)} ä¸ªæŠ€æœ¯è·¯çº¿ä¹‹äº‰\n")
            for i, b in enumerate(bifur[:3], 1):
                logger.info(f"  [åˆ†å‰ç‚¹{i}] {b['parent']['title']} ({b['parent']['year']})")
                logger.info(f"    åˆ†æ­§è¯„åˆ†: {b['divergence_score']:.2f}")
                logger.info(f"    ç«äº‰åˆ†æ”¯:")
                for j, branch in enumerate(b['branches'], 1):
                    logger.info(f"      {j}. [{branch['edge_type']}] {branch['title']} ({branch['year']})")
                    logger.info(f"         åç»­å‘å±•: {branch['subtree_size']}ä¸ªå·¥ä½œ, æ·±åº¦{branch['subtree_depth']}, çŠ¶æ€: {branch['subtree_status']}")
                logger.info(f"    åˆ†æ: {b['branch_comparison']}\n")
        else:
            logger.info(f"  æœªå‘ç°æ˜æ˜¾çš„æŠ€æœ¯åˆ†å‰å£\n")

        # 1.3 è·¨ç•Œå…¥ä¾µ
        logger.info(f"\nğŸŒ 1.3 è·¨ç•Œå…¥ä¾µï¼ˆCross-Domain Invasionï¼‰:")
        invasions = retro.get('cross_domain_invasions', [])
        if invasions:
            logger.info(f"  å‘ç° {len(invasions)} ä¸ªè·¨åŸŸè¿ç§»æ¡ˆä¾‹ï¼ˆAdapts_toï¼‰\n")
            for i, inv in enumerate(invasions[:5], 1):
                impact = inv['impact_analysis']
                logger.info(f"  [å…¥ä¾µ{i}] {inv['cross_domain_story']}")
                logger.info(f"    æº: {inv['to']['title'][:60]}... ({inv['to']['year']})")
                logger.info(f"    ç›®æ ‡: {inv['from']['title'][:60]}... ({inv['from']['year']})")
                logger.info(f"    å½±å“: {impact['descendants_count']}ä¸ªåç»­å·¥ä½œ, æˆåŠŸåº¦: {impact['success_level']}\n")
        else:
            logger.info(f"  æœªå‘ç°è·¨åŸŸè¿ç§»æ¡ˆä¾‹\n")

        # ======================== æ ¸å¿ƒæ–¹å‘2: é¢„æµ‹æœªæ¥ ========================
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ”® ã€æ ¸å¿ƒæ–¹å‘2ã€‘é¢„æµ‹æœªæ¥ (Future Prediction)")
        logger.info(f"{'='*80}")

        future = report.get('future_prediction', {})

        # 2.1 ç­‰çº§ä¸€ï¼šæ¡æ¼å‹Idea
        logger.info(f"\nğŸ’¡ 2.1 ç­‰çº§ä¸€ï¼šæ¡æ¼å‹Ideaï¼ˆLow-Hanging Fruitsï¼‰")
        logger.info(f"  å¯»æ‰¾æœªè¢«Realizedçš„Future Work\n")
        level1 = future.get('level_1_low_hanging_fruits', [])
        if level1:
            logger.info(f"  å‘ç° {len(level1)} ä¸ªç°æˆçš„ç ”ç©¶æœºä¼š\n")
            for i, idea in enumerate(level1[:5], 1):
                paper = idea['paper']
                logger.info(f"  [Idea{i}] {paper['title']}")
                logger.info(f"    æºè®ºæ–‡: {paper['year']}å¹´, å¼•ç”¨æ•°: {paper['cited_by_count']}")
                logger.info(f"    éš¾åº¦: {idea['difficulty']}, å¯è¡Œæ€§: {idea['feasibility_score']:.2f}, ä¼˜å…ˆçº§: {idea['priority']:.1f}")
                logger.info(f"    Future Work: {idea['future_work'][:120]}...")
                logger.info(f"    å»ºè®®: {idea['recommendation']}\n")
        else:
            logger.info(f"  æœªå‘ç°æ˜æ˜¾çš„æ¡æ¼æœºä¼š\n")

        # 2.2 ç­‰çº§äºŒï¼šæ”»åšå‹Idea
        logger.info(f"\nğŸ”¨ 2.2 ç­‰çº§äºŒï¼šæ”»åšå‹Ideaï¼ˆHard Nutsï¼‰")
        logger.info(f"  å¯»æ‰¾æœªè¢«Overcomesçš„Limitation\n")
        level2 = future.get('level_2_hard_nuts', [])
        if level2:
            logger.info(f"  å‘ç° {len(level2)} ä¸ªé«˜ä»·å€¼æ”»åšæ–¹å‘\n")
            for i, idea in enumerate(level2[:5], 1):
                paper = idea['paper']
                logger.info(f"  [Idea{i}] {paper['title']}")
                logger.info(f"    æºè®ºæ–‡: {paper['year']}å¹´, å¼•ç”¨æ•°: {paper['cited_by_count']}")
                logger.info(f"    ä¸¥é‡æ€§: {idea['severity']:.2f}, æ”»åšéš¾åº¦: {idea['attack_difficulty']}, æ½œåœ¨å½±å“: {idea['impact_potential']:.1f}")
                logger.info(f"    Limitation: {idea['limitation'][:120]}...")
                logger.info(f"    æœªè§£å†³åŸå› : {idea['unsolved_reason']}")
                logger.info(f"    ç ”ç©¶æ–¹å‘: {idea['research_direction']}\n")
        else:
            logger.info(f"  æœªå‘ç°æ˜æ˜¾çš„æ”»åšæ–¹å‘\n")

        # 2.3 ç­‰çº§ä¸‰ï¼šåˆ›æ–°å‹Idea
        logger.info(f"\nğŸš€ 2.3 ç­‰çº§ä¸‰ï¼šåˆ›æ–°å‹Ideaï¼ˆCross-Pollination & Hybrid Methodsï¼‰")
        level3 = future.get('level_3_innovative_ideas', {})
        summary3 = level3.get('summary', {})
        logger.info(f"  è·¨åŸŸè¿ç§»: {summary3.get('cross_domain_count', 0)} ä¸ª")
        logger.info(f"  ç»„åˆæ‹³: {summary3.get('hybrid_count', 0)} ä¸ª")
        logger.info(f"  æ€»è®¡: {summary3.get('total_ideas', 0)} ä¸ªåˆ›æ–°å‹Idea\n")

        # æ¨¡å¼Aï¼šè·¨åŸŸè¿ç§»
        cross_domain = level3.get('cross_domain_transfer', [])
        if cross_domain:
            logger.info(f"  ğŸ”„ æ¨¡å¼Aï¼šè·¨åŸŸè¿ç§»ï¼ˆå€Ÿå°¸è¿˜é­‚ï¼‰\n")
            for i, idea in enumerate(cross_domain[:4], 1):
                method = idea['method_paper']
                target = idea['target_paper']
                logger.info(f"    [Idea{i}] {idea['innovation_story']}")
                logger.info(f"      æ–¹æ³•æ¥æº: {method['title']}")
                logger.info(f"      ç›®æ ‡é—®é¢˜: {target['title']}")
                logger.info(f"      å¯è¡Œæ€§: {idea['feasibility']:.2f}, è·¨åŸŸ: {'æ˜¯' if idea['is_cross_domain'] else 'å¦'}\n")

        # æ¨¡å¼Bï¼šç»„åˆæ‹³
        hybrid = level3.get('hybrid_methods', [])
        if hybrid:
            logger.info(f"  ğŸ¥Š æ¨¡å¼Bï¼šç»„åˆæ‹³ï¼ˆæ··åˆæ–¹æ³•ï¼‰\n")
            for i, idea in enumerate(hybrid[:4], 1):
                paper_a = idea['paper_a']
                paper_b = idea['paper_b']
                scores = idea['complementarity_scores']
                logger.info(f"    [Idea{i}] æ··åˆæ–¹æ³•ï¼ˆA + Bï¼‰")
                logger.info(f"      æ–¹æ³•A: {paper_a['title']}")
                logger.info(f"      æ–¹æ³•B: {paper_b['title']}")
                logger.info(f"      äº’è¡¥æ€§: {scores['overall']:.2f}, å¯è¡Œæ€§: {idea['hybrid_feasibility']:.2f}")
                logger.info(f"      {idea['hybrid_strategy']}\n")

        logger.info(f"\n{'='*80}")
        logger.info(f"åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæ¯•")
        logger.info(f"{'='*80}\n")


    def _extract_evolutionary_paths(self, graph: nx.DiGraph, year_stats: Dict) -> List[Dict]:
        """
        æå–å…³é”®è¿›åŒ–è·¯å¾„ï¼ˆCritical Evolutionary Path Extractionï¼‰

        åŸºäºè¿›åŒ–åŠ¨é‡æƒé‡ï¼Œæ‰¾å‡ºæ¨åŠ¨é¢†åŸŸè¿›æ­¥çš„ä¸»å¹²è·¯å¾„ã€‚

        ç®—æ³•æ€æƒ³ï¼š
        1. ä¸ºæ¯æ¡è¾¹èµ‹äºˆ"è¿›åŒ–åŠ¨é‡"æƒé‡ï¼ˆåŸºäºå¼•ç”¨ç±»å‹ï¼‰
        2. åœ¨DAGä¸­å¯»æ‰¾æƒé‡å’Œæœ€å¤§çš„è·¯å¾„
        3. è¿™æ¡è·¯å¾„ä»£è¡¨äº†åˆ›æ–°çš„"è„Šæ¢"ï¼ˆBackbone of Innovationï¼‰

        Args:
            graph: çŸ¥è¯†å›¾è°±
            year_stats: å¹´ä»½ç»Ÿè®¡ä¿¡æ¯

        Returns:
            å…³é”®è¿›åŒ–è·¯å¾„åˆ—è¡¨
        """
        try:
            # 1. éªŒè¯å›¾æ˜¯å¦ä¸ºDAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰
            if not nx.is_directed_acyclic_graph(graph):
                logger.warning("å›¾ä¸­å­˜åœ¨ç¯ï¼Œæ— æ³•æå–è¿›åŒ–è·¯å¾„")
                return []

            # 2. ä¸ºæ¯æ¡è¾¹åˆ†é…è¿›åŒ–åŠ¨é‡æƒé‡
            weighted_graph = self._create_weighted_graph(graph)

            # 3. ç¡®å®šæ—¶é—´çª—å£
            if year_stats:
                years = sorted(year_stats.keys())
                start_year = years[0]
                end_year = years[-1]

                # å¦‚æœé…ç½®äº†æ—¶é—´çª—å£ï¼Œç¼©å°èŒƒå›´
                if self.evol_time_window_years:
                    start_year = max(start_year, end_year - self.evol_time_window_years)
            else:
                start_year = None
                end_year = None

            # 4. æ‰¾å‡ºæ—©æœŸå’Œæ™šæœŸèŠ‚ç‚¹
            early_nodes = []
            late_nodes = []

            for node_id in graph.nodes():
                node_year = graph.nodes[node_id].get('year')
                if not node_year:
                    continue

                # æ—©æœŸèŠ‚ç‚¹ï¼ˆèµ·å§‹çª—å£çš„å‰20%ï¼‰
                if start_year and node_year <= start_year + (end_year - start_year) * 0.2:
                    early_nodes.append(node_id)

                # æ™šæœŸèŠ‚ç‚¹ï¼ˆç»“æŸçª—å£çš„å20%ï¼‰
                if end_year and node_year >= end_year - (end_year - start_year) * 0.2:
                    late_nodes.append(node_id)

            if not early_nodes or not late_nodes:
                logger.warning("æ—¶é—´è·¨åº¦ä¸è¶³ï¼Œæ— æ³•æå–è¿›åŒ–è·¯å¾„")
                return []

            # 5. ä»æ¯ä¸ªæ—©æœŸèŠ‚ç‚¹åˆ°æ¯ä¸ªæ™šæœŸèŠ‚ç‚¹å¯»æ‰¾æœ€é‡è·¯å¾„
            all_paths = []

            for start_node in early_nodes:
                for end_node in late_nodes:
                    if start_node == end_node:
                        continue

                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è·¯å¾„
                    if not nx.has_path(weighted_graph, start_node, end_node):
                        continue

                    # ä½¿ç”¨Bellman-Fordç®—æ³•æ‰¾æœ€é•¿è·¯å¾„ï¼ˆå°†æƒé‡å–è´Ÿï¼‰
                    try:
                        # NetworkXçš„æœ€çŸ­è·¯å¾„ç®—æ³•ï¼Œæƒé‡å–è´Ÿå³ä¸ºæœ€é•¿è·¯å¾„
                        path = nx.shortest_path(
                            weighted_graph,
                            start_node,
                            end_node,
                            weight='neg_weight'
                        )

                        # è®¡ç®—è·¯å¾„çš„æ€»æƒé‡
                        total_weight = 0
                        edges_info = []

                        for i in range(len(path) - 1):
                            u, v = path[i], path[i + 1]
                            edge_data = weighted_graph[u][v]
                            weight = edge_data['weight']
                            edge_type = edge_data.get('edge_type', 'Unknown')

                            total_weight += weight
                            edges_info.append({
                                'from': u,
                                'to': v,
                                'type': edge_type,
                                'weight': weight
                            })

                        # è¿‡æ»¤ä½æƒé‡è·¯å¾„
                        if total_weight < self.evol_min_weight:
                            continue

                        # æ”¶é›†è·¯å¾„ä¿¡æ¯
                        path_papers = []
                        path_years = []

                        for node_id in path:
                            node_data = graph.nodes[node_id]
                            path_papers.append({
                                'id': node_id,
                                'title': node_data.get('title', ''),
                                'year': node_data.get('year')
                            })
                            if node_data.get('year'):
                                path_years.append(node_data.get('year'))

                        all_paths.append({
                            'path': path_papers,
                            'edges': edges_info,
                            'total_weight': total_weight,
                            'length': len(path),
                            'year_range': f"{min(path_years)}-{max(path_years)}" if path_years else "Unknown"
                        })

                    except nx.NetworkXNoPath:
                        continue
                    except Exception as e:
                        logger.debug(f"è·¯å¾„è®¡ç®—å¤±è´¥: {e}")
                        continue

            # 6. æŒ‰æƒé‡æ’åºï¼Œå–top N
            all_paths = sorted(all_paths, key=lambda x: x['total_weight'], reverse=True)[:self.evol_max_paths]

            # 7. ç§»é™¤é‡å¤æˆ–é«˜åº¦é‡å çš„è·¯å¾„
            unique_paths = self._remove_duplicate_paths(all_paths)

            logger.info(f"    å‘ç° {len(unique_paths)} æ¡å…³é”®è¿›åŒ–è·¯å¾„")

            return unique_paths

        except Exception as e:
            logger.warning(f"å…³é”®è¿›åŒ–è·¯å¾„æå–å¤±è´¥: {e}")
            return []

    def _create_weighted_graph(self, graph: nx.DiGraph) -> nx.DiGraph:
        """
        åˆ›å»ºå¸¦æƒé‡çš„å›¾

        ä¸ºæ¯æ¡è¾¹åˆ†é…è¿›åŒ–åŠ¨é‡æƒé‡

        Args:
            graph: åŸå§‹å›¾

        Returns:
            å¸¦æƒé‡çš„å›¾
        """
        weighted_graph = graph.copy()

        for u, v, data in weighted_graph.edges(data=True):
            edge_type = data.get('edge_type', 'Unknown')
            weight = self.evolutionary_weights.get(edge_type, 0.3)

            # è®¾ç½®æ­£æƒé‡å’Œè´Ÿæƒé‡ï¼ˆç”¨äºæœ€é•¿è·¯å¾„ç®—æ³•ï¼‰
            weighted_graph[u][v]['weight'] = weight
            weighted_graph[u][v]['neg_weight'] = -weight

        return weighted_graph

    def _remove_duplicate_paths(self, paths: List[Dict], overlap_threshold: float = 0.7) -> List[Dict]:
        """
        ç§»é™¤é‡å¤æˆ–é«˜åº¦é‡å çš„è·¯å¾„

        Args:
            paths: è·¯å¾„åˆ—è¡¨
            overlap_threshold: é‡å é˜ˆå€¼ï¼ˆèŠ‚ç‚¹é‡å æ¯”ä¾‹ï¼‰

        Returns:
            å»é‡åçš„è·¯å¾„åˆ—è¡¨
        """
        if len(paths) <= 1:
            return paths

        unique_paths = []

        for i, path1 in enumerate(paths):
            is_duplicate = False
            nodes1 = set([p['id'] for p in path1['path']])

            for path2 in unique_paths:
                nodes2 = set([p['id'] for p in path2['path']])

                # è®¡ç®—Jaccardç›¸ä¼¼åº¦
                intersection = len(nodes1 & nodes2)
                union = len(nodes1 | nodes2)

                if union > 0:
                    overlap = intersection / union
                    if overlap >= overlap_threshold:
                        is_duplicate = True
                        break

            if not is_duplicate:
                unique_paths.append(path1)

        return unique_paths

    def _detect_technical_bifurcations(self, graph: nx.DiGraph) -> List[Dict]:
        """
        æ£€æµ‹æŠ€æœ¯åˆ†æ­§ç‚¹ï¼ˆTechnical Bifurcation Detectionï¼‰- å¢å¼ºç‰ˆ

        è¯†åˆ«æŠ€æœ¯å‘å±•ä¸­çš„"å²”è·¯å£"â€”â€”åŒä¸€ä¸ªé—®é¢˜è¡ç”Ÿå‡ºä¸åŒçš„æŠ€æœ¯æµæ´¾

        æ ¸å¿ƒæ€æƒ³ï¼š
        1. å¯»æ‰¾è¢«å¤šä¸ªåç»­å·¥ä½œé€šè¿‡Alternativeå¼•ç”¨çš„èŠ‚ç‚¹
        2. æˆ–è€…ï¼šå¯»æ‰¾Problemå¼•å‘äº†å¤šä¸ªä¸åŒMethodçš„èŠ‚ç‚¹
        3. åˆ†æè¿™äº›åˆ†å‰åçš„å­æ ‘è§„æ¨¡ï¼ˆå“ªæ¡è·¯èµ°å¾—æ›´è¿œï¼Ÿå“ªæ¡è·¯æ­»æ‰äº†ï¼Ÿï¼‰

        ç®—æ³•æ­¥éª¤ï¼š
        1. å¯»æ‰¾åˆ†å‰ç»“æ„ï¼šçˆ¶èŠ‚ç‚¹Pè¢«å¤šä¸ªå­èŠ‚ç‚¹å¼•ç”¨ï¼Œä¸”è¾¹ç±»å‹ä¸ºAlternative/Extends
        2. éªŒè¯å­èŠ‚ç‚¹é—´æ— å¼ºå¼•ç”¨å…³ç³»ï¼ˆç‹¬ç«‹å‘å±•ï¼‰
        3. è¯­ä¹‰éªŒè¯ï¼šå­èŠ‚ç‚¹Methodä¸åŒä½†Problemç›¸åŒ
        4. è¿½è¸ªåˆ†æ”¯åç»­å‘å±•ï¼ˆå­æ ‘å¤§å°åˆ†æï¼‰

        Args:
            graph: çŸ¥è¯†å›¾è°±

        Returns:
            æŠ€æœ¯åˆ†æ­§ç‚¹åˆ—è¡¨ï¼ˆåŒ…å«åˆ†æ”¯åç»­å‘å±•åˆ†æï¼‰
        """
        try:
            bifurcations = []

            # 1. éå†æ‰€æœ‰èŠ‚ç‚¹ï¼Œå¯»æ‰¾æ½œåœ¨çš„åˆ†å‰çˆ¶èŠ‚ç‚¹
            for parent_id in graph.nodes():
                # è·å–çˆ¶èŠ‚ç‚¹çš„æ‰€æœ‰åç»§èŠ‚ç‚¹ï¼ˆè¢«å¼•ç”¨çš„è®ºæ–‡ï¼‰
                successors = list(graph.successors(parent_id))

                if len(successors) < self.bifur_min_children:
                    continue

                # 2. ç­›é€‰ç¬¦åˆfork edgeç±»å‹çš„å­èŠ‚ç‚¹
                fork_children = []
                for child_id in successors:
                    edge_data = graph[parent_id][child_id]
                    edge_type = edge_data.get('edge_type', 'Unknown')

                    if edge_type in self.bifur_fork_edge_types:
                        fork_children.append(child_id)

                if len(fork_children) < self.bifur_min_children:
                    continue

                # 3. æ£€æµ‹å­èŠ‚ç‚¹ä¸¤ä¸¤ä¹‹é—´æ˜¯å¦ç‹¬ç«‹ï¼ˆæ— å¼ºå¼•ç”¨å…³ç³»ï¼‰
                independent_pairs = []

                for i in range(len(fork_children)):
                    for j in range(i + 1, len(fork_children)):
                        child_a = fork_children[i]
                        child_b = fork_children[j]

                        # æ£€æŸ¥Aâ†’Bæˆ–Bâ†’Aæ˜¯å¦å­˜åœ¨å¼ºå¼•ç”¨
                        has_strong_link = False

                        if graph.has_edge(child_a, child_b):
                            edge_type = graph[child_a][child_b].get('edge_type', 'Unknown')
                            if edge_type in ['Overcomes', 'Realizes', 'Extends']:
                                has_strong_link = True

                        if graph.has_edge(child_b, child_a):
                            edge_type = graph[child_b][child_a].get('edge_type', 'Unknown')
                            if edge_type in ['Overcomes', 'Realizes', 'Extends']:
                                has_strong_link = True

                        # å¦‚æœæ²¡æœ‰å¼ºå¼•ç”¨ï¼Œè§†ä¸ºç‹¬ç«‹åˆ†æ”¯
                        if not has_strong_link:
                            independent_pairs.append((child_a, child_b))

                if not independent_pairs:
                    continue

                # 4. è¯­ä¹‰éªŒè¯ï¼šMethodä¸åŒï¼Œä½†Problemç›¸åŒ
                parent_data = graph.nodes[parent_id]

                for child_a, child_b in independent_pairs:
                    child_a_data = graph.nodes[child_a]
                    child_b_data = graph.nodes[child_b]

                    # ä»deep_analysisæå–Methodå’ŒProblemå­—æ®µ
                    child_a_deep = child_a_data.get('deep_analysis', {})
                    child_b_deep = child_b_data.get('deep_analysis', {})

                    method_a = child_a_deep.get('method', {}).get('content', '') or child_a_data.get('title', '')
                    method_b = child_b_deep.get('method', {}).get('content', '') or child_b_data.get('title', '')
                    problem_a = child_a_deep.get('problem', {}).get('content', '') or child_a_data.get('abstract', '')
                    problem_b = child_b_deep.get('problem', {}).get('content', '') or child_b_data.get('abstract', '')

                    # è®¡ç®—ç›¸ä¼¼åº¦
                    method_similarity = self._calculate_text_similarity(method_a, method_b)
                    problem_similarity = self._calculate_text_similarity(problem_a, problem_b)

                    # åˆ¤å®šä¸ºæŠ€æœ¯åˆ†æ­§ï¼šMethodä¸åŒä½†Problemç›¸åŒ
                    if (method_similarity < self.bifur_method_sim_threshold and
                        problem_similarity > self.bifur_problem_sim_threshold):

                        # 5. è¿½è¸ªåˆ†æ”¯åç»­å‘å±•ï¼ˆå­æ ‘åˆ†æï¼‰
                        branch_a_subtree = self._analyze_branch_subtree(graph, child_a)
                        branch_b_subtree = self._analyze_branch_subtree(graph, child_b)

                        bifurcations.append({
                            'parent': {
                                'id': parent_id,
                                'title': parent_data.get('title', '')[:80],
                                'year': parent_data.get('year'),
                                'cited_by_count': parent_data.get('cited_by_count', 0)
                            },
                            'branches': [
                                {
                                    'id': child_a,
                                    'title': child_a_data.get('title', '')[:80],
                                    'year': child_a_data.get('year'),
                                    'edge_type': graph[parent_id][child_a].get('edge_type', 'Unknown'),
                                    'subtree_size': branch_a_subtree['size'],
                                    'subtree_depth': branch_a_subtree['depth'],
                                    'subtree_status': branch_a_subtree['status']
                                },
                                {
                                    'id': child_b,
                                    'title': child_b_data.get('title', '')[:80],
                                    'year': child_b_data.get('year'),
                                    'edge_type': graph[parent_id][child_b].get('edge_type', 'Unknown'),
                                    'subtree_size': branch_b_subtree['size'],
                                    'subtree_depth': branch_b_subtree['depth'],
                                    'subtree_status': branch_b_subtree['status']
                                }
                            ],
                            'method_similarity': method_similarity,
                            'problem_similarity': problem_similarity,
                            'divergence_score': problem_similarity - method_similarity,  # åˆ†æ­§è¯„åˆ†
                            'branch_comparison': self._compare_branches(branch_a_subtree, branch_b_subtree)
                        })

            # 5. æŒ‰åˆ†æ­§è¯„åˆ†æ’åºï¼Œå–top N
            bifurcations = sorted(
                bifurcations,
                key=lambda x: x['divergence_score'],
                reverse=True
            )[:self.bifur_max_bifurcations]

            logger.info(f"    å‘ç° {len(bifurcations)} ä¸ªæŠ€æœ¯åˆ†æ­§ç‚¹")

            return bifurcations

        except Exception as e:
            logger.warning(f"æŠ€æœ¯åˆ†æ­§ç‚¹æ£€æµ‹å¤±è´¥: {e}")
            return []

    def _analyze_branch_subtree(self, graph: nx.DiGraph, root_node: str) -> Dict:
        """
        åˆ†æåˆ†æ”¯çš„å­æ ‘å‘å±•æƒ…å†µ

        Args:
            graph: çŸ¥è¯†å›¾è°±
            root_node: åˆ†æ”¯æ ¹èŠ‚ç‚¹

        Returns:
            å­æ ‘åˆ†æç»“æœ
        """
        try:
            # ä½¿ç”¨BFSæ‰¾åˆ°æ‰€æœ‰åä»£èŠ‚ç‚¹
            descendants = list(nx.descendants(graph, root_node))
            subtree_size = len(descendants)

            # è®¡ç®—æœ€å¤§æ·±åº¦
            max_depth = 0
            if descendants:
                for desc in descendants:
                    if nx.has_path(graph, root_node, desc):
                        try:
                            path = nx.shortest_path(graph, root_node, desc)
                            depth = len(path) - 1
                            max_depth = max(max_depth, depth)
                        except:
                            continue

            # åˆ¤æ–­åˆ†æ”¯çŠ¶æ€
            status = 'unknown'
            if subtree_size == 0:
                status = 'dead'  # æ­»è·¯
            elif subtree_size < 5:
                status = 'weak'  # å¼±åŠ¿å‘å±•
            elif subtree_size >= 5 and subtree_size < 15:
                status = 'moderate'  # ä¸­ç­‰å‘å±•
            else:
                status = 'strong'  # å¼ºåŠ¿å‘å±•

            return {
                'size': subtree_size,
                'depth': max_depth,
                'status': status
            }

        except Exception as e:
            logger.debug(f"åˆ†æ”¯å­æ ‘åˆ†æå¤±è´¥: {e}")
            return {
                'size': 0,
                'depth': 0,
                'status': 'unknown'
            }

    def _compare_branches(self, branch_a: Dict, branch_b: Dict) -> str:
        """
        æ¯”è¾ƒä¸¤ä¸ªåˆ†æ”¯çš„å‘å±•æƒ…å†µ

        Args:
            branch_a: åˆ†æ”¯Açš„å­æ ‘åˆ†æ
            branch_b: åˆ†æ”¯Bçš„å­æ ‘åˆ†æ

        Returns:
            æ¯”è¾ƒç»“è®º
        """
        size_a = branch_a['size']
        size_b = branch_b['size']

        if size_a == 0 and size_b == 0:
            return "ä¸¤æ¡è·¯çº¿å‡æœªè·å¾—åç»­å‘å±•"
        elif size_a == 0:
            return f"åˆ†æ”¯Aå·²æ­»ï¼Œåˆ†æ”¯Bè·å¾—{size_b}ä¸ªåç»­å·¥ä½œï¼Œæˆä¸ºä¸»æµè·¯çº¿"
        elif size_b == 0:
            return f"åˆ†æ”¯Bå·²æ­»ï¼Œåˆ†æ”¯Aè·å¾—{size_a}ä¸ªåç»­å·¥ä½œï¼Œæˆä¸ºä¸»æµè·¯çº¿"
        elif size_a > size_b * 2:
            return f"åˆ†æ”¯Aå¼ºåŠ¿é¢†å…ˆï¼ˆ{size_a} vs {size_b}ï¼‰ï¼Œæˆä¸ºä¸»æµè·¯çº¿"
        elif size_b > size_a * 2:
            return f"åˆ†æ”¯Bå¼ºåŠ¿é¢†å…ˆï¼ˆ{size_b} vs {size_a}ï¼‰ï¼Œæˆä¸ºä¸»æµè·¯çº¿"
        else:
            return f"ä¸¤æ¡è·¯çº¿åŠ¿å‡åŠ›æ•Œï¼ˆ{size_a} vs {size_b}ï¼‰ï¼ŒæŠ€æœ¯è·¯çº¿ä¹‹äº‰ä»åœ¨ç»§ç»­"

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦

        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        if not text1 or not text2:
            return 0.0

        try:
            if self.bifur_use_cosine:
                # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆéœ€è¦å‘é‡åŒ–ï¼‰
                # ç®€å•å®ç°ï¼šä½¿ç”¨è¯è¢‹æ¨¡å‹
                words1 = set(re.findall(r'\b\w+\b', text1.lower()))
                words2 = set(re.findall(r'\b\w+\b', text2.lower()))

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆJaccardç›¸ä¼¼åº¦ä½œä¸ºè¿‘ä¼¼ï¼‰
                if not words1 or not words2:
                    return 0.0

                intersection = len(words1 & words2)
                union = len(words1 | words2)

                return intersection / union if union > 0 else 0.0

            else:
                # ä½¿ç”¨Jaccardç›¸ä¼¼åº¦
                words1 = set(re.findall(r'\b\w+\b', text1.lower()))
                words2 = set(re.findall(r'\b\w+\b', text2.lower()))

                if not words1 or not words2:
                    return 0.0

                intersection = len(words1 & words2)
                union = len(words1 | words2)

                return intersection / union if union > 0 else 0.0

        except Exception as e:
            logger.debug(f"æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def _detect_open_frontiers(self, graph: nx.DiGraph, year_stats: Dict) -> Dict:
        """
        æ£€æµ‹æœªé—­åˆå‰æ²¿ï¼ˆOpen Frontier Detectionï¼‰

        è¯†åˆ«æœªè¢«è§£å†³çš„å¼€æ”¾é—®é¢˜å¹¶ç”Ÿæˆè·¨åŸŸè¿ç§»ç ”ç©¶Idea

        ç®—æ³•æ­¥éª¤ï¼š
        1. ç­›é€‰è¾¹ç¼˜èŠ‚ç‚¹ï¼šæœ€è¿‘Nå¹´çš„è®ºæ–‡
        2. ç¼ºé™·é—­ç¯æ£€æµ‹ï¼šæ£€æŸ¥Limitationæ˜¯å¦è¢«åç»­å·¥ä½œè§£å†³
        3. è·¨åŸŸåŒ¹é…ï¼šåŒ¹é…Limitationä¸å…¶ä»–è®ºæ–‡çš„Contribution

        Args:
            graph: çŸ¥è¯†å›¾è°±
            year_stats: å¹´ä»½ç»Ÿè®¡ä¿¡æ¯

        Returns:
            å¼€æ”¾å‰æ²¿å­—å…¸ï¼ŒåŒ…å«open_problemså’Œresearch_ideas
        """
        try:
            # 1. ç­›é€‰è¾¹ç¼˜èŠ‚ç‚¹ï¼ˆæœ€è¿‘Nå¹´ï¼‰
            if not year_stats:
                logger.warning("æ— å¹´ä»½ä¿¡æ¯ï¼Œæ— æ³•ç­›é€‰è¾¹ç¼˜èŠ‚ç‚¹")
                return {'open_problems': [], 'research_ideas': []}

            years = sorted(year_stats.keys())
            latest_year = years[-1]
            cutoff_year = latest_year - self.frontier_recent_years

            leaf_nodes = []
            for node_id in graph.nodes():
                node_data = graph.nodes[node_id]
                node_year = node_data.get('year')

                if node_year and node_year >= cutoff_year:
                    leaf_nodes.append(node_id)

            if not leaf_nodes:
                logger.warning(f"æ— æœ€è¿‘{self.frontier_recent_years}å¹´çš„è®ºæ–‡")
                return {'open_problems': [], 'research_ideas': []}

            logger.info(f"    ç­›é€‰å‡º {len(leaf_nodes)} ä¸ªè¾¹ç¼˜èŠ‚ç‚¹ï¼ˆ{cutoff_year}-{latest_year}å¹´ï¼‰")

            # 2. ç¼ºé™·é—­ç¯æ£€æµ‹ï¼šæ‰¾å‡ºæœªè¢«è§£å†³çš„Limitation
            open_problems = []

            for node_id in leaf_nodes:
                node_data = graph.nodes[node_id]

                # ä»deep_analysisè·å–limitationæˆ–future_work
                deep_analysis = node_data.get('deep_analysis', {})
                limitation = deep_analysis.get('limitation', {}).get('content', '')
                if not limitation:
                    limitation = deep_analysis.get('future_work', {}).get('content', '')

                if not limitation:
                    continue  # æ— Limitationï¼Œè·³è¿‡

                # æ£€æŸ¥æ˜¯å¦æœ‰åç»­å·¥ä½œé€šè¿‡Overcomesæˆ–Realizesè§£å†³
                has_closure = False

                # è·å–æ‰€æœ‰å¼•ç”¨è¯¥èŠ‚ç‚¹çš„è®ºæ–‡ï¼ˆåç»­å·¥ä½œï¼‰
                predecessors = list(graph.predecessors(node_id))

                for pred_id in predecessors:
                    edge_data = graph[pred_id][node_id]
                    edge_type = edge_data.get('edge_type', 'Unknown')

                    # å¦‚æœæœ‰Overcomesæˆ–Realizesç±»å‹çš„å¼•ç”¨ï¼Œè¯´æ˜é—®é¢˜å·²è¢«è§£å†³
                    if edge_type in ['Overcomes', 'Realizes']:
                        has_closure = True
                        break

                # å¦‚æœæœªè¢«è§£å†³ï¼Œè®°å½•ä¸ºå¼€æ”¾é—®é¢˜
                if not has_closure:
                    open_problems.append({
                        'paper': {
                            'id': node_id,
                            'title': node_data.get('title', ''),
                            'year': node_data.get('year')
                        },
                        'limitation': limitation,
                        'out_degree': graph.out_degree(node_id)  # æœ‰å¤šå°‘åç»­å·¥ä½œï¼ˆä½†æ²¡è§£å†³ï¼‰
                    })

            # æŒ‰out_degreeé™åºæ’åºï¼ˆè¶Šå¤šåç»­å·¥ä½œä½†æœªè§£å†³çš„è¶Šé‡è¦ï¼‰
            open_problems = sorted(
                open_problems,
                key=lambda x: x['out_degree'],
                reverse=True
            )[:self.frontier_max_open_problems]

            logger.info(f"    å‘ç° {len(open_problems)} ä¸ªæœªé—­åˆçš„å¼€æ”¾é—®é¢˜")

            # 3. è·¨åŸŸåŒ¹é…ï¼šä¸ºæ¯ä¸ªå¼€æ”¾é—®é¢˜ç”Ÿæˆç ”ç©¶Idea
            research_ideas = self._generate_cross_domain_ideas(graph, open_problems)

            logger.info(f"    ç”Ÿæˆ {len(research_ideas)} ä¸ªè·¨åŸŸè¿ç§»Idea")

            return {
                'open_problems': open_problems,
                'research_ideas': research_ideas
            }

        except Exception as e:
            logger.warning(f"æœªé—­åˆå‰æ²¿æ¢æµ‹å¤±è´¥: {e}")
            return {'open_problems': [], 'research_ideas': []}

    def _generate_cross_domain_ideas(self, graph: nx.DiGraph, open_problems: List[Dict]) -> List[Dict]:
        """
        ç”Ÿæˆè·¨åŸŸè¿ç§»ç ”ç©¶Idea

        ä¸ºæ¯ä¸ªæœªè§£å†³çš„Limitationæ‰¾åˆ°å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼ˆå…¶ä»–è®ºæ–‡çš„Contributionï¼‰

        Args:
            graph: çŸ¥è¯†å›¾è°±
            open_problems: æœªé—­åˆé—®é¢˜åˆ—è¡¨

        Returns:
            ç ”ç©¶Ideaåˆ—è¡¨
        """
        research_ideas = []

        try:
            # ä¸ºæ¯ä¸ªå¼€æ”¾é—®é¢˜å¯»æ‰¾å€™é€‰è§£å†³æ–¹æ¡ˆ
            for problem in open_problems:
                target_node_id = problem['paper']['id']
                target_limitation = problem['limitation']

                # å€™é€‰è§£å†³æ–¹æ¡ˆåˆ—è¡¨
                candidate_solutions = []

                # éå†å›¾ä¸­æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹
                for candidate_id in graph.nodes():
                    if candidate_id == target_node_id:
                        continue

                    candidate_data = graph.nodes[candidate_id]

                    # ä»deep_analysisè·å–method
                    candidate_deep = candidate_data.get('deep_analysis', {})
                    method = candidate_deep.get('method', {}).get('content', '')

                    if not method:
                        continue

                    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å¼•ç”¨å…³ç³»ï¼ˆé¿å…æ¨èå·²æœ‰çš„å¼•ç”¨ï¼‰
                    if graph.has_edge(target_node_id, candidate_id) or graph.has_edge(candidate_id, target_node_id):
                        continue

                    # è®¡ç®—Limitationä¸Methodçš„è¯­ä¹‰ç›¸ä¼¼åº¦
                    similarity = self._calculate_text_similarity(target_limitation, method)

                    # è¿‡æ»¤ä½ç›¸ä¼¼åº¦çš„å€™é€‰
                    if similarity < self.frontier_lim_sim_threshold:
                        continue

                    candidate_solutions.append({
                        'paper': {
                            'id': candidate_id,
                            'title': candidate_data.get('title', ''),
                            'year': candidate_data.get('year')
                        },
                        'method': method,
                        'similarity': similarity
                    })

                # æŒ‰ç›¸ä¼¼åº¦æ’åº
                candidate_solutions = sorted(
                    candidate_solutions,
                    key=lambda x: x['similarity'],
                    reverse=True
                )

                # ä¸ºè¯¥é—®é¢˜ç”Ÿæˆtop Nä¸ªIdea
                for solution in candidate_solutions[:2]:  # æ¯ä¸ªé—®é¢˜æœ€å¤š2ä¸ªIdea
                    research_ideas.append({
                        'target_paper': problem['paper'],
                        'target_limitation': target_limitation,
                        'solution_paper': solution['paper'],
                        'solution_method': solution['method'],
                        'similarity_score': solution['similarity'],
                        'idea_type': 'cross_domain_transfer'  # è·¨åŸŸè¿ç§»
                    })

            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top N
            research_ideas = sorted(
                research_ideas,
                key=lambda x: x['similarity_score'],
                reverse=True
            )[:self.frontier_max_ideas]

            return research_ideas

        except Exception as e:
            logger.warning(f"ç”Ÿæˆè·¨åŸŸIdeaå¤±è´¥: {e}")
            return []


def create_analyzer(config: Dict = None) -> TopicEvolutionAnalyzer:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºåˆ†æå™¨å®ä¾‹

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        TopicEvolutionAnalyzerå®ä¾‹
    """
    return TopicEvolutionAnalyzer(config)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import networkx as nx

    # åˆ›å»ºæµ‹è¯•å›¾
    G = nx.DiGraph()

    # æ·»åŠ æµ‹è¯•èŠ‚ç‚¹
    papers = [
        {'id': 'p1', 'title': 'Attention Is All You Need', 'year': 2017, 'cited_by_count': 50000},
        {'id': 'p2', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers', 'year': 2018, 'cited_by_count': 30000},
        {'id': 'p3', 'title': 'GPT-3: Language Models are Few-Shot Learners', 'year': 2020, 'cited_by_count': 20000},
        {'id': 'p4', 'title': 'Vision Transformer for Image Recognition', 'year': 2020, 'cited_by_count': 15000},
        {'id': 'p5', 'title': 'Switch Transformers: Scaling to Trillion Parameter Models', 'year': 2021, 'cited_by_count': 5000},
    ]

    for paper in papers:
        G.add_node(paper['id'], **paper)

    # æ·»åŠ å¼•ç”¨å…³ç³»
    G.add_edge('p2', 'p1', edge_type='Extends')
    G.add_edge('p3', 'p2', edge_type='Overcomes')
    G.add_edge('p4', 'p1', edge_type='Adapts_to')
    G.add_edge('p5', 'p3', edge_type='Extends')

    # åˆ›å»ºåˆ†æå™¨
    analyzer = create_analyzer()

    # æ‰§è¡Œåˆ†æ
    report = analyzer.analyze(G, 'Transformer Neural Networks')

    # è¾“å‡ºæŠ¥å‘Š
    print("\n" + "="*60)
    print("åˆ†ææŠ¥å‘Š:")
    print("="*60)
    print(f"ä¸»é¢˜: {report['topic']}")
    print(f"æ—¶é—´è·¨åº¦: {report['graph_overview']['year_range']}")
    print(f"é‡Œç¨‹ç¢‘è®ºæ–‡æ•°: {len(report['milestone_papers'])}")
    print(f"ç ”ç©¶åˆ†æ”¯æ•°: {len(report['research_branches'])}")
