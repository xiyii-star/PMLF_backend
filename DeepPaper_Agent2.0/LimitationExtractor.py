"""
Limitation Extractor (å±€é™æ€§æå–å™¨)
ç»“åˆç« èŠ‚å®šä½å’Œå¼•ç”¨åˆ†ææå–è®ºæ–‡çš„å±€é™æ€§

å·¥ä½œæµç¨‹:
1. ä½¿ç”¨ SectionLocatorAgent å®šä½ limitation ç« èŠ‚
2. ä»å®šä½çš„ç« èŠ‚ä¸­æå– limitation
3. ä½¿ç”¨ CitationDetectiveAgent ä»å¼•ç”¨ä¸­æå–é¢å¤–çš„ limitation
4. åˆå¹¶ä¸¤éƒ¨åˆ†ç»“æœ
5. ğŸ†• ä½¿ç”¨ CriticAgent å®¡æŸ¥å¹¶è‡ªåŠ¨é‡è¯•æå–
"""

import json
import logging
import re
from typing import List, Dict, Optional, Any
from pathlib import Path

# å¯¼å…¥æ•°æ®ç»“æ„
from data_structures import PaperDocument, ExtractionResult, FieldType, SectionScope, CriticFeedback

# å¯¼å…¥å…¶ä»–Agent
from SectionLocatorAgent import SectionLocatorAgent
from CitationDetectiveAgent import CitationDetectiveAgent
from critic_agent import CriticAgent

# å¯¼å…¥LLMé…ç½®
import sys
sys.path.append(str(Path(__file__).parent.parent))
from src.llm_config import LLMClient, LLMConfig

logger = logging.getLogger(__name__)


class LimitationExtractor:
    """
    Limitation æå–å™¨
    ç»“åˆç« èŠ‚å®šä½å’Œå¼•ç”¨åˆ†æ
    """

    def __init__(
        self,
        llm_client: LLMClient,
        use_citation_analysis: bool = False,
        use_critic: bool = True,
        max_context_length: int = 3000,
        max_iterations: int = 3
    ):
        """
        åˆå§‹åŒ– Limitation æå–å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            use_citation_analysis: æ˜¯å¦ä½¿ç”¨å¼•ç”¨åˆ†æ
            use_critic: æ˜¯å¦ä½¿ç”¨CriticAgentè¿›è¡Œè´¨é‡æ£€æŸ¥å’Œé‡è¯•
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            max_iterations: æœ€å¤§é‡è¯•æ¬¡æ•°(å½“use_critic=Trueæ—¶)
        """
        self.llm_client = llm_client
        self.use_citation_analysis = use_citation_analysis
        self.use_critic = use_critic
        self.max_context_length = max_context_length

        # åˆå§‹åŒ–å­ç»„ä»¶
        self.locator = SectionLocatorAgent(llm_client)
        if use_citation_analysis:
            self.citation_detective = CitationDetectiveAgent(llm_client)
        else:
            self.citation_detective = None

        # ğŸ†• åˆå§‹åŒ– CriticAgent
        if use_critic:
            self.critic = CriticAgent(llm_client, max_iterations=max_iterations)
        else:
            self.critic = None

        self.system_prompt = self._build_system_prompt()

    def _build_system_prompt(self) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        return """You are a professional expert in scientific paper analysis.

Your Task: Extract the limitations of the method proposed in this paper.

âš ï¸ Important Distinction:

âœ… Extract: Limitations of "this paper" / "our method" / "we".
âŒ Ignore: Criticisms of "prior work" / "baseline methods" made by the authors.
âŒ Ignore: Weaknesses of other papers mentioned in the "Related Work" section.
Identification Techniques:

Look for transition words: However, Unfortunately, Limitation, still, yet.
Focus on self-references: "our method", "our approach", "the proposed framework".
Pay attention to honest descriptions usually found at the end of the Discussion or Conclusion sections.
Output Requirements:

List 2-4 specific limitations.
Use 1-2 sentences to explain each limitation.
Use bullet points format.
Output the content directly, avoiding meta-talk (e.g., do not say "According to the paragraph...").

"""

    def extract(
        self,
        paper: PaperDocument,
        paper_id: Optional[str] = None,
        feedback: Optional[CriticFeedback] = None
    ) -> ExtractionResult:
        """
        æå–è®ºæ–‡çš„å±€é™æ€§

        Args:
            paper: è®ºæ–‡æ–‡æ¡£
            paper_id: è®ºæ–‡IDï¼ˆç”¨äºå¼•ç”¨åˆ†æï¼Œå¯é€‰ï¼‰
            feedback: CriticAgentåé¦ˆï¼ˆç”¨äºé‡è¯•ï¼Œå¯é€‰ï¼‰

        Returns:
            ExtractionResult: æå–ç»“æœ
        """
        logger.info("ğŸ“‹ å¼€å§‹æå– Limitation...")

        # ç¬¬ä¸€éƒ¨åˆ†: ä»è®ºæ–‡ç« èŠ‚æå–
        logger.info("  ğŸ“– [Part 1] ä»è®ºæ–‡ç« èŠ‚æå–...")
        section_limitations, section_evidence, scope = self._extract_from_sections(paper, feedback)

        # ç¬¬äºŒéƒ¨åˆ†: ä»å¼•ç”¨åˆ†ææå–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        citation_limitations = []
        if self.use_citation_analysis and paper_id:
            logger.info("  ğŸ” [Part 2] ä»å¼•ç”¨åˆ†ææå–...")
            citation_limitations = self._extract_from_citations(paper_id)

        # åˆå¹¶ç»“æœ
        merged_content, merged_evidence = self._merge_results(
            section_limitations,
            section_evidence,
            citation_limitations
        )

        logger.info(f"  âœ… Limitationåˆæ­¥æå–å®Œæˆ")
        logger.info(f"     â†’ ç« èŠ‚æå–: {len(section_limitations)} æ¡")
        logger.info(f"     â†’ å¼•ç”¨æå–: {len(citation_limitations)} æ¡")
        logger.info(f"     â†’ æœ€ç»ˆå†…å®¹: {merged_content[:100]}...")

        initial_result = ExtractionResult(
            field=FieldType.LIMITATION,
            content=merged_content,
            evidence=merged_evidence,
            extraction_method="section_locator + citation_detective" if self.use_citation_analysis else "section_locator",
            confidence=0.8 if section_limitations else 0.3,
            iterations=1
        )

        # ğŸ†• ä½¿ç”¨ CriticAgent è¿›è¡Œè´¨é‡æ£€æŸ¥å’Œè‡ªåŠ¨é‡è¯•
        if self.use_critic and self.critic and not feedback:  # åªåœ¨åˆæ¬¡æå–æ—¶ä½¿ç”¨critic
            logger.info("\n  ğŸ” å¯åŠ¨ CriticAgent è´¨é‡æ£€æŸ¥...")

            # å®šä¹‰é‡æ–°æå–å‡½æ•°
            def retry_extract(paper_doc, critic_feedback):
                return self.extract(paper_doc, paper_id, critic_feedback)

            # è°ƒç”¨ critique_and_retry
            final_result = self.critic.critique_and_retry(
                extraction=initial_result,
                paper=paper,
                extractor_func=retry_extract,
                scope=scope,
                evaluation_level="both"
            )

            return final_result

        return initial_result

    def _extract_from_sections(self, paper: PaperDocument, feedback: Optional[CriticFeedback] = None) -> tuple:
        """
        ä»è®ºæ–‡ç« èŠ‚æå–å±€é™æ€§

        Args:
            paper: è®ºæ–‡æ–‡æ¡£
            feedback: Criticåé¦ˆ(ç”¨äºé‡è¯•)

        Returns:
            (limitationsåˆ—è¡¨, evidenceåˆ—è¡¨, scope)
        """
        # æ­¥éª¤1: å®šä½ç« èŠ‚ (å¦‚æœæœ‰åé¦ˆ,ä½¿ç”¨å»ºè®®çš„ç« èŠ‚)
        if feedback and feedback.suggested_sections:
            logger.info(f"  â†’ ä½¿ç”¨Criticå»ºè®®çš„ç« èŠ‚: {feedback.suggested_sections}")
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶scope
            scope = SectionScope(
                field=FieldType.LIMITATION,
                target_sections=feedback.suggested_sections,
                section_titles=[paper.sections[i].title for i in feedback.suggested_sections if i < len(paper.sections)],
                reasoning="Based on Critic feedback"
            )
        else:
            scope = self.locator.locate(paper, FieldType.LIMITATION)

        if not scope.target_sections:
            logger.warning("  âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ç« èŠ‚")
            return [], [], scope

        # æ­¥éª¤2: æå–ç›¸å…³æ®µè½
        relevant_chunks = self._extract_relevant_chunks(paper, scope)

        if not relevant_chunks:
            logger.warning("  âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ®µè½")
            return [], [], scope

        # æ­¥éª¤3: ä½¿ç”¨LLMæå– (å¸¦ä¸Šfeedbackä»¥å¢å¼ºæç¤º)
        limitations, evidence = self._extract_with_llm(relevant_chunks, paper.title, feedback)

        return limitations, evidence, scope

    def _extract_relevant_chunks(
        self,
        paper: PaperDocument,
        scope: SectionScope
    ) -> List[Dict]:
        """
        æå–ç›¸å…³æ®µè½ï¼ˆä½¿ç”¨å…³é”®è¯åŒ¹é…ï¼‰

        ğŸ†• ä¼˜åŒ–ç­–ç•¥:
        1. æ‰©å±•å…³é”®è¯åˆ—è¡¨ï¼ˆåŒ…æ‹¬reviewç±»è®ºæ–‡çš„ç‰¹æ®Šè¯æ±‡ï¼‰
        2. è€ƒè™‘æ®µè½ä½ç½®ï¼ˆé¦–æ®µ/å°¾æ®µæƒé‡æ›´é«˜ï¼‰
        3. ç»“åˆè½¬æŠ˜è¯è¯†åˆ«
        4. æ™ºèƒ½é™çº§ï¼šå½“åŒ¹é…å°‘æ—¶è‡ªåŠ¨æ‰©å±•åˆ°æ‰€æœ‰æ®µè½
        """
        # ğŸ†• æ‰©å±•å…³é”®è¯åˆ—è¡¨ï¼ˆå‚è€ƒDeepPaper_Agentï¼‰
        keywords = [
            # åŸºç¡€limitationè¯
            'limitation', 'drawback', 'weakness', 'shortcoming',
            'however', 'unfortunately', 'lack', 'cannot',
            'difficult', 'challenge', 'remains', 'future work',
            'still', 'yet to', 'not yet', 'constrained',
            'trade-off', 'sacrifice',
            # ğŸ†• review/å·¥å…·ç±»è®ºæ–‡çš„limitationè¡¨è¿°
            'limited to', 'restricted', 'incomplete', 'partial',
            'not cover', 'exclude', 'beyond scope', 'out of scope',
            'fail to', 'unable to', 'not address', 'not handle',
            # ğŸ†• æ›´å¤šè½¬æŠ˜å’Œæ‰¿è®¤ä¸è¶³çš„è¡¨è¿°
            'although', 'nevertheless', 'despite', 'admits',
            'acknowledge', 'recognize', 'confess', 'concede'
        ]

        relevant_chunks = []

        for section_idx in scope.target_sections:
            section = paper.sections[section_idx]
            paragraphs = self._split_into_paragraphs(section.content)

            for para_idx, para in enumerate(paragraphs):
                # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
                keyword_count = sum(
                    1 for kw in keywords
                    if kw.lower() in para.lower()
                )

                if keyword_count > 0:
                    # ğŸ†• è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆè€ƒè™‘ä½ç½®å’Œè½¬æŠ˜è¯ï¼‰
                    score = keyword_count

                    # ğŸ†• ä½ç½®æƒé‡ï¼šé¦–æ®µå’Œå°¾æ®µæƒé‡é«˜
                    if para_idx == 0:  # é¦–æ®µ
                        score += 1.5
                    elif para_idx >= len(paragraphs) - 2:  # æœ«å°¾ä¸¤æ®µ
                        score += 2.5  # limitationæ›´å¸¸å‡ºç°åœ¨æœ«å°¾

                    # ğŸ†• è½¬æŠ˜è¯æƒé‡ï¼ˆå¯¹Limitationå¾ˆé‡è¦ï¼‰
                    transition_words = [
                        'however', 'unfortunately', 'limitation',
                        'remains', 'still', 'despite', 'although',
                        'nevertheless', 'yet to'
                    ]
                    if any(tw in para.lower() for tw in transition_words):
                        score += 2.0

                    # ğŸ†• "æœ¬æ–‡æ–¹æ³•"æŒ‡ç¤ºè¯æƒé‡ï¼ˆç¡®ä¿æå–çš„æ˜¯æœ¬æ–‡çš„limitationï¼‰
                    self_reference_words = [
                        'our method', 'our approach', 'our model',
                        'our system', 'our work', 'we', 'this paper'
                    ]
                    if any(sw in para.lower() for sw in self_reference_words):
                        score += 1.0

                    relevant_chunks.append({
                        'section': section.title,
                        'text': para,
                        'page': section.page_num,
                        'keyword_count': keyword_count,
                        'score': score,
                        'position': para_idx
                    })

        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        relevant_chunks.sort(key=lambda x: x['score'], reverse=True)

        # ğŸ†• æ™ºèƒ½é™çº§ï¼šå¦‚æœchunkså¾ˆå°‘(<3)ï¼Œæ‰©å±•åˆ°ç›®æ ‡ç« èŠ‚çš„æ‰€æœ‰æ®µè½
        max_chunks = 8  # å¢åŠ åˆ°8ä¸ª
        if len(relevant_chunks) < 3:
            logger.info(f"     â†’ å…³é”®è¯åŒ¹é…chunksè¾ƒå°‘({len(relevant_chunks)})ï¼Œæ‰©å±•åˆ°ç›®æ ‡ç« èŠ‚çš„æ‰€æœ‰æ®µè½")
            all_chunks = []
            for section_idx in scope.target_sections:
                section = paper.sections[section_idx]
                paragraphs = self._split_into_paragraphs(section.content)
                # ğŸ†• é‡ç‚¹æå–æ¯ä¸ªç« èŠ‚çš„æœ«å°¾æ®µè½ï¼ˆlimitationé€šå¸¸åœ¨è¿™é‡Œï¼‰
                for para_idx, para in enumerate(paragraphs):
                    # ä¼˜å…ˆé€‰æ‹©æœ«å°¾æ®µè½
                    if para_idx >= len(paragraphs) - 3 or para_idx < 2:
                        all_chunks.append({
                            'section': section.title,
                            'text': para,
                            'page': section.page_num,
                            'keyword_count': 0,
                            'score': 0.5 if para_idx >= len(paragraphs) - 3 else 0.3,
                            'position': para_idx
                        })
            # æŒ‰ä½ç½®å¾—åˆ†æ’åº
            all_chunks.sort(key=lambda x: x['score'], reverse=True)
            return all_chunks[:max_chunks]

        return relevant_chunks[:max_chunks]

    def _split_into_paragraphs(self, text: str) -> List[str]:
        """åˆ†å‰²æ®µè½"""
        paragraphs = re.split(r'\n\s*\n|\n', text)
        paragraphs = [p.strip() for p in paragraphs if len(p.strip()) > 30]
        return paragraphs

    def _extract_with_llm(
        self,
        chunks: List[Dict],
        paper_title: str,
        feedback: Optional[CriticFeedback] = None
    ) -> tuple:
        """
        ä½¿ç”¨LLMä»chunksä¸­æå–å±€é™æ€§

        Args:
            chunks: ç›¸å…³æ®µè½åˆ—è¡¨
            paper_title: è®ºæ–‡æ ‡é¢˜
            feedback: Criticåé¦ˆ(ç”¨äºå¢å¼ºæç¤º)

        Returns:
            (limitationsåˆ—è¡¨, evidenceåˆ—è¡¨)
        """
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, chunk in enumerate(chunks):
            context_parts.append(
                f"[Evidence {i+1}] From section '{chunk['section']}':\n{chunk['text']}"
            )
        context = "\n\n".join(context_parts)

        # ğŸ†• æ™ºèƒ½æˆªæ–­ï¼šä¼˜å…ˆä¿ç•™å¾—åˆ†æœ€é«˜çš„chunks
        if len(context) > self.max_context_length:
            # æŒ‰å¾—åˆ†é‡æ–°æ’åºï¼Œä¿ç•™æœ€ç›¸å…³çš„
            sorted_chunks = sorted(chunks, key=lambda x: x.get('score', 0), reverse=True)
            top_chunks = sorted_chunks[:5]  # ä¿ç•™å‰5ä¸ª
            context_parts = []
            for i, chunk in enumerate(top_chunks):
                context_parts.append(
                    f"[Evidence {i+1}] From section '{chunk['section']}':\n{chunk['text']}"
                )
            context = "\n\n".join(context_parts)
            logger.info(f"     â†’ ä¸Šä¸‹æ–‡è¿‡é•¿,ä¿ç•™å‰{len(top_chunks)}ä¸ªæœ€ç›¸å…³æ®µè½")
            chunks = top_chunks  # æ›´æ–°chunksç”¨äºåç»­evidenceæ„å»º

        # æ„å»ºæç¤ºè¯
        prompt = f"""è®ºæ–‡æ ‡é¢˜: {paper_title}

ä»»åŠ¡: æå–æœ¬æ–‡æ–¹æ³•çš„å±€é™æ€§

âš ï¸ å…³é”®åŒºåˆ†:
- å¦‚æœè¯´"LSTMæ— æ³•å¤„ç†é•¿åºåˆ—" -> è¿™æ˜¯baselineçš„é—®é¢˜,ä¸è¦æå–
- å¦‚æœè¯´"Our method still struggles with..." -> è¿™æ˜¯æœ¬æ–‡çš„å±€é™æ€§,å¿…é¡»æå–

ç›¸å…³æ®µè½:
{context}

è¾“å‡ºè¦æ±‚:
1. åˆ—ä¸¾2-4ä¸ªå…·ä½“çš„å±€é™æ€§
2. æ¯ä¸ªå±€é™æ€§ç”¨1-2å¥è¯è¯´æ˜
3. ä½¿ç”¨bullet pointsæ ¼å¼ (ä»¥ "- " å¼€å¤´)
4. åªæå–æœ¬æ–‡æ–¹æ³•çš„å±€é™æ€§
5. âš ï¸ é‡è¦: å³ä½¿ä¿¡æ¯ä¸å®Œæ•´,ä¹Ÿè¦å°½é‡ä»æ®µè½ä¸­æå–ç›¸å…³å†…å®¹
6. åªæœ‰åœ¨æ®µè½å®Œå…¨ä¸ç›¸å…³æ—¶æ‰è¾“å‡º"æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
"""

        # ğŸ†• å¦‚æœæœ‰Criticåé¦ˆ,æ·»åŠ åˆ°æç¤ºä¸­
        if feedback and feedback.retry_prompt:
            prompt += f"\nâš ï¸ Criticåé¦ˆ:\n{feedback.retry_prompt}\n"

        prompt += "\nè¾“å‡º:"

        try:
            response = self.llm_client.generate(
                prompt=prompt,
                system_prompt=self.system_prompt,
                temperature=0.3,
                max_tokens=1000
            )

            # è®°å½•åŸå§‹å“åº”,ä¾¿äºè°ƒè¯•
            logger.debug(f"     â†’ LLMåŸå§‹å“åº”: {response[:200]}...")

            # è§£æå“åº”
            limitations = self._parse_llm_response(response)

            # ğŸ†• æ£€æµ‹LLMæ˜¯å¦è¿‡åº¦è°¨æ…è¿”å›"æœªæ‰¾åˆ°"
            if self._is_llm_being_too_cautious(limitations, chunks):
                logger.warning(f"     âš ï¸ LLMè¿”å›ä¸ºç©ºæˆ–è¿‡çŸ­,ä½†æœ‰{len(chunks)}æ¡è¯æ® - ä½¿ç”¨é™çº§ç­–ç•¥")
                limitations = self._fallback_extraction(chunks)

            # æ„å»ºevidence
            evidence = [
                {
                    'section': chunk['section'],
                    'text': chunk['text'],
                    'page': chunk['page']
                }
                for chunk in chunks
            ]

            return limitations, evidence

        except Exception as e:
            logger.error(f"     âŒ LLMæå–å¤±è´¥: {e}")
            # é™çº§ï¼šä½¿ç”¨è§„åˆ™æå–
            fallback_limitations = self._fallback_extraction(chunks)
            evidence = [{'section': chunks[0]['section'], 'text': chunks[0]['text'], 'page': chunks[0]['page']}] if chunks else []
            return fallback_limitations, evidence

    def _parse_llm_response(self, response: str) -> List[str]:
        """
        è§£æLLMå“åº”ï¼Œæå–å±€é™æ€§åˆ—è¡¨

        è¿”å›: limitationsåˆ—è¡¨
        """
        limitations = []
        lines = response.strip().split('\n')

        for line in lines:
            line = line.strip()
            # åŒ¹é…bullet points
            if line.startswith('- ') or line.startswith('â€¢ ') or line.startswith('* '):
                limitation = line[2:].strip()
                if limitation and len(limitation) > 10:
                    limitations.append(limitation)
            # åŒ¹é…æ•°å­—åˆ—è¡¨
            elif re.match(r'^\d+[\.\)]\s+', line):
                limitation = re.sub(r'^\d+[\.\)]\s+', '', line).strip()
                if limitation and len(limitation) > 10:
                    limitations.append(limitation)

        return limitations

    def _fallback_extraction(self, chunks: List[Dict]) -> List[str]:
        """
        é™çº§æå–ç­–ç•¥

        ğŸ†• æ”¹è¿›ç‰ˆæœ¬ï¼š
        1. æå–åŒ…å«å…³é”®è¯çš„å®Œæ•´å¥å­
        2. è¿‡æ»¤è¿‡é•¿/è¿‡çŸ­çš„å¥å­
        3. é™åˆ¶è¿”å›æ•°é‡
        """
        if not chunks:
            return []

        limitations = []
        # é™å®šå…³é”®è¯ï¼šç¡®ä¿æå–çš„æ˜¯limitationç›¸å…³å†…å®¹
        key_indicators = [
            'limitation', 'however', 'cannot', 'still',
            'unfortunately', 'lack', 'difficult', 'challenge',
            'remains', 'constrained', 'trade-off'
        ]

        for chunk in chunks[:4]:  # æ‰©å±•åˆ°å‰4ä¸ªchunks
            text = chunk['text'].strip()
            # ç®€å•æå–åŒ…å«å…³é”®è¯çš„å¥å­
            sentences = text.split('. ')
            for sentence in sentences:
                if any(kw in sentence.lower() for kw in key_indicators):
                    # è¿‡æ»¤é•¿åº¦
                    if 20 < len(sentence) < 300:
                        # æ¸…ç†å¥å­
                        cleaned = sentence.strip()
                        if cleaned and cleaned not in limitations:
                            limitations.append(cleaned)

        return limitations[:4]  # æœ€å¤šè¿”å›4æ¡

    def _is_llm_being_too_cautious(self, limitations: List[str], chunks: List[Dict]) -> bool:
        """
        ğŸ†• æ£€æµ‹LLMæ˜¯å¦è¿‡åº¦è°¨æ…è¿”å›"æœªæ‰¾åˆ°"

        åˆ¤æ–­é€»è¾‘ï¼ˆå‚è€ƒDeepPaper_Agentï¼‰:
        - å¦‚æœlimitationsä¸ºç©ºæˆ–åªæœ‰"æœªæ‰¾åˆ°"ç±»çš„å›ç­”
        - ä½†chunksæ•°é‡ >= 3 (è¯´æ˜æœ‰ç›¸å…³è¯æ®)
        - åˆ™è®¤ä¸ºLLMè¿‡åº¦è°¨æ…

        Args:
            limitations: LLMæå–çš„limitationsåˆ—è¡¨
            chunks: ç›¸å…³æ®µè½åˆ—è¡¨

        Returns:
            bool: Trueè¡¨ç¤ºLLMè¿‡åº¦è°¨æ…
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if not limitations or len(limitations) == 0:
            return len(chunks) >= 3

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¡ç›®éƒ½æ˜¯"æœªæ‰¾åˆ°"ç±»çš„å›ç­”
        empty_indicators = [
            "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯",
            "æœªæ‰¾åˆ°",
            "æ²¡æœ‰æ‰¾åˆ°",
            "æ— ç›¸å…³å†…å®¹",
            "not found",
            "no relevant",
            "no information",
            "no limitation",
            "no clear limitation"
        ]

        all_empty = all(
            any(indicator in lim.lower() for indicator in empty_indicators)
            for lim in limitations
        )

        if all_empty and len(chunks) >= 3:
            return True

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¡ç›®éƒ½å¤ªçŸ­ï¼ˆ<20å­—ç¬¦ï¼‰
        all_too_short = all(len(lim.strip()) < 20 for lim in limitations)
        if all_too_short and len(chunks) >= 2:
            return True

        return False

    def _extract_from_citations(self, paper_id: str) -> List[str]:
        """
        ä»å¼•ç”¨åˆ†æä¸­æå–å±€é™æ€§

        Args:
            paper_id: è®ºæ–‡IDï¼ˆArXiv IDæˆ–DOIï¼‰

        Returns:
            limitationsåˆ—è¡¨
        """
        if not self.citation_detective:
            return []

        try:
            # ä½¿ç”¨å¼•ç”¨ä¾¦æ¢
            result = self.citation_detective.analyze(
                paper_id=paper_id,
                top_k=10,
                min_citation_count=0,
                use_llm_analysis=False  # ä½¿ç”¨è§„åˆ™æå–å³å¯
            )

            return result.extracted_limitations

        except Exception as e:
            logger.warning(f"  âš ï¸ å¼•ç”¨åˆ†æå¤±è´¥: {e}")
            return []

    def _merge_results(
        self,
        section_limitations: List[str],
        section_evidence: List[Dict],
        citation_limitations: List[str]
    ) -> tuple:
        """
        åˆå¹¶ç« èŠ‚æå–å’Œå¼•ç”¨æå–çš„ç»“æœ

        Returns:
            (merged_content, merged_evidence)
        """
        # æ„å»ºæœ€ç»ˆå†…å®¹
        content_parts = []

        # ç¬¬ä¸€éƒ¨åˆ†ï¼šç« èŠ‚æå–
        if section_limitations:
            for limitation in section_limitations:
                content_parts.append(f"- {limitation}")

        # ç¬¬äºŒéƒ¨åˆ†ï¼šå¼•ç”¨æå–
        if citation_limitations:
            if section_limitations:  # å¦‚æœå‰é¢æœ‰å†…å®¹ï¼Œæ·»åŠ ç©ºè¡Œåˆ†éš”
                content_parts.append("")
            for limitation in citation_limitations[:3]:  # æœ€å¤šä¿ç•™3æ¡
                content_parts.append(f"- {limitation}")

        # å¦‚æœéƒ½ä¸ºç©º
        if not content_parts:
            return "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°", []

        merged_content = "\n".join(content_parts)

        # åˆå¹¶evidenceï¼ˆåªä¿ç•™ç« èŠ‚evidenceï¼‰
        merged_evidence = section_evidence

        return merged_content, merged_evidence


def main():
    """æµ‹è¯•ä»£ç """
    import argparse
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    parser = argparse.ArgumentParser(description="Limitation Extractor - å±€é™æ€§æå–")
    parser.add_argument("--config", required=True, help="LLMé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--paper", required=True, help="è®ºæ–‡æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰")
    parser.add_argument("--paper-id", help="è®ºæ–‡IDï¼ˆç”¨äºå¼•ç”¨åˆ†æï¼Œå¯é€‰ï¼‰")
    parser.add_argument("--use-citation", action="store_true", help="æ˜¯å¦ä½¿ç”¨å¼•ç”¨åˆ†æ")
    parser.add_argument("--output", default="limitation_results.json", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        config = LLMConfig.from_file(args.config)
        llm_client = LLMClient(config)

        # è¯»å–è®ºæ–‡æ–‡æ¡£
        with open(args.paper, 'r', encoding='utf-8') as f:
            paper_data = json.load(f)

        # è½¬æ¢ä¸ºPaperDocumentå¯¹è±¡
        from data_structures import PaperSection
        sections = [
            PaperSection(**section) for section in paper_data.get('sections', [])
        ]
        paper = PaperDocument(
            paper_id=paper_data.get('paper_id', 'unknown'),
            title=paper_data.get('title', ''),
            abstract=paper_data.get('abstract', ''),
            authors=paper_data.get('authors', []),
            year=paper_data.get('year'),
            sections=sections,
            metadata=paper_data.get('metadata')
        )

        # åˆ›å»ºæå–å™¨
        extractor = LimitationExtractor(
            llm_client=llm_client,
            use_citation_analysis=args.use_citation
        )

        # æ‰§è¡Œæå–
        result = extractor.extract(paper, paper_id=args.paper_id)

        # ä¿å­˜ç»“æœ
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)

        # æ‰“å°ç»“æœ
        print("\n" + "=" * 80)
        print("Limitation Extraction Results")
        print("=" * 80)
        print(f"\nContent:\n{result.content}")
        print(f"\nEvidence Count: {len(result.evidence)}")
        print(f"Confidence: {result.confidence:.2f}")
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
