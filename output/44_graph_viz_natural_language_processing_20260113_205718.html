
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>äº¤äº’å¼è®ºæ–‡å¼•ç”¨çŸ¥è¯†å›¾è°±</title>
            <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
            <style>
                body {
                    margin: 0;
                    padding: 0;
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                    background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
                    height: 100vh;
                    overflow: hidden;
                }
                .container {
                    display: flex;
                    width: 100%;
                    height: 100vh;
                    background: white;
                    overflow: hidden;
                }
                .graph-section {
                    width: 70%;
                    display: flex;
                    flex-direction: column;
                }
                .graph-container {
                    flex: 1;
                    padding: 10px;
                }
                .legend-container {
                    padding: 15px;
                    background: #f8f9fa;
                    border-top: 1px solid #dee2e6;
                    min-height: 180px;
                    max-height: 200px;
                }
                .details-section {
                    width: 30%;
                    background: #f8f9fa;
                    border-left: 1px solid #dee2e6;
                    display: flex;
                    flex-direction: column;
                }
                .details-header {
                    padding: 15px 20px;
                    background: #6c757d;
                    color: white;
                    font-weight: bold;
                    font-size: 18px;
                }
                .details-content {
                    padding: 20px;
                    flex: 1;
                    overflow-y: auto;
                }
                .paper-info {
                    margin-bottom: 15px;
                }
                .paper-info h3 {
                    color: #2c3e50;
                    margin: 0 0 10px 0;
                    font-size: 16px;
                    line-height: 1.4;
                }
                .paper-info p {
                    margin: 5px 0;
                    color: #5a5a5a;
                    font-size: 14px;
                }
                .legend-grid {
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                    gap: 10px;
                }
                .legend-item {
                    display: flex;
                    align-items: center;
                    padding: 8px;
                    background: white;
                    border-radius: 5px;
                    font-size: 12px;
                }
                .legend-color {
                    width: 20px;
                    height: 3px;
                    margin-right: 8px;
                    border-radius: 2px;
                }
                .stats {
                    background: #e9ecef;
                    padding: 15px;
                    border-radius: 8px;
                    margin-bottom: 15px;
                }
                .stat-item {
                    display: flex;
                    justify-content: space-between;
                    margin: 5px 0;
                    font-size: 14px;
                }
                .placeholder {
                    text-align: center;
                    color: #6c757d;
                    font-style: italic;
                    margin-top: 50px;
                }
                .title {
                    text-align: center;
                    color: #2c3e50;
                    margin-bottom: 20px;
                    font-size: 24px;
                    font-weight: bold;
                }
                /* æ ‡ç­¾é¡µæ ·å¼ */
                .tabs {
                    display: flex;
                    background: #dee2e6;
                    border-bottom: 2px solid #6c757d;
                }
                .tab {
                    flex: 1;
                    padding: 12px 10px;
                    text-align: center;
                    cursor: pointer;
                    border: none;
                    background: #dee2e6;
                    color: #495057;
                    font-size: 14px;
                    font-weight: 500;
                    transition: all 0.3s;
                }
                .tab:hover {
                    background: #c4c8cc;
                }
                .tab.active {
                    background: #6c757d;
                    color: white;
                }
                .tab-content {
                    display: none;
                    padding: 20px;
                    overflow-y: auto;
                    height: calc(100vh - 130px);
                }
                .tab-content.active {
                    display: block;
                }
                .epoch-card {
                    background: white;
                    border-left: 4px solid #3498DB;
                    padding: 15px;
                    margin-bottom: 15px;
                    border-radius: 5px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }
                .epoch-card h4 {
                    margin: 0 0 10px 0;
                    color: #2c3e50;
                    font-size: 15px;
                }
                .epoch-card p {
                    margin: 5px 0;
                    font-size: 13px;
                    color: #555;
                }
                .idea-card {
                    background: white;
                    border-left: 4px solid #2ECC71;
                    padding: 15px;
                    margin-bottom: 15px;
                    border-radius: 5px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }
                .idea-card h4 {
                    margin: 0 0 10px 0;
                    color: #2c3e50;
                    font-size: 15px;
                }
                .idea-card .status-badge {
                    display: inline-block;
                    padding: 3px 8px;
                    border-radius: 3px;
                    font-size: 11px;
                    font-weight: bold;
                    margin-left: 8px;
                }
                .status-success {
                    background: #d4edda;
                    color: #155724;
                }
                .status-incompatible {
                    background: #f8d7da;
                    color: #721c24;
                }
                .pivot-paper {
                    background: #fff3cd;
                    padding: 10px;
                    margin: 8px 0;
                    border-radius: 4px;
                    font-size: 12px;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <!-- å·¦ä¾§å›¾è°±éƒ¨åˆ† (70%) -->
                <div class="graph-section">
                    <div class="title">äº¤äº’å¼è®ºæ–‡å¼•ç”¨çŸ¥è¯†å›¾è°±</div>
                    <div class="graph-container">
                        <div id="graph" style="width:100%; height:100%;"></div>
                    </div>
                    <div class="legend-container">
                        <h4 style="margin-top:0; color:#2c3e50;">ğŸ”Œ Socket Matching å¼•ç”¨å…³ç³»ç±»å‹ï¼ˆ6ç§æ ¸å¿ƒç±»å‹ï¼‰</h4>
                        <div class="legend-grid">
                            <!-- Socket Matching æ ¸å¿ƒç±»å‹ï¼ˆ6ç§ï¼‰-->
                            <div class="legend-item" style="border-left: 3px solid #E74C3C;">
                                <div class="legend-color" style="background-color:#E74C3C; height:3px;"></div>
                                <span><strong>Overcomes</strong> - æ”»å…‹/ä¼˜åŒ–ï¼ˆçºµå‘æ·±åŒ–ï¼‰</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #9B59B6;">
                                <div class="legend-color" style="background-color:#9B59B6; height:3px;"></div>
                                <span><strong>Realizes</strong> - å®ç°æ„¿æ™¯ï¼ˆç§‘ç ”ä¼ æ‰¿ï¼‰</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #2ECC71;">
                                <div class="legend-color" style="background-color:#2ECC71; height:2px;"></div>
                                <span><strong>Extends</strong> - æ–¹æ³•æ‰©å±•ï¼ˆå¾®åˆ›æ–°ï¼‰</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #E67E22;">
                                <div class="legend-color" style="background-color:#E67E22; border: 2px dotted #E67E22; height:1px;"></div>
                                <span><strong>Alternative</strong> - å¦è¾Ÿè¹Šå¾„ï¼ˆé¢ è¦†åˆ›æ–°ï¼‰</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #3498DB;">
                                <div class="legend-color" style="background-color:#3498DB; border: 2px dashed #3498DB; height:1px;"></div>
                                <span><strong>Adapts_to</strong> - è¿ç§»/åº”ç”¨ï¼ˆæ¨ªå‘æ‰©æ•£ï¼‰</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #95A5A6;">
                                <div class="legend-color" style="background-color:#95A5A6; height:1px;"></div>
                                <span><strong>Baselines</strong> - åŸºçº¿å¯¹æ¯”ï¼ˆèƒŒæ™¯å™ªéŸ³ï¼‰</span>
                            </div>
                        </div>
                        <p style="margin-top:10px; font-size:11px; color:#666;">
                            ğŸ’¡ <strong>é€»è¾‘å¯¹æ¥çŸ©é˜µ (4ä¸ªMatch â†’ 6ç§ç±»å‹)</strong>: Match1(Limitationâ†’Problem) â†’ Overcomes | Match2(FutureWorkâ†’Problem) â†’ Realizes | Match3(Methodâ†’Method) â†’ Extends/Alternative | Match4(Problemè·¨åŸŸ) â†’ Adapts_to | æ— åŒ¹é… â†’ Baselines
                        </p>
                    </div>
                </div>

                <!-- å³ä¾§è¯¦æƒ…éƒ¨åˆ† (30%) -->
                <div class="details-section">
                    <!-- æ ‡ç­¾é¡µå¯¼èˆª -->
                    <div class="tabs">
                        <div class="tab active" onclick="switchTab(event, 'paper-tab')">ğŸ“„ è®ºæ–‡è¯¦æƒ…</div>
                        <div class="tab" onclick="switchTab(event, 'survey-tab')">ğŸ“ æ·±åº¦ç»¼è¿°</div>
                        <div class="tab" onclick="switchTab(event, 'ideas-tab')">ğŸ’¡ ç§‘ç ”åˆ›æ„</div>
                    </div>

                    <!-- è®ºæ–‡è¯¦æƒ…æ ‡ç­¾é¡µ -->
                    <div id="paper-tab" class="tab-content active">
                        <div class="stats">
                            <h4 style="margin-top:0;">å›¾è°±ç»Ÿè®¡</h4>
                            <div class="stat-item">
                                <span>è®ºæ–‡æ€»æ•°:</span>
                                <span>44</span>
                            </div>
                            <div class="stat-item">
                                <span>å¼•ç”¨å…³ç³»:</span>
                                <span>50</span>
                            </div>
                            <div class="stat-item">
                                <span>æ—¶é—´è·¨åº¦:</span>
                                <span>1972 - 2024</span>
                            </div>
                        </div>
                        <div class="placeholder">
                            ğŸ‘† ç‚¹å‡»å›¾è°±ä¸­çš„èŠ‚ç‚¹æŸ¥çœ‹è®ºæ–‡è¯¦ç»†ä¿¡æ¯
                        </div>
                    </div>

                    <!-- æ·±åº¦ç»¼è¿°æ ‡ç­¾é¡µ -->
                    <div id="survey-tab" class="tab-content"></div>

                    <!-- ç§‘ç ”åˆ›æ„æ ‡ç­¾é¡µ -->
                    <div id="ideas-tab" class="tab-content"></div>
                </div>
            </div>

            <script>
                // ========== æ•°æ®åˆå§‹åŒ– ==========
                const nodesData = [
  {
    "id": "W4205802268",
    "x": 2022.461989661177,
    "y": -4.24961909167469,
    "title": "The Routledge Handbook of Translation and Methodology",
    "authors": [
      "Federico Zanettin",
      "Christopher Rundle"
    ],
    "first_author": "Federico Zanettin",
    "first_author_surname": "Zanettin",
    "year": 2022,
    "cited_by_count": 41,
    "venue": "",
    "size": 19.30758544727753,
    "color": "hsl(14, 70%, 60%)",
    "label": "Zanettin ,2022",
    "rag_problem": "Interpreter education often struggles to balance practice-oriented training with the acquisition of academic knowledge and the ability to theorize effectively.",
    "rag_method": "The authors propose a model for interpreter education that integrates practice orientation with academic knowledge acquisition and theoretical reasoning.\n\n**Explanation:** By designing an educational approach that combines practical skills training with academic components, the model ensures interpreters develop both operational competence and a deeper understanding of theoretical foundations. This dual focus prepares interpreters not only for real-world challenges but also equips them to critically analyze and improve their practice.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Explore solutions to problematic aspects in interpreter education, as highlighted by the brief critical discussion in the paper, to enhance teaching methodologies.\n- Conduct further research on the two relevant issues identified in interpreter education to better address challenges in practice-orientation and theoretical preparedness.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4360845368",
    "x": 2022.5499669074018,
    "y": -3.3332164728087355,
    "title": "Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Training",
    "authors": [
      "Laura RamÃ­rez-Polo",
      "Chelo Vargas-Sierra"
    ],
    "first_author": "Laura RamÃ­rez-Polo",
    "first_author_surname": "RamÃ­rez-Polo",
    "year": 2023,
    "cited_by_count": 24,
    "venue": "",
    "size": 16.459730687547882,
    "color": "hsl(7, 70%, 60%)",
    "label": "RamÃ­rez-Polo ,2023",
    "rag_problem": "The integration of technology and artificial intelligence in translation practices raises ethical concerns, but these concerns are not adequately addressed in current translator training programs or pedagogical models.",
    "rag_method": "The authors propose incorporating ethical competence into translator training programs by designing pedagogical models that explicitly address ethical frameworks alongside technological skill development.\n\n**Explanation:** By integrating ethical competence into translator training, students will learn to critically evaluate the ethical implications of technology use in their practice. This approach ensures that translators are equipped not only with technical proficiency but also with the ability to make informed, ethically sound decisions in situations involving technology and artificial intelligence. Addressing ethical concerns in training programs directly mitigates the risks posed by unconsidered technology use.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate how ethical concerns related to translation technologies are being integrated into current pedagogical models and teaching programs for translator training. This could involve assessing various curricula to determine if and how these concerns are addressed.\n- Explore the development of new teaching strategies and materials that consider the ethical implications of using artificial intelligence systems in translation. This may involve creating specific modules or workshops focused on ethical competence.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4317823603",
    "x": 2022.1807518337234,
    "y": 3.5404844411696583,
    "title": "Natural Language Processing for Policymaking",
    "authors": [
      "Zhijing Jin",
      "Rada Mihalcea"
    ],
    "first_author": "Zhijing Jin",
    "first_author_surname": "Jin",
    "year": 2022,
    "cited_by_count": 12,
    "venue": "",
    "size": 12.870081724213135,
    "color": "hsl(14, 70%, 60%)",
    "label": "Jin ,2022",
    "rag_problem": "Policymakers face challenges in analyzing large volumes of unstructured text data, such as public comments, legislation drafts, and policy-related documents, which hinder efficient decision-making.",
    "rag_method": "The application of Natural Language Processing (NLP) techniques to process, categorize, and extract insights from unstructured text data relevant to policymaking.\n\n**Explanation:** NLP techniques are designed to handle and interpret large-scale text data by converting unstructured information into structured formats. This enables policymakers to efficiently identify trends, summarize complex documents, and extract actionable insights. For instance, NLP models can automate tasks such as sentiment analysis from public comments, topic clustering in legislative drafts, and keyword extraction from policy discussions, thereby reducing manual workload and improving decision-making accuracy.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2041578073",
    "x": 2013.9730769270816,
    "y": -4.405441125819364,
    "title": "The design and evaluation of a Statistical Machine Translation syllabus for translation students",
    "authors": [
      "Stephen Doherty",
      "Dorothy Kenny"
    ],
    "first_author": "Stephen Doherty",
    "first_author_surname": "Doherty",
    "year": 2014,
    "cited_by_count": 117,
    "venue": "",
    "size": 24.978194347322994,
    "color": "hsl(71, 70%, 60%)",
    "label": "Doherty ,2014",
    "rag_problem": "Translation studies programs fail to integrate Statistical Machine Translation (SMT) effectively into their curricula, despite its growing importance in the field.",
    "rag_method": "The authors propose a holistic Statistical Machine Translation (SMT) syllabus designed specifically for translation students, prioritizing the integration of SMT into translation education.\n\n**Explanation:** The proposed syllabus addresses the problem by systematically teaching SMT concepts, tools, and applications to translation students, ensuring they understand how to use SMT technology in translation processes. By creating a dedicated syllabus, the program ensures students gain hands-on experience and theoretical knowledge, equipping them to work effectively with SMT in professional contexts.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Explore how SMT can be effectively integrated into diverse translation studies curricula, addressing the gap identified in incorporating Statistical Machine Translation holistically.\n- Develop and test new instructional methods or tools to better align SMT training with the capabilities and needs of translation students.\n- Investigate the impact of SMT-focused training on the professional development and employability of translation students in real-world scenarios.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2165612380",
    "x": 1974.8951465639534,
    "y": -4.495538389987943,
    "title": "A vector space model for automatic indexing",
    "authors": [
      "Gerard Salton",
      "Anita M.-Y. Wong",
      "Chulâ€Su Yang"
    ],
    "first_author": "Gerard Salton",
    "first_author_surname": "Salton",
    "year": 1975,
    "cited_by_count": 7329,
    "venue": "",
    "size": 60,
    "color": "hsl(240, 70%, 60%)",
    "label": "Salton ,1975",
    "rag_problem": "In document retrieval or pattern matching environments, stored entities (e.g., documents) are not optimally spaced in the indexing property space, resulting in retrieval inefficiency due to overlapping or densely packed entities in the object space.",
    "rag_method": "The authors propose a vector space model for automatic indexing, where each entity (document or pattern) is positioned in a high-dimensional space to maximize separation between entities.\n\n**Explanation:** By positioning entities as far apart as possible in a vector space model, the indexing system reduces space density and minimizes overlap between entities. This spatial separation enables the retrieval system to distinguish between documents more effectively, improving pattern matching and retrieval accuracy. The model uses mathematical properties of the vector space to optimize entity positioning, ensuring that each entity has a unique representation that correlates inversely with space density.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Explore methods to optimize the density of the indexing space to improve retrieval performance, as the paper suggests a potential inverse correlation between space density and retrieval success. Future work could involve testing alternative space configurations or algorithms that minimize density while enhancing indexing accuracy.\n- Investigate the practical implementation of the proposed vector space model in various real-world document retrieval scenarios, such as large-scale databases or dynamic search environments, to evaluate its scalability and effectiveness under diverse conditions.\n- Conduct further experiments to analyze how different measures or metrics of \"space density\" impact the retrieval accuracy, as the paper hints at a need to express retrieval performance as a mathematical function of this parameter clearly.\n- Develop advanced techniques for automatically adapting the indexing model to changing data distributions, as the existing work focuses on a static environment, and dynamic optimization could improve usability in evolving datasets.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2118020653",
    "x": 2001.7244140467408,
    "y": -0.19457827538451988,
    "title": "Machine learning in automated text categorization",
    "authors": [
      "Fabrizio Sebastiani"
    ],
    "first_author": "Fabrizio Sebastiani",
    "first_author_surname": "Sebastiani",
    "year": 2002,
    "cited_by_count": 7820,
    "venue": "",
    "size": 60,
    "color": "hsl(155, 70%, 60%)",
    "label": "Sebastiani ,2002",
    "rag_problem": "The need to efficiently categorize large volumes of texts into predefined categories as digital document availability grows, which traditional manual methods cannot handle.",
    "rag_method": "Using machine learning techniques to automatically build classifiers by learning from preclassified documents.\n\n**Explanation:** Machine learning algorithms streamline text categorization by employing an inductive process to extract patterns and category characteristics from preclassified text data. This eliminates the need for manual intervention and allows the system to handle the scalability challenges introduced by the increasing volume of digital content. As classifiers are automatically trained on labeled examples, they can generalize to new, unseen documents, fulfilling the need for efficient text categorization.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W168564468",
    "x": 2010.0571236460805,
    "y": -4.855967352034188,
    "title": "Software Framework for Topic Modelling with Large Corpora",
    "authors": [
      "Radim Å˜ehÅ¯Å™ek",
      "Petr Sojka"
    ],
    "first_author": "Radim Å˜ehÅ¯Å™ek",
    "first_author_surname": "Å˜ehÅ¯Å™ek",
    "year": 2010,
    "cited_by_count": 3795,
    "venue": "",
    "size": 60,
    "color": "hsl(99, 70%, 60%)",
    "label": "Å˜ehÅ¯Å™ek ,2010",
    "rag_problem": "Existing implementations of popular topic modelling algorithms struggle with scalability when processing large corpora due to memory limitations.",
    "rag_method": "A Natural Language Processing software framework based on document streaming, which processes corpora document by document in a memory-independent fashion.\n\n**Explanation:** The document streaming framework minimizes the memory usage by processing each document individually instead of loading the entire corpus into memory. This approach allows the system to handle large corpora without being constrained by the memory capacity of the machine, thereby improving scalability and usability.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Explore methods to enhance the scalability of the framework to handle even larger corpora efficiently, addressing memory and computational constraints more comprehensively.\n- Investigate the integration of additional popular algorithms into the framework to expand its applicability and flexibility in topic modeling tasks.\n- Develop tools for better user interaction and ease of use, aiming to simplify the deployment of the framework for non-expert users.\n- Consider incorporating advanced techniques like deep learning or hybrid models to improve the quality and accuracy of topic modeling results.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W1662133657",
    "x": 2009.656490187967,
    "y": -1.6645211143681848,
    "title": "From Frequency to Meaning: Vector Space Models of Semantics",
    "authors": [
      "Peter D. Turney",
      "Patrick Pantel"
    ],
    "first_author": "Peter D. Turney",
    "first_author_surname": "Turney",
    "year": 2010,
    "cited_by_count": 2831,
    "venue": "",
    "size": 58.33262903629804,
    "color": "hsl(99, 70%, 60%)",
    "label": "Turney ,2010",
    "rag_problem": "Computers have a very limited ability to understand the meaning of human language, which restricts tasks such as giving instructions to computers, enabling computers to explain their actions, and effective text analysis and processing.",
    "rag_method": "The introduction and use of Vector Space Models (VSMs) for semantic processing of text, which encode word semantics as mathematical structures in vector spaces.\n\n**Explanation:** VSMs address the issue by representing words or phrases as vectors in a multi-dimensional space based on their distributional properties in language. This mathematical encoding allows computers to capture and reason about semantic similarity, relationships, and implicit meanings in text, enabling tasks like following instructions, explaining actions, and processing text more effectively. By shifting the problem of language understanding into a measurable vector space, VSMs provide a scalable and interpretable framework for semantic representation.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2045108252",
    "x": 2002.4086969565847,
    "y": -0.4895327274232032,
    "title": "Visualizing knowledge domains",
    "authors": [
      "Katy BÃ¶rner",
      "Chaomei Chen",
      "Kevin W. Boyack"
    ],
    "first_author": "Katy BÃ¶rner",
    "first_author_surname": "BÃ¶rner",
    "year": 2003,
    "cited_by_count": 1736,
    "venue": "",
    "size": 49.67549081893524,
    "color": "hsl(148, 70%, 60%)",
    "label": "BÃ¶rner ,2003",
    "rag_problem": "The challenge of effectively representing and understanding complex knowledge domains for analysis and visualization.",
    "rag_method": "Developing a method or system for visualizing knowledge domains that organizes and represents relationships between concepts effectively.\n\n**Explanation:** By creating a visualization method specifically tailored for knowledge domains, the relationships, hierarchies, and organization of information can be rendered in a way that highlights important patterns, trends, or connections. This enables better comprehension, analysis, and communication of the underlying structure of the domain.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2301363727",
    "x": 2004.5218939022157,
    "y": 0.34785656762022976,
    "title": "EXPERT SYSTEMS WITH APPLICATIONS",
    "authors": [
      "Short Communication",
      "Been-chian Chien A",
      "Jung Yi Lin A"
    ],
    "first_author": "Short Communication",
    "first_author_surname": "Communication",
    "year": 2004,
    "cited_by_count": 1660,
    "venue": "",
    "size": 49.368499761762415,
    "color": "hsl(141, 70%, 60%)",
    "label": "Communication ,2004",
    "rag_problem": "æœªæ‰¾åˆ°æ˜ç¡®çš„ç ”ç©¶é—®é¢˜æè¿°",
    "rag_method": "æœªæ‰¾åˆ°æ˜ç¡®çš„æ–¹æ³•æè¿°",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2075006521",
    "x": 1973.3343838043643,
    "y": -4.031911337894542,
    "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
    "authors": [
      "G. Salton",
      "Chulâ€Su Yang"
    ],
    "first_author": "G. Salton",
    "first_author_surname": "Salton",
    "year": 1973,
    "cited_by_count": 571,
    "venue": "",
    "size": 42.0536569670707,
    "color": "hsl(240, 70%, 60%)",
    "label": "Salton ,1973",
    "rag_problem": "Standard theories for specifying term values (weights) in automatic indexing are inadequate for effectively assigning weights to index terms in diverse document collections.",
    "rag_method": "New techniques are proposed for assigning weights to index terms, tailored to the specific characteristics of individual document collections.\n\n**Explanation:** The proposed techniques address the inadequacy of standard theories by focusing on the unique properties of each document collection, allowing for a more precise and context-relevant weighting of index terms. This improves the indexing process by ensuring that term weights are more representative of their significance within the specific dataset, ultimately enhancing retrieval effectiveness.",
    "rag_limitation": "- The proposed methods rely heavily on the characteristics of individual document collections, which may limit their generalizability to other collections with significantly different characteristics.\n- The evaluation of the methods' effectiveness is limited to certain scenarios, meaning their performance in broader or more diverse indexing contexts is not fully explored.",
    "rag_future_work": "- Investigating additional techniques for assigning weights: Future work could focus on exploring alternative methods for specifying term weights that address the inadequacies identified in current theories.\n- Customizing indexing methods for different document collections: Further research may involve developing adaptive approaches that tailor term value assignments more effectively to the specific characteristics of diverse document collections.\n- Evaluating the effectiveness of broader weighting schemes: An expanded evaluation of various proposed weighting methods across a wider range of document sets could be conducted to determine their generalizability.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2144211451",
    "x": 1972.304248683457,
    "y": 0.14320141654570917,
    "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
    "authors": [
      "Karen SpÃ¤rck Jones"
    ],
    "first_author": "Karen SpÃ¤rck Jones",
    "first_author_surname": "Jones",
    "year": 1972,
    "cited_by_count": 4313,
    "venue": "",
    "size": 60,
    "color": "hsl(240, 70%, 60%)",
    "label": "Jones ,1972",
    "rag_problem": "Term specificity in information retrieval is typically interpreted based on the meaning of terms rather than their statistical use, leading to inefficiencies in retrieval systems.",
    "rag_method": "Propose a statistical interpretation of term specificity, where specificity is based on the frequency of term use rather than its semantic meaning.\n\n**Explanation:** By defining term specificity statistically, the retrieval system can prioritize terms based on their actual frequency patterns across documents. This approach ensures that frequently-used terms, which contribute significantly to retrieval performance, are appropriately weighted and incorporated into the system. This avoids the limitations of semantic-based approaches that may overlook statistically important terms, enhancing overall retrieval performance.",
    "rag_limitation": "- The method assumes that term specificity can be solely interpreted statistically through term usage, which may oversimplify the complexities of term meaning and context in retrieval tasks.\n- The approach emphasizes the importance of frequently-occurring terms for good overall performance, which could limit its effectiveness in scenarios where rare but significant terms are critical for accurate retrieval.",
    "rag_future_work": "- Explore statistical models that further refine the measurement of term specificity based on usage patterns, potentially improving the accuracy of term weighting in retrieval systems.\n- Investigate the impact of incorporating domain-specific frequently occurring terms into retrieval systems, as the experiment results indicate their importance for enhanced performance.\n- Develop methods to dynamically adjust term specificity weights in real-time retrieval scenarios to accommodate various types and sizes of test collections.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W1908696901",
    "x": 1975.4101859412747,
    "y": 3.886062099473424,
    "title": "A Theory of Indexing",
    "authors": [
      "Gerard Salton"
    ],
    "first_author": "Gerard Salton",
    "first_author_surname": "Salton",
    "year": 1975,
    "cited_by_count": 120,
    "venue": "",
    "size": 25.11601021539626,
    "color": "hsl(240, 70%, 60%)",
    "label": "Salton ,1975",
    "rag_problem": "æœªæ‰¾åˆ°æ˜ç¡®çš„ç ”ç©¶é—®é¢˜æè¿°",
    "rag_method": "æœªæ‰¾åˆ°æ˜ç¡®çš„æ–¹æ³•æè¿°",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2568360633",
    "x": 1973.3708915182237,
    "y": 3.8242168238738747,
    "title": "Contribution to the Theory of Indexing",
    "authors": [
      "Gerard Salton",
      "Chulâ€Su Yang",
      "C. Yu"
    ],
    "first_author": "Gerard Salton",
    "first_author_surname": "Salton",
    "year": 1973,
    "cited_by_count": 17,
    "venue": "",
    "size": 14.65644799104848,
    "color": "hsl(240, 70%, 60%)",
    "label": "Salton ,1973",
    "rag_problem": "The usefulness of terms in documents is difficult to determine because terms with very high or very low frequency are less effective for indexing, and there is a lack of a systematic method to identify optimal indexing terms.",
    "rag_method": "Characterize the usefulness of terms based on their frequency characteristics, identifying medium-frequency terms with skewed distributions as the most effective for indexing.\n\n**Explanation:** The solution proposes using term frequency characteristics to systematically evaluate term usefulness. By focusing on medium-frequency terms with skewed distributions, the mechanism avoids over-represented (high-frequency) and under-represented (low-frequency) terms, which are less reliable for effective document retrieval. Medium-frequency, skewed terms have better discriminatory power across documents, improving indexing quality.",
    "rag_limitation": "- The method relies on terms with medium frequency and skewed distributions, which may limit its effectiveness in document collections where such term distributions are uncommon.\n- Terms with very high or very low frequency are deemed less useful, potentially resulting in the exclusion of rare but contextually significant terms from the indexing vocabulary.",
    "rag_future_work": "- Investigate methods to optimize and enhance the grouping of low-frequency terms to improve the indexing vocabulary, as indicated by the potential utility of addressing terms with skewed frequency distributions.\n- Explore strategies for incorporating terms with medium frequency and skewed distributions in diverse document collections to maximize their usefulness for indexing.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2914159168",
    "x": 2019.338304885046,
    "y": -4.187046420582407,
    "title": "The impact of using machine translation on EFL studentsâ€™ writing",
    "authors": [
      "Sangminâ€Michelle Lee"
    ],
    "first_author": "Sangminâ€Michelle Lee",
    "first_author_surname": "Lee",
    "year": 2019,
    "cited_by_count": 295,
    "venue": "",
    "size": 30.026636281014035,
    "color": "hsl(35, 70%, 60%)",
    "label": "Lee ,2019",
    "rag_problem": "Limited understanding of how machine translation (MT) can be effectively used as a pedagogical tool in EFL classrooms, particularly for improving students' writing skills.",
    "rag_method": "The study explores the role of machine translation as a CALL tool (Computer-Assisted Language Learning) in EFL writing tasks through a specific instructional design and analysis framework.\n\n**Explanation:** Machine translation is analyzed not only as a means for students to post-edit translations but as a broader pedagogical tool to directly influence and assist EFL students' writing processes. By employing a structured exploration of MT's application inside the classroom, the study aims to fill the gap in pedagogical understanding and provide insights into how MT can support the development of writing skills in a systematic educational context.",
    "rag_limitation": "- The study provides limited insights regarding MT as a pedagogical tool specifically within EFL classrooms, as broader applications or variations in educational contexts are not thoroughly addressed.\n- The research design does not focus on student postediting of MT output, which might restrict the understanding of how MT can facilitate language learning through active correction processes.",
    "rag_future_work": "- Investigate the pedagogical implications of using machine translation (MT) as a tool in EFL classrooms. Since the study highlights the limited research in this area, future work could explore how MT can effectively integrate into language teaching methodologies.\n- Examine other uses of MT in L2 learning beyond postediting. As most prior studies have focused on student postediting, future research could explore alternative uses or innovative designs to leverage MT for language development more comprehensively.\n- Conduct longitudinal studies on the impact of MT on EFL writing. This would help understand the long-term effects and how consistent usage of MT influences language acquisition over extended periods.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4211028722",
    "x": 2017.5667950333232,
    "y": 0.4306393579782326,
    "title": "Crowdsourcing and online collaborative translations",
    "authors": [
      "Miguel A. JimÃ©nez-Crespo"
    ],
    "first_author": "Miguel A. JimÃ©nez-Crespo",
    "first_author_surname": "JimÃ©nez-Crespo",
    "year": 2017,
    "cited_by_count": 162,
    "venue": "",
    "size": 26.75162311799359,
    "color": "hsl(49, 70%, 60%)",
    "label": "JimÃ©nez-Crespo ,2017",
    "rag_problem": "Traditional translation theories and practices struggle to adapt to the dynamic, decentralized, and technologically-driven processes introduced by crowdsourcing and online collaborative translations.",
    "rag_method": "The study proposes examining crowdsourcing and online collaborative translations as transformative phenomena to redefine existing translation theories and better align them with modern translational practices.\n\n**Explanation:** By analyzing crowdsourcing and online collaborative translation practices, the study aims to address the disconnect between traditional translation theories and modern processes. This reframing helps expand theoretical frameworks to include decentralized, community-driven, and technology-based translation methods, thus making the theories more relevant to current trends and public expectations.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate how crowdsourcing and online collaborative translations can contribute to reframing existing translation theories, potentially redefining core principles within Translation Studies.\n- Explore the implications of the â€œtechnological turnâ€ for translation practices, including its effects on the evolving role of translators and their methodologies.\n- Study the impact of crowdsourcing initiatives on public perceptions of translation, especially in terms of quality, accessibility, and the social value of translation work.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2517695692",
    "x": 2015.5004664288654,
    "y": -0.24275343480567713,
    "title": "The impact of translation technologies on the process and product of translation",
    "authors": [
      "Stephen Doherty"
    ],
    "first_author": "Stephen Doherty",
    "first_author_surname": "Doherty",
    "year": 2016,
    "cited_by_count": 140,
    "venue": "",
    "size": 25.955716630850407,
    "color": "hsl(56, 70%, 60%)",
    "label": "Doherty ,2016",
    "rag_problem": "Traditional translation methods are time-consuming and prone to inconsistencies, making it difficult to ensure productivity and quality in interlingual communication.",
    "rag_method": "Computer-assisted translation tools provide functionalities like translation memory and terminology management to streamline translation processes.\n\n**Explanation:** These tools enhance the consistency and efficiency of translations by allowing translators to reuse previously translated segments and ensure adherence to standardized terminologies, reducing the time required for manual adjustments and quality assurance.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate the integration of emerging translation technologies: Future work could explore how upcoming advancements in artificial intelligence and machine learning can further enhance the effectiveness of translation tools and processes.\n- Analyze the long-term impact of translation technologies on linguistic diversity: Research could focus on whether these tools contribute to preserving lesser-used languages or inadvertently favor dominant languages.\n- Develop more user-friendly translation interfaces: Efforts could be directed towards designing more intuitive and accessible tools for non-expert users, enabling broader adoption.\n- Study the balance between human creativity and machine efficiency: Further research could examine how to maintain the creative aspects of translation while leveraging technology for productivity.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2911800777",
    "x": 2019.524362417826,
    "y": 4.220388691851232,
    "title": "Modeling the intention to use machine translation for student translators: An extension of Technology Acceptance Model",
    "authors": [
      "Yanxia Yang",
      "Xiangling Wang"
    ],
    "first_author": "Yanxia Yang",
    "first_author_surname": "Yang",
    "year": 2019,
    "cited_by_count": 135,
    "venue": "",
    "size": 25.757522645831344,
    "color": "hsl(35, 70%, 60%)",
    "label": "Yang ,2019",
    "rag_problem": "Student translators often face challenges in adopting machine translation tools due to a lack of understanding of factors influencing their intention to use these technologies.",
    "rag_method": "The authors extend the Technology Acceptance Model (TAM) to specifically analyze and model student translators' intention to use machine translation tools.\n\n**Explanation:** By adapting the Technology Acceptance Model (TAM), the authors incorporate factors such as perceived usefulness, perceived ease of use, and other relevant constructs tailored to the context of student translators. This extended framework provides insights into the key determinants influencing their intention to use machine translation tools, helping educators and developers better address adoption barriers.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2025173440",
    "x": 2014.6268171189768,
    "y": -0.314100296829776,
    "title": "A survey of machine translation competences: Insights for translation technology educators and practitioners",
    "authors": [
      "Federico Gaspari",
      "Hala Almaghout",
      "Stephen Doherty"
    ],
    "first_author": "Federico Gaspari",
    "first_author_surname": "Gaspari",
    "year": 2015,
    "cited_by_count": 134,
    "venue": "",
    "size": 25.717010453970875,
    "color": "hsl(64, 70%, 60%)",
    "label": "Gaspari ,2015",
    "rag_problem": "The translation and localization industry lacks a clear understanding of the required competencies for effectively utilizing machine translation (MT) technologies.",
    "rag_method": "The authors conducted a large-scale survey involving 438 validated respondents, including diverse stakeholders (freelance translators, language service providers, translator trainers, and academics) to identify and map MT-related competencies.\n\n**Explanation:** By surveying a broad range of stakeholders, the authors collected data on real-world MT use and the required competencies, providing insights into what skills and knowledge are necessary for effective MT usage. This targeted approach directly addresses the knowledge gap by synthesizing data into actionable insights for educators and practitioners to better prepare professionals in the industry.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate approaches to better integrate machine translation (MT) competencies into the curricula of translator training programs, ensuring alignment with the evolving demands of the translation and localization industry.\n- Conduct further studies to explore the specific needs and challenges of different stakeholder groups (e.g., freelance translators, language service providers, translator trainers, and academics) in adopting and utilizing MT technologies effectively.\n- Explore advancements in translation technology tools and assess their impact on the skill requirements for translators, aiming to guide the development of both training and practical applications.\n- Address the limitations in survey design and sampling to refine future data collection approaches for a more comprehensive understanding of MT competencies.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3175985315",
    "x": 2009.4209071526122,
    "y": 4.68263473881454,
    "title": "Toward a Model of Active and Situated Learning in the Teaching of Computer-Aided Translation: Introducing the CERTT Project",
    "authors": [
      "Lynne Bowker",
      "Elizabeth Marshman"
    ],
    "first_author": "Lynne Bowker",
    "first_author_surname": "Bowker",
    "year": 2010,
    "cited_by_count": 17,
    "venue": "",
    "size": 14.65644799104848,
    "color": "hsl(99, 70%, 60%)",
    "label": "Bowker ,2010",
    "rag_problem": "Translator education programs face challenges in preparing graduates who are proficient with modern translation tools due to limited time and resources.",
    "rag_method": "The CERTT Project proposes an active and situated learning model that integrates translation tool usage across different elements of the curriculum, aiming for a holistic educational approach.\n\n**Explanation:** By adopting an active and situated learning model, the CERTT Project ensures that students experience translation tools as part of their real-world translation practice, bridging the gap between theoretical instruction and practical application. This approach utilizes holistic methods to embed tool usage throughout the program, which makes efficient use of limited teaching resources while ensuring graduates are adequately prepared for industry demands.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate strategies to better integrate computer-aided translation tools across the entire translator education program, ensuring a holistic approach to tool use in various core and peripheral elements of the curriculum.\n- Explore methods to address time and resource constraints in translator education, potentially by designing more efficient pedagogical techniques for incorporating translation technologies.\n- Further develop the CERTT project to refine active and situated learning models that align with the evolving demands of translation technology and industry needs.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2126512988",
    "x": 1995.4470493029771,
    "y": 0.17332023974369637,
    "title": "Computer Self-Efficacy: Development of a Measure and Initial Test",
    "authors": [
      "Deborah Compeau",
      "Christopher A. Higgins"
    ],
    "first_author": "Deborah Compeau",
    "first_author_surname": "Compeau",
    "year": 1995,
    "cited_by_count": 6058,
    "venue": "",
    "size": 60,
    "color": "hsl(205, 70%, 60%)",
    "label": "Compeau ,1995",
    "rag_problem": "There is a lack of a validated measure to assess individualsâ€™ beliefs about their ability to competently use computers (computer self-efficacy), which is necessary to understand its impact on computer use behavior.",
    "rag_method": "The authors developed and validated a measure specifically designed to assess computer self-efficacy through a survey of Canadian managers and professionals.\n\n**Explanation:** By creating a validated measure for computer self-efficacy, the authors provide a reliable tool to quantitatively assess individualsâ€™ beliefs about their computer-related abilities. This measurement enables researchers and practitioners to systematically examine how these beliefs influence computer use, outcome expectations, and emotional reactions. The validation process ensures the measure's accuracy and applicability to real-world organizational contexts.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Explore the application of computer self-efficacy across diverse roles and industries beyond Canadian managers and professionals, to broaden the understanding of its impacts in varied contexts.\n- Investigate additional antecedents and external factors influencing computer self-efficacy to develop a more comprehensive framework for its measurement and effects.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2168353148",
    "x": 2009.9365634317787,
    "y": 1.7625448933077728,
    "title": "Studies of expansive learning: Foundations, findings and future challenges",
    "authors": [
      "YrjÃ¶ EngestrÃ¶m",
      "Annalisa Sannino"
    ],
    "first_author": "YrjÃ¶ EngestrÃ¶m",
    "first_author_surname": "EngestrÃ¶m",
    "year": 2010,
    "cited_by_count": 1554,
    "venue": "",
    "size": 48.91600845332309,
    "color": "hsl(99, 70%, 60%)",
    "label": "EngestrÃ¶m ,2010",
    "rag_problem": "Traditional learning theories often focus narrowly on individual cognition and fail to adequately address the dynamics of collective learning processes occurring in complex, collaborative environments.",
    "rag_method": "The concept of expansive learning, which emphasizes collective transformation processes occurring when groups tackle systemic contradictions and collaboratively generate new knowledge or practices.\n\n**Explanation:** Expansive learning addresses this issue by shifting the focus from individual cognition to shared, collective efforts aimed at resolving contradictions within an activity system. By emphasizing the collaborative development of solutions and innovations, expansive learning provides a framework for understanding and enhancing how groups engage in transformative learning processes within their environments.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2039483660",
    "x": 1994.215240792668,
    "y": 0.3999583151261171,
    "title": "Enhancing self-efficacy for computer technologies through the use of positive classroom experiences",
    "authors": [
      "Peggy A. Ertmer",
      "Elizabeth Evenbeck",
      "Katherine S. Cennamo"
    ],
    "first_author": "Peggy A. Ertmer",
    "first_author_surname": "Ertmer",
    "year": 1994,
    "cited_by_count": 103,
    "venue": "",
    "size": 24.284920465958436,
    "color": "hsl(212, 70%, 60%)",
    "label": "Ertmer ,1994",
    "rag_problem": "Students often struggle with low self-efficacy when engaging with computer technologies, hindering their ability to learn and adapt to technological advancements.",
    "rag_method": "Creating positive classroom experiences tailored to enhancing self-efficacy for computer technologies.\n\n**Explanation:** The solution leverages structured positive interactions and experiences within the classroom to boost students' belief in their ability to use computer technologies effectively. By designing these experiences intentionally, students are encouraged through supportive pedagogy, interactive learning, and low-stress challenges, which help build confidence and competence progressively. Positive reinforcement and contextual learning further ensure sustained self-efficacy growth.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W1986845215",
    "x": 2011.57761057715,
    "y": -4.405230318954405,
    "title": "Translating by post-editing: is it the way forward?",
    "authors": [
      "Ignacio GonzÃ¡lez GarcÃ­a"
    ],
    "first_author": "Ignacio GonzÃ¡lez GarcÃ­a",
    "first_author_surname": "GarcÃ­a",
    "year": 2011,
    "cited_by_count": 107,
    "venue": "",
    "size": 24.492091357445375,
    "color": "hsl(92, 70%, 60%)",
    "label": "GarcÃ­a ,2011",
    "rag_problem": "Traditional human translation is time-intensive and expensive, while machine translation often results in low-quality outputs that require significant effort to correct.",
    "rag_method": "Post-editing machine translation (PEMT), where human translators revise machine-generated translations to enhance quality.\n\n**Explanation:** The post-editing mechanism leverages the speed and cost-effectiveness of machine translation while improving the accuracy and linguistic quality through human intervention. This approach reduces the overall time and cost compared to traditional human translation and ensures better quality than raw machine translation outputs.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4381332452",
    "x": 2023.121921881119,
    "y": -6.537057093129635,
    "title": "Generative artificial intelligence in the metaverse era",
    "authors": [
      "Zhihan Lv"
    ],
    "first_author": "Zhihan Lv",
    "first_author_surname": "Lv",
    "year": 2023,
    "cited_by_count": 261,
    "venue": "",
    "size": 29.356850207762307,
    "color": "hsl(7, 70%, 60%)",
    "label": "Lv ,2023",
    "rag_problem": "æœªæ‰¾åˆ°æ˜ç¡®çš„ç ”ç©¶é—®é¢˜æè¿°",
    "rag_method": "æœªæ‰¾åˆ°æ˜ç¡®çš„æ–¹æ³•æè¿°",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4390583680",
    "x": 2023.2839114490132,
    "y": -0.2527375382720557,
    "title": "The Integration and Utilization of Artificial Intelligence (AI) in Supporting Older/Senior Lecturers to Adapt to the Changing Landscape in Translation Pedagogy",
    "authors": [
      "Nisar Ahmad Koka"
    ],
    "first_author": "Nisar Ahmad Koka",
    "first_author_surname": "Koka",
    "year": 2023,
    "cited_by_count": 7,
    "venue": "",
    "size": 10.204946247248435,
    "color": "hsl(7, 70%, 60%)",
    "label": "Koka ,2023",
    "rag_problem": "Older/senior lecturers struggle to adapt to the rapidly changing requirements of translation pedagogy due to the integration of AI tools, requiring constant updates to instructional methods and familiarity with new technologies.",
    "rag_method": "The utilization of Artificial Intelligence (AI) as a supportive mechanism to assist older lecturers in learning and adapting to AI-based translation tools, improving their understanding and proficiency in modern pedagogical practices.\n\n**Explanation:** The solution addresses the problem by leveraging AI's capability to support personalized learning and skill development, enabling older/senior lecturers to familiarize themselves with AI tools at their own pace. AI can offer intuitive tutorials, contextual demonstrations, and automated assistance tailored to the lecturers' needs. This approach reduces the cognitive and technical barriers often faced by older educators when adapting to innovative technologies, thus keeping them abreast of the changing landscape in translation pedagogy.",
    "rag_limitation": "- The proposed approach may not fully address the challenges older/senior lecturers face in adapting to constantly evolving AI translation tools, as their learning curve and familiarity with emerging technologies are not accounted for comprehensively.\n- There may be limitations in the ability of AI to tailor solutions specifically for overcoming the pedagogical challenges unique to senior educators, potentially making the integration process less effective.",
    "rag_future_work": "- Explore targeted AI training programs for older/senior lecturers to enhance their ability to adapt to evolving translation tools and methods, addressing their specific learning needs.\n- Investigate the development of AI-driven pedagogical tools tailored for translation education, focusing on simplifying complex AI mechanisms for educators with limited technology experience.\n- Conduct longitudinal studies on the impact of AI integration in improving the teaching efficiency and adaptability of older/senior lecturers within evolving translation pedagogy contexts.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4389456104",
    "x": 2023.542566007402,
    "y": 3.2457411855267444,
    "title": "Unifying Linguistic Landscapes",
    "authors": [
      "Ray Gutierrez"
    ],
    "first_author": "Ray Gutierrez",
    "first_author_surname": "Gutierrez",
    "year": 2023,
    "cited_by_count": 4,
    "venue": "",
    "size": 8,
    "color": "hsl(7, 70%, 60%)",
    "label": "Gutierrez ,2023",
    "rag_problem": "Global language barriers persist, hindering effective communication across different linguistic groups.",
    "rag_method": "Recent advancements in machine translation leveraging neural networks achieve near-human-level accuracy in translation, facilitating cross-linguistic communication.\n\n**Explanation:** Neural networks enable advanced computational learning, analyzing vast linguistic data to provide improved context-aware translations. This addresses the communication challenge by significantly reducing inaccuracies in translations, which are critical for bridging gaps between languages.",
    "rag_limitation": "- The proposed approach involving nanotechnology and augmented reality for real-time translation may face challenges related to potential misuse or ethical concerns, which are not fully addressed in the paper.\n- Despite advancements in machine translation through neural networks, the method still struggles with achieving consistent near-human-level accuracy across all languages and contexts.",
    "rag_future_work": "- Investigate advancements in nanotechnology for enabling real-time translation through augmented reality and wearable devices, addressing its practical feasibility and application.\n- Enhance machine translation systems by leveraging neural networks to achieve higher levels of accuracy, aiming for near-human performance in diverse linguistic contexts.\n- Examine ethical and sociocultural challenges posed by the widespread adoption of these technologies, ensuring equitable and responsible usage globally.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4399213274",
    "x": 2023.8318536966099,
    "y": 1.5568766726030812,
    "title": "Human Intelligence and Artificial Intelligence in Professional Translations â€” Redesigning the Translator Profession",
    "authors": [
      "Felicia Constantin",
      "Anamaria-Mirabela Pop",
      "Monica-Ariana Sim"
    ],
    "first_author": "Felicia Constantin",
    "first_author_surname": "Constantin",
    "year": 2024,
    "cited_by_count": 4,
    "venue": "",
    "size": 8,
    "color": "hsl(0, 70%, 60%)",
    "label": "Constantin ,2024",
    "rag_problem": "The profession of translation faces a significant risk of losing its traditional role and consistency due to the growing reliance on and competition from AI tools, which increasingly shift the focus from creative translation work to post-editing tasks.",
    "rag_method": "Redesigning the translator profession to integrate human intelligence and artificial intelligence in a way that leverages their respective strengths while redefining job roles to maintain the relevance and identity of translators.\n\n**Explanation:** By approaching the issue from an economic perspective and examining the collaboration between human intelligence and artificial intelligence, the proposed redesign outlines how translators can transition from direct translation roles to more strategic tasks such as supervising, refining, and creatively augmenting AI-generated content. This preserves the unique contribution of human skills (contextual understanding, creativity) while utilizing AI for efficiency and routine operations.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate the evolving role of human intelligence (HI) alongside artificial intelligence (AI) in the translation profession, focusing on redefining the value and unique contributions of human translators as AI grows more dominant.\n- Analyze the economic implications of the shift from traditional translation work to post-editing workflows, examining how professionals can adapt to ensure sustainable careers amidst these changes.\n- Develop frameworks or tools to optimize the collaboration between HI and AI, ensuring that AI advancements enhance rather than diminish the professional practice of translation.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4394684629",
    "x": 2023.526376833195,
    "y": 7.7196820497496486,
    "title": "Enhancing translation pedagogy through culture-specific terms",
    "authors": [
      "Matteo Sanesi"
    ],
    "first_author": "Matteo Sanesi",
    "first_author_surname": "Sanesi",
    "year": 2024,
    "cited_by_count": 3,
    "venue": "",
    "size": 8,
    "color": "hsl(0, 70%, 60%)",
    "label": "Sanesi ,2024",
    "rag_problem": "Culture-specific terms often lack direct equivalents in other languages, leading to challenges in communication and translation, such as misreadings, frustration, and involuntary cultural misunderstandings.",
    "rag_method": "Enhancing translation pedagogy by incorporating focused training on culture-specific terms to improve translators' ability to accurately convey ideas across linguistic and cultural boundaries.\n\n**Explanation:** Introducing structured training methods specifically addressing culture-specific terms builds translators' cultural competence and their understanding of how such terms embody the essence of beliefs and values unique to a culture. By equipping translators with strategies to navigate these terms, misunderstandings and miscommunication are minimized, enabling more accurate and effective cross-cultural translation.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2083078026",
    "x": 2013.488642506938,
    "y": 3.827682570695137,
    "title": "Ethical Aspects of Translation: Striking a Balance between Following Translation Ethics and Producing a TT for Serving a Specific Purpose",
    "authors": [
      "Rafat Y. Alwazna"
    ],
    "first_author": "Rafat Y. Alwazna",
    "first_author_surname": "Alwazna",
    "year": 2014,
    "cited_by_count": 10,
    "venue": "",
    "size": 11.953058860449694,
    "color": "hsl(71, 70%, 60%)",
    "label": "Alwazna ,2014",
    "rag_problem": "Strict translation ethics prioritize preserving the original meaning of a source text, making it insufficiently flexible to account for specific audience expectations in certain contexts.",
    "rag_method": "Advocate for a balanced approach in translation ethics, allowing distortion of parts of the source text meaning when necessary to meet the target audience's needs and expectations.\n\n**Explanation:** The solution proposes a reinterpretation of translation ethics where the translator is permitted to adjust the source text meaning in specific cases, enabling the target text to better align with the intended audience's needs. This approach recognizes the practical challenges translators face, ensuring that the target text is functional and empathetic to the cultural or purpose-driven demands of an audience, rather than strictly adhering to rigid ethical codes.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Explore how translation ethics can be dynamically adapted to meet varying audience expectations without compromising core ethical principles. This could involve examining different cultural or situational contexts.\n- Investigate additional frameworks or methodologies that reconcile the opposing views of scholars on translation ethics, particularly focusing on practical implementations in professional translation scenarios.\n- Develop tools or guidelines for translators to assess when and how to ethically deviate from the source text to achieve specific communicative purposes.\n- Conduct empirical studies on audience reception to translations that purposefully alter the meaning of the source text, aiming to better understand ethical implications and audience responses.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3133702157",
    "x": 2021.331427547599,
    "y": -0.3184858334078021,
    "title": "On the Dangers of Stochastic Parrots",
    "authors": [
      "Emily M. Bender",
      "Timnit Gebru",
      "Angelina McMillan-Major"
    ],
    "first_author": "Emily M. Bender",
    "first_author_surname": "Bender",
    "year": 2021,
    "cited_by_count": 4418,
    "venue": "",
    "size": 60,
    "color": "hsl(21, 70%, 60%)",
    "label": "Bender ,2021",
    "rag_problem": "Increasingly large language models pose ethical concerns, including environmental impact, data bias, and lack of transparency in their inner workings.",
    "rag_method": "The authors advocate for a more cautious and ethical approach to developing and using large language models, emphasizing critical examination of their purpose, training data, societal impact, and alignment with ethical standards.\n\n**Explanation:** The proposed solution directly addresses the ethical concerns by guiding developers and researchers to scrutinize the broader implications of language model design and deployment. This includes reducing environmental costs, ensuring fairness in training data by avoiding biases, and fostering transparency regarding how these models function and are used. A careful alignment with ethical practices ensures these models serve societal interests without unintentional harm.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3183428091",
    "x": 2013.979428066921,
    "y": 0.04596638186556001,
    "title": "TEACHING ETHICS AND CRITICAL THINKING IN CONTEMPORARY SCHOOLS",
    "authors": [
      "Bojan Borstner",
      "Smiljana Gartner"
    ],
    "first_author": "Bojan Borstner",
    "first_author_surname": "Borstner",
    "year": 2014,
    "cited_by_count": 11,
    "venue": "",
    "size": 12.430697119148457,
    "color": "hsl(71, 70%, 60%)",
    "label": "Borstner ,2014",
    "rag_problem": "Contemporary schools often fail to equip students with the ability to make ethical, critical, and reflective decisions, which are essential in personal and professional contexts where decisions may have irreversible consequences.",
    "rag_method": "Introduce teaching methods that focus on critical thinking, reflective decision-making, and systematic thought processes in ethics education.\n\n**Explanation:** By emphasizing critical thinking and reflective decision-making strategies, students are trained to approach ethical dilemmas systematically rather than impulsively, enabling them to consider the consequences of their actions on both themselves and others. Systematic methods provide a framework for analyzing the complexities and nuances of ethical dilemmas, ensuring well-reasoned and responsible decision-making.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4244669226",
    "x": 2020.1742009047325,
    "y": 0.0092549012546419,
    "title": "The Routledge Handbook of Translation and Ethics",
    "authors": [
      "Koskinen, Kaisa 1966-",
      "Pokorn, Nike K. 1967-"
    ],
    "first_author": "Koskinen, Kaisa 1966-",
    "first_author_surname": "1966-",
    "year": 2020,
    "cited_by_count": 77,
    "venue": "",
    "size": 22.705725090610027,
    "color": "hsl(28, 70%, 60%)",
    "label": "1966- ,2020",
    "rag_problem": "In Translation Studies, there is a lack of comprehensive exploration of ethical dilemmas faced by translatorial actors, such as translators, trainers, and researchers, which are crucial for addressing ethical challenges in the field.",
    "rag_method": "The Routledge Handbook of Translation and Ethics provides a systematic and wide-ranging analysis of ethical issues, including philosophical and theoretical foundations, and dilemmas specific to various roles in Translation Studies.\n\n**Explanation:** By charting the philosophical and theoretical underpinnings of ethical thinking and addressing the dilemmas specific to actors in translational contexts, the handbook creates a structured framework and practical insights to guide ethical decision-making in translation practice and research. This systematic approach fills the existing gap in understanding and application of ethical principles.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4241903662",
    "x": 2018.4357158329233,
    "y": 0.23440036170023137,
    "title": "Machine Translation and Global Research: Towards Improved Machine Translation Literacy in the Scholarly Community",
    "authors": [
      "Lynne Bowker",
      "Jairo Buitrago"
    ],
    "first_author": "Lynne Bowker",
    "first_author_surname": "Bowker",
    "year": 2019,
    "cited_by_count": 177,
    "venue": "",
    "size": 27.23487136908839,
    "color": "hsl(35, 70%, 60%)",
    "label": "Bowker ,2019",
    "rag_problem": "Scholars and librarians in the digital age lack a specific literacy to effectively use and understand machine translation tools for maximizing global reach and impact of scholarly work.",
    "rag_method": "Introduction of the concept of machine translation literacy, tailored to educate researchers and information professionals on improving their understanding and utilization of machine translation tools.\n\n**Explanation:** Machine translation literacy provides a structured framework for scholars and librarians to better comprehend the limitations, capabilities, and appropriate applications of machine translation tools. By equipping them with the knowledge to use these tools effectively, researchers and librarians can enhance the accessibility and dissemination of scholarly work across multiple languages, bridging linguistic gaps and increasing global impact.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Develop targeted educational programs to enhance machine translation literacy among scholars and librarians, focusing on practical applications and potential pitfalls of using machine translation in scholarly work.\n- Explore strategies to integrate machine translation literacy into existing curricula for researchers and information professionals, ensuring a broader reach and consistent understanding across disciplines.\n- Investigate the impact of improved machine translation literacy on the global dissemination and accessibility of scholarly research, particularly in underrepresented languages or regions.\n- Conduct empirical studies to assess the effectiveness of machine translation literacy initiatives and refine approaches based on the feedback and outcomes from the scholarly community.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4403637392",
    "x": 2024.4451135609759,
    "y": -7.4224666448979075,
    "title": "How developments in natural language processing help us in understanding human behaviour",
    "authors": [
      "Rada Mihalcea",
      "Laura Biester",
      "Ryan L. Boyd"
    ],
    "first_author": "Rada Mihalcea",
    "first_author_surname": "Mihalcea",
    "year": 2024,
    "cited_by_count": 14,
    "venue": "",
    "size": 13.655616215673966,
    "color": "hsl(0, 70%, 60%)",
    "label": "Mihalcea ,2024",
    "rag_problem": "The specific challenge or issue mentioned in the paper would be described here.",
    "rag_method": "The key development or mechanism proposed by the authors would be summarized here.\n\n**Explanation:** A detailed explanation of how the proposed solution addresses the identified problem would be included here.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4402418067",
    "x": 2024.4087529310254,
    "y": -4.2863444308305505,
    "title": "Governing with Intelligence: The Impact of Artificial Intelligence on Policy Development",
    "authors": [
      "Muhammad Asfand E Yar",
      "Mahani Hamdan",
      "Muhammad Anshari"
    ],
    "first_author": "Muhammad Asfand E Yar",
    "first_author_surname": "Yar",
    "year": 2024,
    "cited_by_count": 7,
    "venue": "",
    "size": 10.204946247248435,
    "color": "hsl(0, 70%, 60%)",
    "label": "Yar ,2024",
    "rag_problem": "The lack of efficient tools and frameworks in public policy development to process and analyze extensive data and complex scenarios.",
    "rag_method": "Utilization of artificial intelligence to enhance data processing, predictive modeling, and decision-making capabilities in public policy development.\n\n**Explanation:** Artificial intelligence provides mechanisms to analyze large datasets, simulate various policy scenarios through predictive models, and support evidence-based decision-making. By leveraging AI tools, policymakers can gain deeper insights into patterns, trends, and potential outcomes of policies, which addresses the inefficiencies and complexities in traditional frameworks.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4394828653",
    "x": 2024.5776747940777,
    "y": -1.609514740733554,
    "title": "Artificial Intelligence for the Internal Democracy of Political Parties",
    "authors": [
      "Claudio Novelli",
      "Giuliano Formisano",
      "Prathm Juneja"
    ],
    "first_author": "Claudio Novelli",
    "first_author_surname": "Novelli",
    "year": 2024,
    "cited_by_count": 6,
    "venue": "",
    "size": 9.471942080880634,
    "color": "hsl(0, 70%, 60%)",
    "label": "Novelli ,2024",
    "rag_problem": "Political parties face challenges in ensuring internal democracy due to issues such as biased decision-making, lack of transparency, and inefficiencies in member participation and decision processes.",
    "rag_method": "Implementation of artificial intelligence techniques to improve decision-making processes and enhance transparency in political party operations.\n\n**Explanation:** Artificial intelligence can process large amounts of data objectively, enabling fairer and more inclusive decision-making by eliminating human biases. Additionally, AI can enhance transparency by providing systematic and auditable mechanisms for member participation, tracking decisions, and presenting real-time analytics, thus addressing the inefficiencies and opacity present in traditional political party operations.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4393097350",
    "x": 2023.7078833271303,
    "y": 4.054598299833437,
    "title": "Exploring the role of uncertainty, emotions, and scientific discourse during the COVID-19 pandemic",
    "authors": [
      "Antoine Lemor",
      "Ã‰ric Montpetit"
    ],
    "first_author": "Antoine Lemor",
    "first_author_surname": "Lemor",
    "year": 2024,
    "cited_by_count": 4,
    "venue": "",
    "size": 8,
    "color": "hsl(0, 70%, 60%)",
    "label": "Lemor ,2024",
    "rag_problem": "Policymaking during the COVID-19 pandemic was affected by high levels of uncertainty and emotional responses, making it challenging to establish evidence-based and balanced public health policies.",
    "rag_method": "The development and use of indices via natural language processing (NLP) techniques to measure sentiments of uncertainty, negative emotional sentiments, and the prevalence of scientific statements in policymaker discourse.\n\n**Explanation:** By creating indices that quantitatively measure uncertainty, negative emotions, and scientific discourse prevalence, the authors provide a method to systematically analyze how these factors influenced decision-making. This enables policymakers to better understand the complex interplay of emotions and scientific reasoning, thereby improving the foundation for creating more informed and balanced policies.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Future studies could expand the geographic focus beyond Quebec, Canada, to understand how uncertainty, emotions, and scientific discourse impact COVID-19 policies in other regions or countries. This would enhance the generalizability of the findings.\n- Improvements to the natural language processing (NLP) techniques and indices used could be explored to provide more precise measures of uncertainty, negative sentiments, and scientific discourse in policymaking contexts.\n- Further research might investigate the long-term effects of uncertainty and emotions on public trust in science and policymaking during pandemics to provide deeper insights into maintaining effective communication during crises.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4367397709",
    "x": 2022.801400454831,
    "y": 6.444175854191593,
    "title": "A Study of Ethical Issues in Natural Language Processing with Artificial Intelligence",
    "authors": [
      "Yongfeng Ma"
    ],
    "first_author": "Yongfeng Ma",
    "first_author_surname": "Ma",
    "year": 2023,
    "cited_by_count": 4,
    "venue": "",
    "size": 8,
    "color": "hsl(7, 70%, 60%)",
    "label": "Ma ,2023",
    "rag_problem": "Ethical issues in natural language processing are increasingly prominent due to the extensive applications of NLP systems, but there is a lack of systematic research to address these concerns effectively.",
    "rag_method": "Conducting dedicated research to analyze and address ethical concerns specific to natural language processing and its intertwining with artificial intelligence technologies.\n\n**Explanation:** By systematically studying the ethical implications of NLP in artificial intelligence, this solution provides insights into areas such as bias, privacy, autonomy, and accountability within NLP systems. This research is intended to guide the development of more ethical frameworks and practices specific to NLP technologies, thus directly addressing the lack of comprehension and established guidelines for mitigating ethical risks in the field.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4238374879",
    "x": 2010.4809647163866,
    "y": 4.36815196489241,
    "title": "Congressional Reforms",
    "authors": [
      "E. Scott Adler"
    ],
    "first_author": "E. Scott Adler",
    "first_author_surname": "Adler",
    "year": 2011,
    "cited_by_count": 7,
    "venue": "",
    "size": 10.204946247248435,
    "color": "hsl(92, 70%, 60%)",
    "label": "Adler ,2011",
    "rag_problem": "Insufficient context provided to identify specific pain points related to congressional reforms.",
    "rag_method": "No solution or mechanism can be derived due to lack of substantive content in the provided text.\n\n**Explanation:** The paper's title alone does not describe any problem, mechanism, or intended solution, making it impossible to extract logical relationships or analyze how a solution would address the problem.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2117002298",
    "x": 2005.500234274346,
    "y": 0.03155004681229845,
    "title": "Whose Deaths Matter? Mortality, Advocacy, and Attention to Disease in the Mass Media",
    "authors": [
      "Elizabeth Armstrong",
      "Daniel Carpenter",
      "Marie Hojnacki"
    ],
    "first_author": "Elizabeth Armstrong",
    "first_author_surname": "Armstrong",
    "year": 2006,
    "cited_by_count": 75,
    "venue": "",
    "size": 22.563135850860544,
    "color": "hsl(127, 70%, 60%)",
    "label": "Armstrong ,2006",
    "rag_problem": "Media attention to diseases is inconsistent, and it is unclear how mortality rates and advocacy efforts influence the level of media coverage.",
    "rag_method": "The authors collected and analyzed a unique dataset of print and broadcast media attention to seven diseases across nineteen years, examining both mortality rates and organized interest group advocacy activities.\n\n**Explanation:** By analyzing cross-disease and cross-temporal media attention trends alongside mortality statistics and advocacy activities, the study explicitly identifies the correlation (or lack thereof) between these factors and media coverage. This mechanism offers empirical insights into how mortality and advocacy influence public discourse on diseases, directly addressing the inconsistency in media attention.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2251172991",
    "x": 2013.1447350102283,
    "y": -0.20610304290862924,
    "title": "The New Eye of Government: Citizen Sentiment Analysis in Social Media",
    "authors": [
      "R. Arunachalam",
      "Sandipan Sarkar"
    ],
    "first_author": "R. Arunachalam",
    "first_author_surname": "Arunachalam",
    "year": 2013,
    "cited_by_count": 38,
    "venue": "",
    "size": 18.90077884336159,
    "color": "hsl(78, 70%, 60%)",
    "label": "Arunachalam ,2013",
    "rag_problem": "Governments struggle to achieve transparency and engagement with their citizens due to the lack of efficient tools for understanding public sentiment through large-scale communication channels like social media.",
    "rag_method": "The authors propose an approach to monitor and analyze citizen sentiment on social media platforms, enabling government agencies to comprehend public opinion systematically.\n\n**Explanation:** The proposed mechanism leverages social media data to extract and analyze citizen sentiment. By systematically interpreting public sentiment, governments can address concerns, respond to public needs, and improve transparency and engagement. This analytic tool acts as a bridge between citizens and authorities by offering actionable insights derived from widespread, unstructured communication.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Investigate more advanced sentiment analysis techniques: Future work could focus on improving the accuracy and reliability of sentiment analysis by exploring newer machine learning or deep learning algorithms that consider nuance and complexity in social media language.\n- Expand the scope to diverse social media platforms: Future research could analyze citizen sentiment across a wider range of social media platforms to include varied demographics and communication styles, addressing potential biases from platform-specific data.\n- Address multi-language sentiment analysis: Governments could benefit from systems capable of analyzing sentiment in multiple languages, enabling broader applicability in multilingual regions or global contexts.\n- Study the long-term impact on government policies: Future studies could evaluate how real-time citizen sentiment analysis influences policy changes and government decision-making over extended periods.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W1889043906",
    "x": 1980.481296187212,
    "y": 0.4771880770579643,
    "title": "The Conflict and Peace Data Bank (COPDAB) Project",
    "authors": [
      "Edward E. Azar"
    ],
    "first_author": "Edward E. Azar",
    "first_author_surname": "Azar",
    "year": 1980,
    "cited_by_count": 300,
    "venue": "",
    "size": 30.118587795860822,
    "color": "hsl(240, 70%, 60%)",
    "label": "Azar ,1980",
    "rag_problem": "The lack of systematic procedures and theories to analyze and understand events leading to war, instability, international tension, and events promoting peace and equitable interdependence.",
    "rag_method": "The COPDAB project aims to create a structured data bank to systematize observations and improve analytical skills in studying political events and international relations.\n\n**Explanation:** By developing a comprehensive data bank, COPDAB enables scholars to gather and analyze political event data more effectively. This structured approach facilitates the identification of patterns, correlations, and causal relationships in conflicts and peace-making processes, addressing the gap in systematic methodologies for analyzing international relations.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2099921486",
    "x": 2012.0326523599251,
    "y": 0.33709454904156344,
    "title": "Measuring party positions in Europe",
    "authors": [
      "Ryan Bakker",
      "Catherine E. De Vries",
      "Erica Edwards"
    ],
    "first_author": "Ryan Bakker",
    "first_author_surname": "Bakker",
    "year": 2012,
    "cited_by_count": 829,
    "venue": "",
    "size": 44.60818463017277,
    "color": "hsl(85, 70%, 60%)",
    "label": "Bakker ,2012",
    "rag_problem": "There is a lack of consistent, reliable, and cross-validated measures for national party positions on European integration, ideology, and various EU and non-EU policies over time in Europe.",
    "rag_method": "The 2010 Chapel Hill expert surveys (CHES) and the CHES trend file provide expert-based measures of party positions from 1999âˆ’2010, validated against other datasets such as the Comparative Manifesto Project and European Elections Studies survey.\n\n**Explanation:** The CHES surveys collect expert judgments on party positioning across a wide range of European integration and policy areas, ensuring that data remains consistent and comparable across countries and over time. Validation against independent datasets further enhances its reliability. By providing longitudinal data, the CHES trend file allows for tracking changes in party positioning since 1999, addressing the issue of temporal consistency and aiding researchers wanting to analyze ideological and policy trends in Europe.",
    "rag_limitation": "æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°",
    "rag_future_work": "- Expand the temporal coverage of the CHES trend file beyond 2010 to include more recent data, allowing for analysis of ongoing and evolving trends in party positioning.\n- Investigate the applicability of CHES data to further dimensions of party behavior and explore additional policy areas beyond those currently included.\n- Develop improved methods to cross-validate expert survey data with other sources, such as manifesto analyses or election studies, to enhance data reliability and robustness.\n- Analyze potential methodological limitations in expert judgment and refine techniques for ensuring consistency and accuracy in assessments across different countries and time periods.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  }
];
                const edgesData = [
  {
    "from": "W4205802268",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4205802268",
    "to": "W2165612380",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4205802268",
    "to": "W2118020653",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W4205802268",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W2083078026",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W3133702157",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W3183428091",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W4244669226",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W4241903662",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4360845368",
    "to": "W4211028722",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W4205802268",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W4238374879",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W2117002298",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W2251172991",
    "type": "Alternative",
    "color": "#E67E22",
    "original_color": "#E67E22",
    "width": 2.0,
    "dash": "dot",
    "description": "å¦è¾Ÿè¹Šå¾„ - Bç”¨å®Œå…¨ä¸åŒçš„èŒƒå¼è§£å†³é—®é¢˜ï¼ˆé¢ è¦†åˆ›æ–°ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W1889043906",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4317823603",
    "to": "W2099921486",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2041578073",
    "to": "W3175985315",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2041578073",
    "to": "W2126512988",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2041578073",
    "to": "W2168353148",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2041578073",
    "to": "W2039483660",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2041578073",
    "to": "W1986845215",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2165612380",
    "to": "W2075006521",
    "type": "Alternative",
    "color": "#E67E22",
    "original_color": "#E67E22",
    "width": 2.0,
    "dash": "dot",
    "description": "å¦è¾Ÿè¹Šå¾„ - Bç”¨å®Œå…¨ä¸åŒçš„èŒƒå¼è§£å†³é—®é¢˜ï¼ˆé¢ è¦†åˆ›æ–°ï¼‰"
  },
  {
    "from": "W2165612380",
    "to": "W2144211451",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2165612380",
    "to": "W1908696901",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W2165612380",
    "to": "W2568360633",
    "type": "Alternative",
    "color": "#E67E22",
    "original_color": "#E67E22",
    "width": 2.0,
    "dash": "dot",
    "description": "å¦è¾Ÿè¹Šå¾„ - Bç”¨å®Œå…¨ä¸åŒçš„èŒƒå¼è§£å†³é—®é¢˜ï¼ˆé¢ è¦†åˆ›æ–°ï¼‰"
  },
  {
    "from": "W2118020653",
    "to": "W2165612380",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W168564468",
    "to": "W2165612380",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W1662133657",
    "to": "W2165612380",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "è¿ç§»/åº”ç”¨ - Bå°†Açš„æ–¹æ³•åº”ç”¨åˆ°æ–°é¢†åŸŸï¼ˆæ¨ªå‘æ‰©æ•£ï¼‰"
  },
  {
    "from": "W2045108252",
    "to": "W2165612380",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "è¿ç§»/åº”ç”¨ - Bå°†Açš„æ–¹æ³•åº”ç”¨åˆ°æ–°é¢†åŸŸï¼ˆæ¨ªå‘æ‰©æ•£ï¼‰"
  },
  {
    "from": "W2301363727",
    "to": "W2165612380",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2075006521",
    "to": "W2144211451",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "æ”»å…‹/ä¼˜åŒ– - Bè§£å†³äº†Açš„å±€é™æ€§ï¼ˆçºµå‘æ·±åŒ–ï¼‰"
  },
  {
    "from": "W2914159168",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4211028722",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2517695692",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2517695692",
    "to": "W2025173440",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2911800777",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W2025173440",
    "to": "W2041578073",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4381332452",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4390583680",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4390583680",
    "to": "W4390583680",
    "type": "Realizes",
    "color": "#9B59B6",
    "original_color": "#9B59B6",
    "width": 2.5,
    "dash": "solid",
    "description": "å®ç°æ„¿æ™¯ - Bå®ç°äº†Açš„æœªæ¥å·¥ä½œå»ºè®®ï¼ˆç§‘ç ”ä¼ æ‰¿ï¼‰"
  },
  {
    "from": "W4389456104",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4399213274",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4399213274",
    "to": "W2025173440",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4399213274",
    "to": "W4241903662",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4394684629",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4403637392",
    "to": "W4317823603",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "è¿ç§»/åº”ç”¨ - Bå°†Açš„æ–¹æ³•åº”ç”¨åˆ°æ–°é¢†åŸŸï¼ˆæ¨ªå‘æ‰©æ•£ï¼‰"
  },
  {
    "from": "W4402418067",
    "to": "W4317823603",
    "type": "Alternative",
    "color": "#E67E22",
    "original_color": "#E67E22",
    "width": 2.0,
    "dash": "dot",
    "description": "å¦è¾Ÿè¹Šå¾„ - Bç”¨å®Œå…¨ä¸åŒçš„èŒƒå¼è§£å†³é—®é¢˜ï¼ˆé¢ è¦†åˆ›æ–°ï¼‰"
  },
  {
    "from": "W4394828653",
    "to": "W4317823603",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  },
  {
    "from": "W4393097350",
    "to": "W4317823603",
    "type": "Extends",
    "color": "#2ECC71",
    "original_color": "#2ECC71",
    "width": 2.0,
    "dash": "solid",
    "description": "æ–¹æ³•æ‰©å±• - Båœ¨Açš„æ–¹æ³•åŸºç¡€ä¸Šåšå¢é‡æ”¹è¿›ï¼ˆå¾®åˆ›æ–°ï¼‰"
  },
  {
    "from": "W4367397709",
    "to": "W4317823603",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "åŸºçº¿å¯¹æ¯” - Bä»…æŠŠAä½œä¸ºå¯¹æ¯”å¯¹è±¡ï¼ˆæ— ç›´æ¥ç»§æ‰¿ï¼‰"
  }
];

                // Deep Surveyæ•°æ®
                const deepSurveyData = {
  "topic": "natural language processing",
  "timestamp": "2026-01-13T20:50:04.956425",
  "pruning_stats": {
    "original_papers": 44,
    "pruned_papers": 15,
    "removed_papers": 29,
    "pruning_mode": "comprehensive",
    "strong_components_count": 3,
    "components_with_seed": 0,
    "largest_component_size": 7,
    "seed_papers": 1,
    "original_edges": 50,
    "strong_edges": 12,
    "weak_edges_removed": 1,
    "retention_rate": 0.3409090909090909,
    "relation_type_distribution": {
      "Overcomes": 4,
      "Alternative": 4,
      "Baselines": 1,
      "Adapts_to": 3,
      "Extends": 1
    }
  },
  "evolutionary_paths": [
    {
      "thread_type": "convergence",
      "pattern_type": "The Convergence (æ±‡èšæ¨¡å¼)",
      "title": "å¤šæŠ€æœ¯è·¯çº¿æ±‡èšåˆ° The authors propose a vector space model for automatic indexing, where each enti",
      "narrative": "**èƒŒæ™¯**: åœ¨1975å¹´ä¹‹å‰ï¼Œè¯¥é¢†åŸŸå­˜åœ¨å¤šä¸ªç‹¬ç«‹çš„ç ”ç©¶æ–¹å‘ã€‚**è·¯çº¿1** (æ›¿ä»£å‹): æä¾›äº† Propose a statistical interpretation of term specificity, where specificity is b ä½œä¸ºå¹³è¡Œæ–¹æ¡ˆï¼Œè¢«ä¸­å¿ƒè®ºæ–‡æ•´åˆï¼›**è·¯çº¿2** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿3** (æ›¿ä»£å‹): æä¾›äº† Characterize the usefulness of terms based on their frequency characteristics, i ä½œä¸ºå¹³è¡Œæ–¹æ¡ˆï¼Œè¢«ä¸­å¿ƒè®ºæ–‡æ•´åˆã€‚è¿™äº›æ–¹å‘å„è‡ªä¸ºæ”¿ï¼Œç¼ºä¹ç³»ç»Ÿæ€§æ•´åˆã€‚\n\n**æ±‡èš**: è®ºæ–‡ã€ŠA vector space model for automatic indexingã€‹(1975) åœ¨æ­¤èƒŒæ™¯ä¸‹åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡ The authors propose a vector space model for automatic indexing, where each enti å°†è¿™ 3 æ¡ç‹¬ç«‹è·¯çº¿æœ‰æœºæ•´åˆï¼Œå½¢æˆäº†ç»Ÿä¸€çš„æŠ€æœ¯æ¡†æ¶æ¥è§£å†³ In document retrieval or pattern matching environments, stored entities (e.g., dã€‚\n\n**æ„ä¹‰**: è¿™ç§å¤šæ–¹å‘æ±‡èšæ ‡å¿—ç€è¯¥é¢†åŸŸä»æ¢ç´¢é˜¶æ®µè¿ˆå…¥ç³»ç»ŸåŒ–é˜¶æ®µï¼Œä½¿å¾—åŸæœ¬åˆ†æ•£çš„æŠ€æœ¯è·¯çº¿å¾—ä»¥ååŒå‘æŒ¥ä½œç”¨ï¼Œæ¨åŠ¨äº†é¢†åŸŸçš„ç†è®ºç»Ÿä¸€å’Œå®è·µæ·±åŒ–ã€‚",
      "center_paper": "A vector space model for automatic indexing",
      "routes_count": 3,
      "routes": [
        {
          "relation_type": "Alternative",
          "papers": [
            {
              "paper_id": "W2075006521",
              "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
              "year": 1973,
              "cited_by_count": 571
            },
            {
              "paper_id": "W2144211451",
              "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
              "year": 1972,
              "cited_by_count": 4313
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W1908696901",
              "title": "A Theory of Indexing",
              "year": 1975,
              "cited_by_count": 120
            }
          ]
        },
        {
          "relation_type": "Alternative",
          "papers": [
            {
              "paper_id": "W2568360633",
              "title": "Contribution to the Theory of Indexing",
              "year": 1973,
              "cited_by_count": 17
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W2165612380",
          "title": "A vector space model for automatic indexing",
          "year": 1975,
          "cited_by_count": 7329,
          "role": "center"
        },
        {
          "paper_id": "W2075006521",
          "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
          "year": 1973,
          "cited_by_count": 571
        },
        {
          "paper_id": "W2144211451",
          "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
          "year": 1972,
          "cited_by_count": 4313
        },
        {
          "paper_id": "W1908696901",
          "title": "A Theory of Indexing",
          "year": 1975,
          "cited_by_count": 120
        },
        {
          "paper_id": "W2568360633",
          "title": "Contribution to the Theory of Indexing",
          "year": 1973,
          "cited_by_count": 17
        }
      ],
      "total_citations": 12350,
      "visual_structure": "3 Routes -> Center",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2075006521",
            "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
            "year": 1973
          },
          "to_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "relation_type": "Alternative",
          "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W1908696901",
            "title": "A Theory of Indexing",
            "year": 1975
          },
          "to_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "relation_type": "Overcomes",
          "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
          "route_id": 2,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2568360633",
            "title": "Contribution to the Theory of Indexing",
            "year": 1973
          },
          "to_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "relation_type": "Alternative",
          "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
          "route_id": 3,
          "direction": "chronological"
        }
      ],
      "component_id": 133914149640512
    },
    {
      "thread_type": "divergence",
      "pattern_type": "The Divergence (åˆ†åŒ–æ¨¡å¼)",
      "title": "é’ˆå¯¹ In document retrieval or pattern matching environments, stored entities (e.g., d çš„å¤šæŠ€æœ¯è·¯çº¿åšå¼ˆ",
      "narrative": "**ç„¦ç‚¹**: è®ºæ–‡ã€ŠA vector space model for automatic indexingã€‹(1975) æ˜¯è¯¥é¢†åŸŸçš„åŸºçŸ³å·¥ä½œï¼Œå®ƒèšç„¦äº In document retrieval or pattern matching environments, stored entities (e.g., dï¼Œä½†ç•™ä¸‹äº† æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° çš„é—®é¢˜ã€‚\n\n**åˆ†æ­§**: å­¦æœ¯ç•Œå¯¹æ­¤äº§ç”Ÿäº†ä¸åŒçš„æ¼”è¿›è·¯çº¿ã€‚**è·¯çº¿1** (æ¨ªå‘æ‰©æ•£): å°†æŠ€æœ¯è¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé‡‡ç”¨ The introduction and use of Vector Space Models (VSMs) for semantic processing oï¼›**è·¯çº¿2** (æ¨ªå‘æ‰©æ•£): å°†æŠ€æœ¯è¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé‡‡ç”¨ Developing a method or system for visualizing knowledge domains that organizes aã€‚\n\n**å¯¹æ¯”**: è¿™äº›è·¯çº¿å„æœ‰ä¼˜åŠ¿ï¼Œå…±åŒæ¨åŠ¨äº†é¢†åŸŸçš„å¤šå…ƒåŒ–å‘å±•ã€‚ï¼ˆè¯¦è§å„è·¯çº¿è®ºæ–‡çš„æ€§èƒ½å¯¹æ¯”ï¼‰",
      "center_paper": "A vector space model for automatic indexing",
      "routes_count": 2,
      "routes": [
        {
          "relation_type": "Adapts_to",
          "papers": [
            {
              "paper_id": "W1662133657",
              "title": "From Frequency to Meaning: Vector Space Models of Semantics",
              "year": 2010,
              "cited_by_count": 2831
            }
          ]
        },
        {
          "relation_type": "Adapts_to",
          "papers": [
            {
              "paper_id": "W2045108252",
              "title": "Visualizing knowledge domains",
              "year": 2003,
              "cited_by_count": 1736
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W2165612380",
          "title": "A vector space model for automatic indexing",
          "year": 1975,
          "cited_by_count": 7329,
          "role": "center"
        },
        {
          "paper_id": "W1662133657",
          "title": "From Frequency to Meaning: Vector Space Models of Semantics",
          "year": 2010,
          "cited_by_count": 2831
        },
        {
          "paper_id": "W2045108252",
          "title": "Visualizing knowledge domains",
          "year": 2003,
          "cited_by_count": 1736
        }
      ],
      "total_citations": 11896,
      "visual_structure": "Center -> 2 Routes",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "to_paper": {
            "id": "W1662133657",
            "title": "From Frequency to Meaning: Vector Space Models of Semantics",
            "year": 2010
          },
          "relation_type": "Adapts_to",
          "narrative_relation": "Was_Adapted_By",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975
          },
          "to_paper": {
            "id": "W2045108252",
            "title": "Visualizing knowledge domains",
            "year": 2003
          },
          "relation_type": "Adapts_to",
          "narrative_relation": "Was_Adapted_By",
          "route_id": 2,
          "direction": "chronological"
        }
      ],
      "component_id": 133914149640512
    },
    {
      "thread_type": "divergence",
      "pattern_type": "The Divergence (åˆ†åŒ–æ¨¡å¼)",
      "title": "é’ˆå¯¹ Policymakers face challenges in analyzing large volumes of unstructured text dat çš„å¤šæŠ€æœ¯è·¯çº¿åšå¼ˆ",
      "narrative": "**ç„¦ç‚¹**: è®ºæ–‡ã€ŠNatural Language Processing for Policymakingã€‹(2022) æ˜¯è¯¥é¢†åŸŸçš„åŸºçŸ³å·¥ä½œï¼Œå®ƒèšç„¦äº Policymakers face challenges in analyzing large volumes of unstructured text datï¼Œä½†ç•™ä¸‹äº† æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° çš„é—®é¢˜ã€‚\n\n**åˆ†æ­§**: å­¦æœ¯ç•Œå¯¹æ­¤äº§ç”Ÿäº†ä¸åŒçš„æ¼”è¿›è·¯çº¿ã€‚**è·¯çº¿1** (æ¨ªå‘æ‰©æ•£): å°†æŠ€æœ¯è¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé‡‡ç”¨ The key development or mechanism proposed by the authors would be summarized herï¼›**è·¯çº¿2** (é¢ è¦†åˆ›æ–°): æå‡ºæˆªç„¶ä¸åŒçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½¿ç”¨ Utilization of artificial intelligence to enhance data processing, predictive moï¼›**è·¯çº¿3** (å¾®åˆ›æ–°): ä¿ç•™æ ¸å¿ƒæ¶æ„å¹¶æ‰©å±•ï¼Œé‡‡ç”¨ The development and use of indices via natural language processing (NLP) techniqã€‚\n\n**å¯¹æ¯”**: è¿™äº›è·¯çº¿ä½“ç°äº†å­¦æœ¯ç ”ç©¶çš„å¤šæ ·æ€§ï¼šä¸€äº›é€‰æ‹©é¢ è¦†å¼åˆ›æ–°ï¼Œå¦ä¸€äº›é€‰æ‹©æ¸è¿›å¼æ”¹è¿›ã€‚ï¼ˆè¯¦è§å„è·¯çº¿è®ºæ–‡çš„æ€§èƒ½å¯¹æ¯”ï¼‰",
      "center_paper": "Natural Language Processing for Policymaking",
      "routes_count": 3,
      "routes": [
        {
          "relation_type": "Adapts_to",
          "papers": [
            {
              "paper_id": "W4403637392",
              "title": "How developments in natural language processing help us in understanding human behaviour",
              "year": 2024,
              "cited_by_count": 14
            }
          ]
        },
        {
          "relation_type": "Alternative",
          "papers": [
            {
              "paper_id": "W4402418067",
              "title": "Governing with Intelligence: The Impact of Artificial Intelligence on Policy Development",
              "year": 2024,
              "cited_by_count": 7
            }
          ]
        },
        {
          "relation_type": "Extends",
          "papers": [
            {
              "paper_id": "W4393097350",
              "title": "Exploring the role of uncertainty, emotions, and scientific discourse during the COVID-19 pandemic",
              "year": 2024,
              "cited_by_count": 4
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W4317823603",
          "title": "Natural Language Processing for Policymaking",
          "year": 2022,
          "cited_by_count": 12,
          "role": "center"
        },
        {
          "paper_id": "W4403637392",
          "title": "How developments in natural language processing help us in understanding human behaviour",
          "year": 2024,
          "cited_by_count": 14
        },
        {
          "paper_id": "W4402418067",
          "title": "Governing with Intelligence: The Impact of Artificial Intelligence on Policy Development",
          "year": 2024,
          "cited_by_count": 7
        },
        {
          "paper_id": "W4393097350",
          "title": "Exploring the role of uncertainty, emotions, and scientific discourse during the COVID-19 pandemic",
          "year": 2024,
          "cited_by_count": 4
        }
      ],
      "total_citations": 37,
      "visual_structure": "Center -> 3 Routes",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W4317823603",
            "title": "Natural Language Processing for Policymaking",
            "year": 2022
          },
          "to_paper": {
            "id": "W4403637392",
            "title": "How developments in natural language processing help us in understanding human behaviour",
            "year": 2024
          },
          "relation_type": "Adapts_to",
          "narrative_relation": "Was_Adapted_By",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W4317823603",
            "title": "Natural Language Processing for Policymaking",
            "year": 2022
          },
          "to_paper": {
            "id": "W4402418067",
            "title": "Governing with Intelligence: The Impact of Artificial Intelligence on Policy Development",
            "year": 2024
          },
          "relation_type": "Alternative",
          "narrative_relation": "Led_To_Alternative",
          "route_id": 2,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W4317823603",
            "title": "Natural Language Processing for Policymaking",
            "year": 2022
          },
          "to_paper": {
            "id": "W4393097350",
            "title": "Exploring the role of uncertainty, emotions, and scientific discourse during the COVID-19 pandemic",
            "year": 2024
          },
          "relation_type": "Extends",
          "narrative_relation": "Was_Extended_By",
          "route_id": 3,
          "direction": "chronological"
        }
      ],
      "component_id": 133914147174656
    },
    {
      "thread_type": "convergence",
      "pattern_type": "The Convergence (æ±‡èšæ¨¡å¼)",
      "title": "å¤šæŠ€æœ¯è·¯çº¿æ±‡èšåˆ° The authors propose incorporating ethical competence into translator training pr",
      "narrative": "**èƒŒæ™¯**: åœ¨2023å¹´ä¹‹å‰ï¼Œè¯¥é¢†åŸŸå­˜åœ¨å¤šä¸ªç‹¬ç«‹çš„ç ”ç©¶æ–¹å‘ã€‚**è·¯çº¿1** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿2** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ã€‚è¿™äº›æ–¹å‘å„è‡ªä¸ºæ”¿ï¼Œç¼ºä¹ç³»ç»Ÿæ€§æ•´åˆã€‚\n\n**æ±‡èš**: è®ºæ–‡ã€ŠTranslation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Trainingã€‹(2023) åœ¨æ­¤èƒŒæ™¯ä¸‹åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡ The authors propose incorporating ethical competence into translator training pr å°†è¿™ 2 æ¡ç‹¬ç«‹è·¯çº¿æœ‰æœºæ•´åˆï¼Œå½¢æˆäº†ç»Ÿä¸€çš„æŠ€æœ¯æ¡†æ¶æ¥è§£å†³ The integration of technology and artificial intelligence in translation practicã€‚\n\n**æ„ä¹‰**: è¿™ç§å¤šæ–¹å‘æ±‡èšä¸ºè¯¥é¢†åŸŸæä¾›äº†ç†è®ºæ•´åˆçš„èŒƒä¾‹ï¼Œä½¿å¾—åŸæœ¬åˆ†æ•£çš„æŠ€æœ¯è·¯çº¿å¾—ä»¥ååŒå‘æŒ¥ä½œç”¨ï¼Œæ¨åŠ¨äº†é¢†åŸŸçš„ç†è®ºç»Ÿä¸€å’Œå®è·µæ·±åŒ–ã€‚",
      "center_paper": "Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Training",
      "routes_count": 2,
      "routes": [
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W2083078026",
              "title": "Ethical Aspects of Translation: Striking a Balance between Following Translation Ethics and Producing a TT for Serving a Specific Purpose",
              "year": 2014,
              "cited_by_count": 10
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W3183428091",
              "title": "TEACHING ETHICS AND CRITICAL THINKING IN CONTEMPORARY SCHOOLS",
              "year": 2014,
              "cited_by_count": 11
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W4360845368",
          "title": "Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Training",
          "year": 2023,
          "cited_by_count": 24,
          "role": "center"
        },
        {
          "paper_id": "W2083078026",
          "title": "Ethical Aspects of Translation: Striking a Balance between Following Translation Ethics and Producing a TT for Serving a Specific Purpose",
          "year": 2014,
          "cited_by_count": 10
        },
        {
          "paper_id": "W3183428091",
          "title": "TEACHING ETHICS AND CRITICAL THINKING IN CONTEMPORARY SCHOOLS",
          "year": 2014,
          "cited_by_count": 11
        }
      ],
      "total_citations": 45,
      "visual_structure": "2 Routes -> Center",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2083078026",
            "title": "Ethical Aspects of Translation: Striking a Balance between Following Translation Ethics and Producing a TT for Serving a Specific Purpose",
            "year": 2014
          },
          "to_paper": {
            "id": "W4360845368",
            "title": "Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Training",
            "year": 2023
          },
          "relation_type": "Overcomes",
          "narrative_relation": "è¢«Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Trainingæ•´åˆ",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W3183428091",
            "title": "TEACHING ETHICS AND CRITICAL THINKING IN CONTEMPORARY SCHOOLS",
            "year": 2014
          },
          "to_paper": {
            "id": "W4360845368",
            "title": "Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Training",
            "year": 2023
          },
          "relation_type": "Overcomes",
          "narrative_relation": "è¢«Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Trainingæ•´åˆ",
          "route_id": 2,
          "direction": "chronological"
        }
      ],
      "component_id": 133914147163136
    },
    {
      "thread_type": "chain",
      "pattern_type": "The Chain (çº¿æ€§é“¾æ¡)",
      "title": "ä» The authors propose an approach to monitor and analyze citizen sentiment on soci åˆ° The key development or mechanism proposed by the authors would be summarized her çš„æ¼”è¿›ä¹‹è·¯",
      "narrative": "**èµ·æº** (2013å¹´): è®ºæ–‡ã€ŠThe New Eye of Government: Citizen Sentiment Analysis in Social Mediaã€‹é¦–æ¬¡æå‡ºäº† The authors propose an approach to monitor and analyze citizen sentiment on soci æ¥è§£å†³ Governments struggle to achieve transparency and engagement with their citizens ï¼Œå¼€åˆ›äº†è¿™ä¸€ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œè¯¥å·¥ä½œåœ¨ æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚\n\n**å¦è¾Ÿè¹Šå¾„** (2022å¹´): ä¸åŒäºå‰äººã€ŒThe authors propose an approach to monitor and analyze citizen sentiment on sociã€çš„æ€è·¯ï¼Œè®ºæ–‡ã€ŠNatural Language Processing for Policymakingã€‹æå‡ºäº† The application of Natural Language Processing (NLP) techniques to process, cateï¼Œæ¢ç´¢äº†è§£å†³é—®é¢˜çš„æ–°èŒƒå¼ã€‚\n\n**è·¨åŸŸè¿ç§»** (2024å¹´): è®ºæ–‡ã€ŠHow developments in natural language processing help us in understanding human behaviourã€‹å°†å‰äººåœ¨ã€ŒåŸé¢†åŸŸã€çš„æŠ€æœ¯æˆåŠŸè¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé€šè¿‡ The key development or mechanism proposed by the authors would be summarized her éªŒè¯äº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "papers": [
        {
          "paper_id": "W2251172991",
          "title": "The New Eye of Government: Citizen Sentiment Analysis in Social Media",
          "year": 2013,
          "cited_by_count": 38
        },
        {
          "paper_id": "W4317823603",
          "title": "Natural Language Processing for Policymaking",
          "year": 2022,
          "cited_by_count": 12
        },
        {
          "paper_id": "W4403637392",
          "title": "How developments in natural language processing help us in understanding human behaviour",
          "year": 2024,
          "cited_by_count": 14
        }
      ],
      "total_citations": 64,
      "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2251172991",
            "title": "The New Eye of Government: Citizen Sentiment Analysis in Social Media",
            "year": 2013
          },
          "to_paper": {
            "id": "W4317823603",
            "title": "Natural Language Processing for Policymaking",
            "year": 2022
          },
          "relation_type": "Alternative",
          "narrative_relation": "Led_To_Alternative",
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W4317823603",
            "title": "Natural Language Processing for Policymaking",
            "year": 2022
          },
          "to_paper": {
            "id": "W4403637392",
            "title": "How developments in natural language processing help us in understanding human behaviour",
            "year": 2024
          },
          "relation_type": "Adapts_to",
          "narrative_relation": "Was_Adapted_By",
          "direction": "chronological"
        }
      ],
      "component_id": 133914147174656
    }
  ],
  "survey_report": {
    "title": "Deep Survey: natural language processing",
    "abstract": "æœ¬ç»¼è¿°åŸºäºçŸ¥è¯†å›¾è°±åˆ†æäº† natural language processing é¢†åŸŸçš„æ¼”è¿›å†ç¨‹ã€‚é€šè¿‡å…³ç³»å‰ªæï¼Œæˆ‘ä»¬ä»åŸå§‹å›¾è°±ä¸­ç­›é€‰å‡º 15 ç¯‡é«˜è´¨é‡è®ºæ–‡ï¼Œå¹¶è¯†åˆ«å‡º 5 æ¡å…³é”®æ¼”åŒ–è·¯å¾„ã€‚å…¶ä¸­åŒ…æ‹¬ 1 æ¡çº¿æ€§æŠ€æœ¯é“¾æ¡ã€2 ä¸ªåˆ†åŒ–ç»“æ„å’Œ 2 ä¸ªæ±‡èšç»“æ„ï¼Œå®Œæ•´å‘ˆç°äº†è¯¥é¢†åŸŸçš„æŠ€æœ¯æ¼”è¿›è„‰ç»œã€åˆ†åŒ–è¶‹åŠ¿å’Œæ•´åˆæ¨¡å¼ã€‚",
    "threads": [
      {
        "thread_id": 1,
        "thread_name": "Thread 1: The Convergence (æ±‡èšæ¨¡å¼)",
        "title": "å¤šæŠ€æœ¯è·¯çº¿æ±‡èšåˆ° The authors propose a vector space model for automatic indexing, where each enti",
        "pattern_type": "The Convergence (æ±‡èšæ¨¡å¼)",
        "thread_type": "convergence",
        "narrative": "**èƒŒæ™¯**: åœ¨1975å¹´ä¹‹å‰ï¼Œè¯¥é¢†åŸŸå­˜åœ¨å¤šä¸ªç‹¬ç«‹çš„ç ”ç©¶æ–¹å‘ã€‚**è·¯çº¿1** (æ›¿ä»£å‹): æä¾›äº† Propose a statistical interpretation of term specificity, where specificity is b ä½œä¸ºå¹³è¡Œæ–¹æ¡ˆï¼Œè¢«ä¸­å¿ƒè®ºæ–‡æ•´åˆï¼›**è·¯çº¿2** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿3** (æ›¿ä»£å‹): æä¾›äº† Characterize the usefulness of terms based on their frequency characteristics, i ä½œä¸ºå¹³è¡Œæ–¹æ¡ˆï¼Œè¢«ä¸­å¿ƒè®ºæ–‡æ•´åˆã€‚è¿™äº›æ–¹å‘å„è‡ªä¸ºæ”¿ï¼Œç¼ºä¹ç³»ç»Ÿæ€§æ•´åˆã€‚\n\n**æ±‡èš**: è®ºæ–‡ã€ŠA vector space model for automatic indexingã€‹(1975) åœ¨æ­¤èƒŒæ™¯ä¸‹åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡ The authors propose a vector space model for automatic indexing, where each enti å°†è¿™ 3 æ¡ç‹¬ç«‹è·¯çº¿æœ‰æœºæ•´åˆï¼Œå½¢æˆäº†ç»Ÿä¸€çš„æŠ€æœ¯æ¡†æ¶æ¥è§£å†³ In document retrieval or pattern matching environments, stored entities (e.g., dã€‚\n\n**æ„ä¹‰**: è¿™ç§å¤šæ–¹å‘æ±‡èšæ ‡å¿—ç€è¯¥é¢†åŸŸä»æ¢ç´¢é˜¶æ®µè¿ˆå…¥ç³»ç»ŸåŒ–é˜¶æ®µï¼Œä½¿å¾—åŸæœ¬åˆ†æ•£çš„æŠ€æœ¯è·¯çº¿å¾—ä»¥ååŒå‘æŒ¥ä½œç”¨ï¼Œæ¨åŠ¨äº†é¢†åŸŸçš„ç†è®ºç»Ÿä¸€å’Œå®è·µæ·±åŒ–ã€‚",
        "papers": [
          {
            "paper_id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975,
            "cited_by_count": 7329,
            "role": "center"
          },
          {
            "paper_id": "W2075006521",
            "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
            "year": 1973,
            "cited_by_count": 571
          },
          {
            "paper_id": "W2144211451",
            "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
            "year": 1972,
            "cited_by_count": 4313
          },
          {
            "paper_id": "W1908696901",
            "title": "A Theory of Indexing",
            "year": 1975,
            "cited_by_count": 120
          },
          {
            "paper_id": "W2568360633",
            "title": "Contribution to the Theory of Indexing",
            "year": 1973,
            "cited_by_count": 17
          }
        ],
        "total_citations": 12350,
        "visual_structure": "3 Routes -> Center",
        "relation_stats": {
          "total_relations": 3,
          "relation_distribution": {
            "Alternative": 2,
            "Overcomes": 1
          },
          "dominant_relation": "Alternative"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2075006521",
              "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
              "year": 1973
            },
            "to_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "relation_type": "Alternative",
            "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W1908696901",
              "title": "A Theory of Indexing",
              "year": 1975
            },
            "to_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "relation_type": "Overcomes",
            "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
            "route_id": 2,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2568360633",
              "title": "Contribution to the Theory of Indexing",
              "year": 1973
            },
            "to_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "relation_type": "Alternative",
            "narrative_relation": "è¢«A vector space model for automatic indexingæ•´åˆ",
            "route_id": 3,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2165612380",
              "label": "Center",
              "title": "A vector space model for automatic indexing",
              "year": 1975,
              "citations": 7329,
              "role": "center"
            },
            {
              "id": "W2075006521",
              "label": "ON THE SPECIFICATION OF TERM V...",
              "title": "ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING",
              "year": 1973,
              "citations": 571
            },
            {
              "id": "W2144211451",
              "label": "A STATISTICAL INTERPRETATION O...",
              "title": "A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL",
              "year": 1972,
              "citations": 4313
            },
            {
              "id": "W1908696901",
              "label": "A Theory of Indexing...",
              "title": "A Theory of Indexing",
              "year": 1975,
              "citations": 120
            },
            {
              "id": "W2568360633",
              "label": "Contribution to the Theory of ...",
              "title": "Contribution to the Theory of Indexing",
              "year": 1973,
              "citations": 17
            }
          ],
          "edges": [
            {
              "source": "W2075006521",
              "target": "W2165612380",
              "type": "Alternative",
              "direction": "forward"
            },
            {
              "source": "W1908696901",
              "target": "W2165612380",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W2568360633",
              "target": "W2165612380",
              "type": "Alternative",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "ç®­å¤´æ–¹å‘è¡¨ç¤ºæ—¶é—´æ¼”è¿›æ–¹å‘ï¼ˆæ—©å¹´ä»½ â†’ æ™šå¹´ä»½ï¼‰",
          "pattern_note": "divergence=ä¸­å¿ƒæ‰©æ•£, convergence=å¤šæºæ±‡èš"
        }
      },
      {
        "thread_id": 2,
        "thread_name": "Thread 2: The Divergence (åˆ†åŒ–æ¨¡å¼)",
        "title": "é’ˆå¯¹ In document retrieval or pattern matching environments, stored entities (e.g., d çš„å¤šæŠ€æœ¯è·¯çº¿åšå¼ˆ",
        "pattern_type": "The Divergence (åˆ†åŒ–æ¨¡å¼)",
        "thread_type": "divergence",
        "narrative": "**ç„¦ç‚¹**: è®ºæ–‡ã€ŠA vector space model for automatic indexingã€‹(1975) æ˜¯è¯¥é¢†åŸŸçš„åŸºçŸ³å·¥ä½œï¼Œå®ƒèšç„¦äº In document retrieval or pattern matching environments, stored entities (e.g., dï¼Œä½†ç•™ä¸‹äº† æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° çš„é—®é¢˜ã€‚\n\n**åˆ†æ­§**: å­¦æœ¯ç•Œå¯¹æ­¤äº§ç”Ÿäº†ä¸åŒçš„æ¼”è¿›è·¯çº¿ã€‚**è·¯çº¿1** (æ¨ªå‘æ‰©æ•£): å°†æŠ€æœ¯è¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé‡‡ç”¨ The introduction and use of Vector Space Models (VSMs) for semantic processing oï¼›**è·¯çº¿2** (æ¨ªå‘æ‰©æ•£): å°†æŠ€æœ¯è¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé‡‡ç”¨ Developing a method or system for visualizing knowledge domains that organizes aã€‚\n\n**å¯¹æ¯”**: è¿™äº›è·¯çº¿å„æœ‰ä¼˜åŠ¿ï¼Œå…±åŒæ¨åŠ¨äº†é¢†åŸŸçš„å¤šå…ƒåŒ–å‘å±•ã€‚ï¼ˆè¯¦è§å„è·¯çº¿è®ºæ–‡çš„æ€§èƒ½å¯¹æ¯”ï¼‰",
        "papers": [
          {
            "paper_id": "W2165612380",
            "title": "A vector space model for automatic indexing",
            "year": 1975,
            "cited_by_count": 7329,
            "role": "center"
          },
          {
            "paper_id": "W1662133657",
            "title": "From Frequency to Meaning: Vector Space Models of Semantics",
            "year": 2010,
            "cited_by_count": 2831
          },
          {
            "paper_id": "W2045108252",
            "title": "Visualizing knowledge domains",
            "year": 2003,
            "cited_by_count": 1736
          }
        ],
        "total_citations": 11896,
        "visual_structure": "Center -> 2 Routes",
        "relation_stats": {
          "total_relations": 2,
          "relation_distribution": {
            "Adapts_to": 2
          },
          "dominant_relation": "Adapts_to"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "to_paper": {
              "id": "W1662133657",
              "title": "From Frequency to Meaning: Vector Space Models of Semantics",
              "year": 2010
            },
            "relation_type": "Adapts_to",
            "narrative_relation": "Was_Adapted_By",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2165612380",
              "title": "A vector space model for automatic indexing",
              "year": 1975
            },
            "to_paper": {
              "id": "W2045108252",
              "title": "Visualizing knowledge domains",
              "year": 2003
            },
            "relation_type": "Adapts_to",
            "narrative_relation": "Was_Adapted_By",
            "route_id": 2,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2165612380",
              "label": "Center",
              "title": "A vector space model for automatic indexing",
              "year": 1975,
              "citations": 7329,
              "role": "center"
            },
            {
              "id": "W1662133657",
              "label": "From Frequency to Meaning: Vec...",
              "title": "From Frequency to Meaning: Vector Space Models of Semantics",
              "year": 2010,
              "citations": 2831
            },
            {
              "id": "W2045108252",
              "label": "Visualizing knowledge domains...",
              "title": "Visualizing knowledge domains",
              "year": 2003,
              "citations": 1736
            }
          ],
          "edges": [
            {
              "source": "W2165612380",
              "target": "W1662133657",
              "type": "Adapts_to",
              "direction": "forward"
            },
            {
              "source": "W2165612380",
              "target": "W2045108252",
              "type": "Adapts_to",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "ç®­å¤´æ–¹å‘è¡¨ç¤ºæ—¶é—´æ¼”è¿›æ–¹å‘ï¼ˆæ—©å¹´ä»½ â†’ æ™šå¹´ä»½ï¼‰",
          "pattern_note": "divergence=ä¸­å¿ƒæ‰©æ•£, convergence=å¤šæºæ±‡èš"
        }
      },
      {
        "thread_id": 3,
        "thread_name": "Thread 3: The Divergence (åˆ†åŒ–æ¨¡å¼)",
        "title": "é’ˆå¯¹ Policymakers face challenges in analyzing large volumes of unstructured text dat çš„å¤šæŠ€æœ¯è·¯çº¿åšå¼ˆ",
        "pattern_type": "The Divergence (åˆ†åŒ–æ¨¡å¼)",
        "thread_type": "divergence",
        "narrative": "**ç„¦ç‚¹**: è®ºæ–‡ã€ŠNatural Language Processing for Policymakingã€‹(2022) æ˜¯è¯¥é¢†åŸŸçš„åŸºçŸ³å·¥ä½œï¼Œå®ƒèšç„¦äº Policymakers face challenges in analyzing large volumes of unstructured text datï¼Œä½†ç•™ä¸‹äº† æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° çš„é—®é¢˜ã€‚\n\n**åˆ†æ­§**: å­¦æœ¯ç•Œå¯¹æ­¤äº§ç”Ÿäº†ä¸åŒçš„æ¼”è¿›è·¯çº¿ã€‚**è·¯çº¿1** (æ¨ªå‘æ‰©æ•£): å°†æŠ€æœ¯è¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé‡‡ç”¨ The key development or mechanism proposed by the authors would be summarized herï¼›**è·¯çº¿2** (é¢ è¦†åˆ›æ–°): æå‡ºæˆªç„¶ä¸åŒçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½¿ç”¨ Utilization of artificial intelligence to enhance data processing, predictive moï¼›**è·¯çº¿3** (å¾®åˆ›æ–°): ä¿ç•™æ ¸å¿ƒæ¶æ„å¹¶æ‰©å±•ï¼Œé‡‡ç”¨ The development and use of indices via natural language processing (NLP) techniqã€‚\n\n**å¯¹æ¯”**: è¿™äº›è·¯çº¿ä½“ç°äº†å­¦æœ¯ç ”ç©¶çš„å¤šæ ·æ€§ï¼šä¸€äº›é€‰æ‹©é¢ è¦†å¼åˆ›æ–°ï¼Œå¦ä¸€äº›é€‰æ‹©æ¸è¿›å¼æ”¹è¿›ã€‚ï¼ˆè¯¦è§å„è·¯çº¿è®ºæ–‡çš„æ€§èƒ½å¯¹æ¯”ï¼‰",
        "papers": [
          {
            "paper_id": "W4317823603",
            "title": "Natural Language Processing for Policymaking",
            "year": 2022,
            "cited_by_count": 12,
            "role": "center"
          },
          {
            "paper_id": "W4403637392",
            "title": "How developments in natural language processing help us in understanding human behaviour",
            "year": 2024,
            "cited_by_count": 14
          },
          {
            "paper_id": "W4402418067",
            "title": "Governing with Intelligence: The Impact of Artificial Intelligence on Policy Development",
            "year": 2024,
            "cited_by_count": 7
          },
          {
            "paper_id": "W4393097350",
            "title": "Exploring the role of uncertainty, emotions, and scientific discourse during the COVID-19 pandemic",
            "year": 2024,
            "cited_by_count": 4
          }
        ],
        "total_citations": 37,
        "visual_structure": "Center -> 3 Routes",
        "relation_stats": {
          "total_relations": 3,
          "relation_distribution": {
            "Adapts_to": 1,
            "Alternative": 1,
            "Extends": 1
          },
          "dominant_relation": "Adapts_to"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W4317823603",
              "title": "Natural Language Processing for Policymaking",
              "year": 2022
            },
            "to_paper": {
              "id": "W4403637392",
              "title": "How developments in natural language processing help us in understanding human behaviour",
              "year": 2024
            },
            "relation_type": "Adapts_to",
            "narrative_relation": "Was_Adapted_By",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W4317823603",
              "title": "Natural Language Processing for Policymaking",
              "year": 2022
            },
            "to_paper": {
              "id": "W4402418067",
              "title": "Governing with Intelligence: The Impact of Artificial Intelligence on Policy Development",
              "year": 2024
            },
            "relation_type": "Alternative",
            "narrative_relation": "Led_To_Alternative",
            "route_id": 2,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W4317823603",
              "title": "Natural Language Processing for Policymaking",
              "year": 2022
            },
            "to_paper": {
              "id": "W4393097350",
              "title": "Exploring the role of uncertainty, emotions, and scientific discourse during the COVID-19 pandemic",
              "year": 2024
            },
            "relation_type": "Extends",
            "narrative_relation": "Was_Extended_By",
            "route_id": 3,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W4317823603",
              "label": "Center",
              "title": "Natural Language Processing for Policymaking",
              "year": 2022,
              "citations": 12,
              "role": "center"
            },
            {
              "id": "W4403637392",
              "label": "How developments in natural la...",
              "title": "How developments in natural language processing help us in understanding human behaviour",
              "year": 2024,
              "citations": 14
            },
            {
              "id": "W4402418067",
              "label": "Governing with Intelligence: T...",
              "title": "Governing with Intelligence: The Impact of Artificial Intelligence on Policy Development",
              "year": 2024,
              "citations": 7
            },
            {
              "id": "W4393097350",
              "label": "Exploring the role of uncertai...",
              "title": "Exploring the role of uncertainty, emotions, and scientific discourse during the COVID-19 pandemic",
              "year": 2024,
              "citations": 4
            }
          ],
          "edges": [
            {
              "source": "W4317823603",
              "target": "W4403637392",
              "type": "Adapts_to",
              "direction": "forward"
            },
            {
              "source": "W4317823603",
              "target": "W4402418067",
              "type": "Alternative",
              "direction": "forward"
            },
            {
              "source": "W4317823603",
              "target": "W4393097350",
              "type": "Extends",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "ç®­å¤´æ–¹å‘è¡¨ç¤ºæ—¶é—´æ¼”è¿›æ–¹å‘ï¼ˆæ—©å¹´ä»½ â†’ æ™šå¹´ä»½ï¼‰",
          "pattern_note": "divergence=ä¸­å¿ƒæ‰©æ•£, convergence=å¤šæºæ±‡èš"
        }
      },
      {
        "thread_id": 4,
        "thread_name": "Thread 4: The Convergence (æ±‡èšæ¨¡å¼)",
        "title": "å¤šæŠ€æœ¯è·¯çº¿æ±‡èšåˆ° The authors propose incorporating ethical competence into translator training pr",
        "pattern_type": "The Convergence (æ±‡èšæ¨¡å¼)",
        "thread_type": "convergence",
        "narrative": "**èƒŒæ™¯**: åœ¨2023å¹´ä¹‹å‰ï¼Œè¯¥é¢†åŸŸå­˜åœ¨å¤šä¸ªç‹¬ç«‹çš„ç ”ç©¶æ–¹å‘ã€‚**è·¯çº¿1** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ï¼›**è·¯çº¿2** (å…‹æœå‹): è¯†åˆ«äº†ã€Œæœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿°ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜ã€‚è¿™äº›æ–¹å‘å„è‡ªä¸ºæ”¿ï¼Œç¼ºä¹ç³»ç»Ÿæ€§æ•´åˆã€‚\n\n**æ±‡èš**: è®ºæ–‡ã€ŠTranslation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Trainingã€‹(2023) åœ¨æ­¤èƒŒæ™¯ä¸‹åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡ The authors propose incorporating ethical competence into translator training pr å°†è¿™ 2 æ¡ç‹¬ç«‹è·¯çº¿æœ‰æœºæ•´åˆï¼Œå½¢æˆäº†ç»Ÿä¸€çš„æŠ€æœ¯æ¡†æ¶æ¥è§£å†³ The integration of technology and artificial intelligence in translation practicã€‚\n\n**æ„ä¹‰**: è¿™ç§å¤šæ–¹å‘æ±‡èšä¸ºè¯¥é¢†åŸŸæä¾›äº†ç†è®ºæ•´åˆçš„èŒƒä¾‹ï¼Œä½¿å¾—åŸæœ¬åˆ†æ•£çš„æŠ€æœ¯è·¯çº¿å¾—ä»¥ååŒå‘æŒ¥ä½œç”¨ï¼Œæ¨åŠ¨äº†é¢†åŸŸçš„ç†è®ºç»Ÿä¸€å’Œå®è·µæ·±åŒ–ã€‚",
        "papers": [
          {
            "paper_id": "W4360845368",
            "title": "Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Training",
            "year": 2023,
            "cited_by_count": 24,
            "role": "center"
          },
          {
            "paper_id": "W2083078026",
            "title": "Ethical Aspects of Translation: Striking a Balance between Following Translation Ethics and Producing a TT for Serving a Specific Purpose",
            "year": 2014,
            "cited_by_count": 10
          },
          {
            "paper_id": "W3183428091",
            "title": "TEACHING ETHICS AND CRITICAL THINKING IN CONTEMPORARY SCHOOLS",
            "year": 2014,
            "cited_by_count": 11
          }
        ],
        "total_citations": 45,
        "visual_structure": "2 Routes -> Center",
        "relation_stats": {
          "total_relations": 2,
          "relation_distribution": {
            "Overcomes": 2
          },
          "dominant_relation": "Overcomes"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2083078026",
              "title": "Ethical Aspects of Translation: Striking a Balance between Following Translation Ethics and Producing a TT for Serving a Specific Purpose",
              "year": 2014
            },
            "to_paper": {
              "id": "W4360845368",
              "title": "Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Training",
              "year": 2023
            },
            "relation_type": "Overcomes",
            "narrative_relation": "è¢«Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Trainingæ•´åˆ",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W3183428091",
              "title": "TEACHING ETHICS AND CRITICAL THINKING IN CONTEMPORARY SCHOOLS",
              "year": 2014
            },
            "to_paper": {
              "id": "W4360845368",
              "title": "Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Training",
              "year": 2023
            },
            "relation_type": "Overcomes",
            "narrative_relation": "è¢«Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Trainingæ•´åˆ",
            "route_id": 2,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W4360845368",
              "label": "Center",
              "title": "Translation Technology and Ethical Competence: An Analysis and Proposal for Translatorsâ€™ Training",
              "year": 2023,
              "citations": 24,
              "role": "center"
            },
            {
              "id": "W2083078026",
              "label": "Ethical Aspects of Translation...",
              "title": "Ethical Aspects of Translation: Striking a Balance between Following Translation Ethics and Producing a TT for Serving a Specific Purpose",
              "year": 2014,
              "citations": 10
            },
            {
              "id": "W3183428091",
              "label": "TEACHING ETHICS AND CRITICAL T...",
              "title": "TEACHING ETHICS AND CRITICAL THINKING IN CONTEMPORARY SCHOOLS",
              "year": 2014,
              "citations": 11
            }
          ],
          "edges": [
            {
              "source": "W2083078026",
              "target": "W4360845368",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W3183428091",
              "target": "W4360845368",
              "type": "Overcomes",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "ç®­å¤´æ–¹å‘è¡¨ç¤ºæ—¶é—´æ¼”è¿›æ–¹å‘ï¼ˆæ—©å¹´ä»½ â†’ æ™šå¹´ä»½ï¼‰",
          "pattern_note": "divergence=ä¸­å¿ƒæ‰©æ•£, convergence=å¤šæºæ±‡èš"
        }
      },
      {
        "thread_id": 5,
        "thread_name": "Thread 5: The Chain (çº¿æ€§é“¾æ¡)",
        "title": "ä» The authors propose an approach to monitor and analyze citizen sentiment on soci åˆ° The key development or mechanism proposed by the authors would be summarized her çš„æ¼”è¿›ä¹‹è·¯",
        "pattern_type": "The Chain (çº¿æ€§é“¾æ¡)",
        "thread_type": "chain",
        "narrative": "**èµ·æº** (2013å¹´): è®ºæ–‡ã€ŠThe New Eye of Government: Citizen Sentiment Analysis in Social Mediaã€‹é¦–æ¬¡æå‡ºäº† The authors propose an approach to monitor and analyze citizen sentiment on soci æ¥è§£å†³ Governments struggle to achieve transparency and engagement with their citizens ï¼Œå¼€åˆ›äº†è¿™ä¸€ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œè¯¥å·¥ä½œåœ¨ æœªæ‰¾åˆ°æ˜ç¡®çš„å±€é™æ€§æè¿° æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚\n\n**å¦è¾Ÿè¹Šå¾„** (2022å¹´): ä¸åŒäºå‰äººã€ŒThe authors propose an approach to monitor and analyze citizen sentiment on sociã€çš„æ€è·¯ï¼Œè®ºæ–‡ã€ŠNatural Language Processing for Policymakingã€‹æå‡ºäº† The application of Natural Language Processing (NLP) techniques to process, cateï¼Œæ¢ç´¢äº†è§£å†³é—®é¢˜çš„æ–°èŒƒå¼ã€‚\n\n**è·¨åŸŸè¿ç§»** (2024å¹´): è®ºæ–‡ã€ŠHow developments in natural language processing help us in understanding human behaviourã€‹å°†å‰äººåœ¨ã€ŒåŸé¢†åŸŸã€çš„æŠ€æœ¯æˆåŠŸè¿ç§»åˆ°ã€Œæ–°é¢†åŸŸã€ï¼Œé€šè¿‡ The key development or mechanism proposed by the authors would be summarized her éªŒè¯äº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚",
        "papers": [
          {
            "paper_id": "W2251172991",
            "title": "The New Eye of Government: Citizen Sentiment Analysis in Social Media",
            "year": 2013,
            "cited_by_count": 38
          },
          {
            "paper_id": "W4317823603",
            "title": "Natural Language Processing for Policymaking",
            "year": 2022,
            "cited_by_count": 12
          },
          {
            "paper_id": "W4403637392",
            "title": "How developments in natural language processing help us in understanding human behaviour",
            "year": 2024,
            "cited_by_count": 14
          }
        ],
        "total_citations": 64,
        "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
        "relation_stats": {
          "total_relations": 0,
          "relation_distribution": {},
          "dominant_relation": "Unknown"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2251172991",
              "title": "The New Eye of Government: Citizen Sentiment Analysis in Social Media",
              "year": 2013
            },
            "to_paper": {
              "id": "W4317823603",
              "title": "Natural Language Processing for Policymaking",
              "year": 2022
            },
            "relation_type": "Alternative",
            "narrative_relation": "Led_To_Alternative",
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W4317823603",
              "title": "Natural Language Processing for Policymaking",
              "year": 2022
            },
            "to_paper": {
              "id": "W4403637392",
              "title": "How developments in natural language processing help us in understanding human behaviour",
              "year": 2024
            },
            "relation_type": "Adapts_to",
            "narrative_relation": "Was_Adapted_By",
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2251172991",
              "label": "Paper 1",
              "title": "The New Eye of Government: Citizen Sentiment Analysis in Social Media",
              "year": 2013,
              "citations": 38
            },
            {
              "id": "W4317823603",
              "label": "Paper 2",
              "title": "Natural Language Processing for Policymaking",
              "year": 2022,
              "citations": 12
            },
            {
              "id": "W4403637392",
              "label": "Paper 3",
              "title": "How developments in natural language processing help us in understanding human behaviour",
              "year": 2024,
              "citations": 14
            }
          ],
          "edges": [
            {
              "source": "W2251172991",
              "target": "W4317823603",
              "type": "chronological_evolution",
              "label": "2013 â†’ 2022"
            },
            {
              "source": "W4317823603",
              "target": "W4403637392",
              "type": "chronological_evolution",
              "label": "2022 â†’ 2024"
            }
          ],
          "layout": "hierarchical",
          "direction_note": "ç®­å¤´æ–¹å‘è¡¨ç¤ºæ—¶é—´æ¼”è¿›æ–¹å‘ï¼ˆæ—©å¹´ä»½ â†’ æ™šå¹´ä»½ï¼‰",
          "pattern_note": ""
        }
      }
    ],
    "metadata": {
      "total_papers_analyzed": 44,
      "papers_after_pruning": 15,
      "total_threads": 5,
      "generation_date": "2026-01-13T20:50:04.956364"
    }
  },
  "summary": {
    "original_papers": 44,
    "pruned_papers": 15,
    "total_threads": 5
  }
};

                // Research Ideasæ•°æ®
                const researchIdeasData = {
  "topic": "natural language processing",
  "total_ideas": 10,
  "successful_ideas": 10,
  "ideas": [
    {
      "limitation": "- Explore how SMT can be effectively integrated into diverse translation studies curricula, addressing the gap identified in incorporating Statistical Machine Translation holistically.\n- Develop and test new instructional methods or tools to better align SMT training with the capabilities and needs of translation students.\n- Investigate the impact of SMT-focused training on the professional development and employability of translation students in real-world scenarios.",
      "method": "Introduction of the concept of machine translation literacy, tailored to educate researchers and information professionals on improving their understanding and utilization of machine translation tools.\n\n**Explanation:** Machine translation literacy provides a structured framework for scholars and librarians to better comprehend the limitations, capabilities, and appropriate applications of machine translation tools. By equipping them with the knowledge to use these tools effectively, researchers and librarians can enhance the accessibility and dissemination of scholarly work across multiple languages, bridging linguistic gaps and increasing global impact.",
      "status": "SUCCESS",
      "title": "Integrating Machine Translation Literacy into Translation Studies Curricula: A Framework for Professional Development",
      "abstract": "Background: Statistical Machine Translation (SMT) is a crucial field in translation studies, yet there is a significant gap in effectively incorporating SMT into diverse translation studies curricula. Current training methods often fail to align with the needs of translation students, limiting their preparedness for professional environments. Gap: While 'machine translation literacy' has shown promise for enhancing comprehension of machine translation tools among researchers and librarians, its principles have yet to be adapted to meet the specific pedagogical and practical requirements of translation students. Proposed Method: This study proposes extending the concept of machine translation literacy into a comprehensive educational framework tailored for translation studies curricula. By focusing on practical case studies, contextualized exercises, and real-world professional applications, the framework bridges the gap between theoretical knowledge and practical SMT utility in professional translation tasks. Expected Result: The proposed framework is expected to enhance studentsâ€™ proficiency with SMT tools, improve their employability, and provide a structured methodology to incorporate SMT into curriculum designs effectively.",
      "modification": "Adapt the machine translation literacy framework to include contextualized educational content for translation studies, emphasizing practical SMT applications, case-based learning, and alignment with professional requirements.",
      "reasoning": "Step 1: Compatibility Analysis â€” The candidate concept of machine translation literacy offers a structured understanding of the capabilities, limitations, and utilization of machine translation tools. This directly aligns with the need to bridge the gap in SMT training within translation studies curricula. The methodâ€™s adaptability to address specific user needs, as evidenced in its application for researchers and librarians, demonstrates its flexibility. Thus, the method is compatible. Step 2: Gap Identification â€” The key gap lies in the lack of tailored content and pedagogical methodologies for SMT-focused training in translation studies. Machine translation literacy currently focuses on researchers and information professionals but lacks domain-specific educational frameworks that contextualize SMT for translation training. The bridging variable is the adaptation of machine translation literacy to create curriculum-specific frameworks that include practical exercises, real-world scenarios, and professional alignment. Step 3: Idea Drafting â€” Leveraging the compatibility and addressing the identified gap, the proposed academic framework integrates adapted principles of machine translation literacy into translation studies curricula. The expected outcome is a more practical and professional-ready approach to SMT education, filling the critical gap in current educational practices.",
      "rationale": "To address the identified limitation of integrating SMT into translation curricula, the proposed idea carefully evaluates the suitability of machine translation literacy as a candidate method. Following the complete three-step reasoning path, the solution adapts the principles of this framework into a domain-specific context, drawing heavily from research evolution patterns like 'convergence,' where synthesis of diverse but related methodologies yields impactful solutions. By introducing an SMT-focused educational framework that emphasizes practical tools and professional alignment, the method not only fills the existing gap but also aligns with broader educational and employability objectives."
    },
    {
      "limitation": "- Explore how SMT can be effectively integrated into diverse translation studies curricula, addressing the gap identified in incorporating Statistical Machine Translation holistically.\n- Develop and test new instructional methods or tools to better align SMT training with the capabilities and needs of translation students.\n- Investigate the impact of SMT-focused training on the professional development and employability of translation students in real-world scenarios.",
      "method": "The study proposes examining crowdsourcing and online collaborative translations as transformative phenomena to redefine existing translation theories and better align them with modern translational practices.\n\n**Explanation:** By analyzing crowdsourcing and online collaborative translation practices, the study aims to address the disconnect between traditional translation theories and modern processes. This reframing helps expand theoretical frameworks to include decentralized, community-driven, and technology-based translation methods, thus making the theories more relevant to current trends and public expectations.",
      "status": "SUCCESS",
      "title": "Integrating Crowdsourcing Practices into SMT Training: A Pedagogical Framework for Translation Curricula",
      "abstract": "Background: The integration of Statistical Machine Translation (SMT) into translation studies curricula remains a significant challenge, especially with regard to aligning instructional methods with the professional realities and technological advancements in translation practices. Crowdsourcing and online collaborative translation methods have emerged as dominant paradigms in modern translation, leveraging decentralized, technology-driven approaches that challenge traditional theories. Gap: Current SMT training approaches in academic settings fail to holistically prepare students for these emerging practices, and little research exists on how to incorporate the dynamics of crowdsourcing into translation pedagogy. Proposed Method: This study proposes a novel pedagogical framework that integrates insights from crowdsourcing and online collaborative translation practices into SMT training. Through the development of collaborative online SMT tools and controlled trials in classroom contexts, the framework aims to bridge the gap between theoretical SMT instruction and practical, community-driven translation workflows. Expected Result: By embedding these modern paradigms into translation curricula, the proposed method is expected to enhance students' adaptability, professional readiness, and employability in technology-centric translation industries.",
      "modification": "The bridging innovation requires a modification to include collaborative SMT platforms, designed to simulate real-world crowdsourcing environments. These platforms will allow students to engage in collaborative SMT tasks in a guided, pedagogical setting, emphasizing both translation quality and community interactions.",
      "reasoning": "The reasoning follows a structured, analytical path. Step 1 (Compatibility Analysis): The candidate method of studying crowdsourcing and online collaborative translations aligns well with the limitation's focus on SMT and translation curricula. Both involve technology-driven processes, and the theoretical frameworks of collaborative translation can inform pedagogical adaptations in SMT training. Step 2 (Gap Identification): To address the specific gap of integrating SMT holistically into translation curricula, the existing method needs to be expanded to include: (1) the development of collaborative SMT tools tailored to educational settings, and (2) clear pedagogical guidelines for implementing crowdsourcing dynamics in training contexts. These relate directly to the identified need for bridging traditional SMT instruction with modern, collaborative translation workflows. The bridging variable is the inclusion of collaborative SMT platforms. Step 3 (Drafting the Idea): Building on convergence principles observed in previous evolutionary paths, this proposal synthesizes traditional SMT methodologies with insights from collaborative translation theories to create a comprehensive framework that directly addresses the limitation's constraints and goals.",
      "rationale": "Step 1 Analysis: Statistical Machine Translation (SMT) training requires an alignment of algorithmic capabilities with pedagogical goals. Crowdsourcing and online collaborative translation methods are theoretically compatible, as they involve decentralized but structured translation workflows that mirror collaborative SMT environments. The proposed method does not conflict with SMT's mathematical basis; rather, it complements it by introducing new paradigms for group translation, quality control, and iterative improvement. Step 2 Analysis: The gap lies in the absence of practical tools and pedagogical strategies for incorporating the dynamics of collaborative translation into SMT training. Existing SMT methods are often limited to isolated or theoretical exercises, lacking the interactive, community-driven elements observed in real-world translation environments. The bridging variable is the creation of collaborative SMT platforms, which integrate real-time, group-based translation tasks with technological monitoring and guided feedback to simulate professional translation scenarios. Step 3 Analysis: Inspired by evolution patterns, particularly 'The Convergence,' this idea consolidates insights from collaborative translation and SMT to develop a unified framework for translation pedagogy. By mimicking the successful integration of independent theories and tools into cohesive models (e.g., vector space models), this proposal unites two distinct but complementary domains: SMT and crowdsourcing-based translation. Evidence from academic and industry trends supporting technology-based collaborative methods further substantiates its feasibility."
    },
    {
      "limitation": "- Explore how SMT can be effectively integrated into diverse translation studies curricula, addressing the gap identified in incorporating Statistical Machine Translation holistically.\n- Develop and test new instructional methods or tools to better align SMT training with the capabilities and needs of translation students.\n- Investigate the impact of SMT-focused training on the professional development and employability of translation students in real-world scenarios.",
      "method": "The CERTT Project proposes an active and situated learning model that integrates translation tool usage across different elements of the curriculum, aiming for a holistic educational approach.\n\n**Explanation:** By adopting an active and situated learning model, the CERTT Project ensures that students experience translation tools as part of their real-world translation practice, bridging the gap between theoretical instruction and practical application. This approach utilizes holistic methods to embed tool usage throughout the program, which makes efficient use of limited teaching resources while ensuring graduates are adequately prepared for industry demands.",
      "status": "SUCCESS",
      "title": "Integrating Statistical Machine Translation (SMT) Training into Translation Curriculum via Active Situated Learning",
      "abstract": "Background: Statistical Machine Translation (SMT) is a crucial tool for modern translation professionals, yet its integration into educational curricula remains limited. This gap affects the readiness of translation students to leverage SMT in real-world scenarios. Gap: Existing models like CERTT Project use active and situated learning but lack explicit focus on SMT-specific training that bridges theory and practice comprehensively. Proposed Method: We propose an enhanced framework that aligns the active situated learning principles of the CERTT Project with dedicated SMT training modules. By embedding realistic SMT usage scenarios into diverse curriculum elements and measuring impacts on professional growth, the method ensures alignment with industry demands. Expected Result: Students will gain practical proficiency in SMT tools, which will improve employability and adaptation to professional demands, fostering a holistic comprehension of translation technologies.",
      "modification": "Develop supplemental SMT-focused modules integrated within the CERTTâ€™s active and situated learning framework. These modules will include real-world project tasks, SMT tool mastery components, and impact assessments focusing on employability and professional readiness.",
      "reasoning": "Step 1 Analysis: The CERTT Project's active and situated learning model is grounded in principles that support practical tool usage within real-world contexts. This aligns well with the need to integrate SMT into translation studies, given SMT's practical application and technical demands. SMT training naturally requires hands-on exposure and must be embedded in real-life-like scenarios, making the CERTT model theoretically compatible. Computational complexity and implementation logistics of an active learning environment are manageable within an academic curriculum, indicating compatibility. Step 2 Analysis: The gap is that the CERTT model does not currently emphasize SMT tools specifically, nor does it assess professional development outcomes related to mastering SMT. The bridging variable involves designing supplemental SMT modules that include realistic translation tasks, tool-specific training, and an evaluative framework for employability impacts. Learning lessons from evolutionary patterns, such as convergence in integrating isolated research streams (see Path 1), suggests merging SMT technical training with CERTT concepts for comprehensive integration. Step 3 Idea Drafting: Drawing from the reasoning that effective curriculum integration demands marrying practical skill development with industry relevancy, the enhanced method combines CERTT's situated learning with SMT-specific modules, providing tools and measures for real-world application readiness. By using evolutionary insights, we leverage the integration principle observed in past convergence examples to propose modules that holistically connect theory to practical skill development for immediate industry relevance.",
      "rationale": "Step 1 Analysis established compatibility by showing that the CERTT Projectâ€™s framework could effectively align with the hands-on requirements of SMT usage education. CERTTâ€™s principles of active and situated learning mirror the demands of SMT instruction, including real-world scenario embedding. Step 2 Analysis identified the primary gap: the lack of SMT-specific focus within CERTT's general tool integration model. To address this, a bridging variable was proposedâ€”supplemental SMT-focused modules that align with active learning. This was inspired by evolutionary paths like Path 1, which demonstrated the successful integration of technical tools into comprehensive frameworks. The convergence logic here serves as a guiding pattern. Step 3 Idea Drafting culminated in a structured proposal for an adapted framework that marries SMT training with CERTT concepts. The drafting process explicitly tied theoretical and operational principles to the enhancement of employability outcomes of translation students, guided by past successes in holistic tool integration models. The decision chain moved logically from assessing compatibility â†’ identifying a curriculum gap â†’ proposing a modular solution that accounts for the evolutionary insights of industry-ready translations systems. Evidence includes the CERTT framework's existing success in practical application training and demonstrated demand for SMT-centric education among translation students. The idea is feasible due to its alignment with known instructional capacities and evolutionary paths in integrating tool-specific curricula."
    },
    {
      "limitation": "- Explore how SMT can be effectively integrated into diverse translation studies curricula, addressing the gap identified in incorporating Statistical Machine Translation holistically.\n- Develop and test new instructional methods or tools to better align SMT training with the capabilities and needs of translation students.\n- Investigate the impact of SMT-focused training on the professional development and employability of translation students in real-world scenarios.",
      "method": "Enhancing translation pedagogy by incorporating focused training on culture-specific terms to improve translators' ability to accurately convey ideas across linguistic and cultural boundaries.\n\n**Explanation:** Introducing structured training methods specifically addressing culture-specific terms builds translators' cultural competence and their understanding of how such terms embody the essence of beliefs and values unique to a culture. By equipping translators with strategies to navigate these terms, misunderstandings and miscommunication are minimized, enabling more accurate and effective cross-cultural translation.",
      "status": "SUCCESS",
      "title": "Integrating Statistical Machine Translation (SMT) with Cultural Competence Training: A Comprehensive Curriculum for Translation Studies",
      "abstract": "Background: Statistical Machine Translation (SMT) has significant potential in professional translation settings, yet current translation studies curricula often lack comprehensive integration of SMT training that aligns with students' practical needs and skills development. This gap particularly impacts the ability to navigate culture-specific terms during cross-cultural translations. Gap: While existing translation pedagogy emphasizes linguistic competency, there is minimal focus on structured methodologies for effectively training on SMT and addressing culture-specific terms within translation frameworks. Proposed Method: This study proposes a curriculum enhancement whereby focused modules on culture-specific terms are integrated with SMT training. By leveraging a hybrid pedagogy that combines SMT workflows with cultural competence strategies, the method aims to improve students' capability to address the dual challenges of linguistic precision and cultural contextuality. Expected Result: The modified curriculum is expected to create a robust training paradigm that not only enhances studentsâ€™ employability and professional readiness but also bridges theoretical knowledge and practical application of SMT in diverse cultural contexts.",
      "modification": "Develop a hybrid curriculum model incorporating SMT mechanics alongside structured training on culture-specific term translation methodologies. This approach would require a dual focus on algorithmic SMT processes and cultural contextual learning through examples and case studies.",
      "reasoning": "Step 1: Compatibility Analysis - The candidate method, which emphasizes the training of translators on culture-specific terms, aligns well with the limitation of integrating SMT into translation curricula. SMT inherently deals with statistical patterns and linguistic structures, which can be augmented by training in cultural terms to improve fluency and accuracy. Computational complexity is unlikely to be a barrier, as the curriculum enhancement primarily addresses instructional design and application rather than algorithmic elaboration directly. Hence, the foundational assumptions of SMT are compatible with the focus on culture-specific training. Step 2: Gap Identification - The limitation lies in the absence of holistic SMT integration into curricula and the lack of emphasis on cultural nuance. The bridging variable here is the development of a hybrid pedagogy that combines SMT workflows with structured training on navigating culture-specific terms. This modification requires a dual-layer approach to training, enabling students to understand SMT mechanisms while honing their cultural interpretive skills. Step 3: Idea Drafting - The methodological innovation centers around integrating SMT with cultural competence training into a cohesive curriculum. Drawing from evolutionary paths, such as the convergence logic (multi-direction integration of Indexing models) and divergence growth patterns in computational linguistics, this new curriculum design borrows the principle of combining diverse, previously disjoint techniques into a unified framework for translation education.",
      "rationale": "Step 1 Analysis: SMT's mathematical foundation, based on probabilistic modeling, is compatible with curriculum development aimed at cultural translation training. The proposed method emphasizes training about culture-specific terms within the workflow of SMTâ€”addressing linguistic complexity and cultural nuances simultaneously. SMTâ€™s algorithmic assumptions on language patterns align naturally with the pedagogical goal of enabling accurate translations across linguistic and cultural boundaries. No fundamental incompatibility exists. Step 2 Analysis: Bridging the identified gap requires addressing two levels: the integration of SMT into pragmatic translation training and embedding cultural competence as a critical skill. The evolutionary logic supports this dual-layer model through the principle of convergence (combining linguistic and cultural elements) and divergence (adapting SMT across real-world cultural contexts). The bridging innovation ensures students are equipped with the theoretical and applied tools necessary for professional translation jobs. Step 3 Analysis: Inspired by evolutionary paths, the drafted idea integrates SMT workflows with cultural competence modules to address the limitation effectively. It follows a curriculum design that focuses simultaneously on the statistical mechanics of SMT and dynamic cultural nuances. Evidence from successful multi-method approaches to index and semantic models supports the feasibility of such integration. Decision Chain: The limitation demands SMT integration into curricula â†’ The methodâ€™s emphasis on culture-specific learning complements SMT applications â†’ Hybrid curricular solutions bridge the gap between SMT mechanics and cultural specificity. Evidence: Convergence logic from evolutionary patterns demonstrates successful integration when multiple disconnected techniques are unified to address broader domain challenges. Thus, a comprehensive curriculum enhances employability and professional readiness by teaching translation students SMT within culturally nuanced contexts."
    },
    {
      "limitation": "- Explore how SMT can be effectively integrated into diverse translation studies curricula, addressing the gap identified in incorporating Statistical Machine Translation holistically.\n- Develop and test new instructional methods or tools to better align SMT training with the capabilities and needs of translation students.\n- Investigate the impact of SMT-focused training on the professional development and employability of translation students in real-world scenarios.",
      "method": "The concept of expansive learning, which emphasizes collective transformation processes occurring when groups tackle systemic contradictions and collaboratively generate new knowledge or practices.\n\n**Explanation:** Expansive learning addresses this issue by shifting the focus from individual cognition to shared, collective efforts aimed at resolving contradictions within an activity system. By emphasizing the collaborative development of solutions and innovations, expansive learning provides a framework for understanding and enhancing how groups engage in transformative learning processes within their environments.",
      "status": "SUCCESS",
      "title": "Expansive Learning for Holistic SMT Education: Collaborative Frameworks for Transformative Translation Training",
      "abstract": "Background: Statistical Machine Translation (SMT) integration into translation studies curricula has faced barriers in aligning training with diverse student needs, industry demands, and real-world contexts. Expansive learning, emphasizing collective and systemic problem-solving, offers a promising lens for addressing such pedagogical challenges. Gap: Existing SMT training methods remain largely individualistic and fail to leverage collaborative learning environments, leaving a gap in equipping students for both professional adaptability and the co-creation of knowledge. Proposed Method: We propose an expansive learning framework tailored to SMT education, fostering collaborative projects where students and instructors identify system-level contradictions in translation tasks and co-develop instructional methods or tools. This will include developing interactive modules and challenges, alongside feedback loops aligning with professional scenarios. Expected Result: The approach is anticipated to yield a transformative impact on SMT pedagogical practices by enhancing the alignment of curricula with both educational and industry needs, ultimately improving student employability and professional preparedness.",
      "modification": "Design a pedagogical framework within the expansive learning model that connects the collective resolution of systemic contradictions in translation studies to practical SMT training. This involves creating collaborative problem-solving modules tied to SMT scenarios.",
      "reasoning": "Expansive learning is inherently compatible with addressing systemic barriers in translation studies curricula, particularly for SMT, as it focuses on resolving contradictions through collaborative processes. However, direct application to SMT education requires a deliberate effort to align the method with specific pedagogical challenges such as curriculum design, student engagement in problem-solving, and industry-context simulations. Drawing from the evolutionary patterns, such as how vector space modeling was adapted and extended into new domains, a similar approach can be taken to transform expansive learning principles into tailored frameworks for SMT education, bridging the gap between theoretical knowledge and real-world application. The bridging variable here is a pedagogical adaptation that couples expansive learning's systemic focus with SME-specific collaborative training tools and methodologies.",
      "rationale": "### Step 1: Analyze Compatibility\nExpansive learning operates on principles highly suited to addressing the limitations of SMT integration in translation studies curricula. Its collective, systemic resolution of contradictions directly targets the challenges of effectively aligning training with diverse student needs and industry requirements. Unlike traditional, individual-focused pedagogical models, expansive learning provides a framework focused on collaboration and transformation, critical for a domain like SMT that requires adaptability and co-creation of knowledge. Hence, at the theoretical and methodological level, the principles of expansive learning are compatible with the objectives of transforming SMT curricula.\n\n### Step 2: Identify the Gap\nDespite its compatibility, expansive learning as a broad theory requires specific adaptations to address SMTâ€™s unique pedagogical and professional demands. Traditional design in expansive learning has not yet been tied directly to the operational demands of SMT education. To bridge this gap, a pedagogical adaptation must be made: (1) a framework that integrates expansive learning into real-world translation tasks, (2) collaborative problem-solving modules that mirror both students' learning needs and professional scenarios, and (3) interactive feedback mechanisms to evaluate learning progress. The key bridging variable is the incorporation of SMT-specific tools and methods into the expansive learning processes, enabling both theoretical and practical education alignment.\n\n### Step 3: Draft the Idea\nLearning from the convergence patterns seen in the first evolutionary path, where multiple methodologies were synthesized into a single cohesive model, we adapt expansive learning principles into a cohesive pedagogical framework for SMT. The model will include collaborative modules for systemic contradiction resolution, training students in both theoretical and practical aspects of SMT. This ensures a comprehensive approach to learning that simulates real-world scenarios and aligns well with professional demands.\n\nThe proposed title reflects the transformative ambition of this framework, while the structured abstract charts the logical pathway from existing barriers to the innovative solution. By explicitly delineating background gaps, the bridging variable, and expected outcomes, we create a robust roadmap for the research idea. Furthermore, referencing evolutionary divergence patterns, such as lateral adaptation of a base methodology to new domains, solidifies the feasibility and innovation potential of this approach."
    },
    {
      "limitation": "- Explore how SMT can be effectively integrated into diverse translation studies curricula, addressing the gap identified in incorporating Statistical Machine Translation holistically.\n- Develop and test new instructional methods or tools to better align SMT training with the capabilities and needs of translation students.\n- Investigate the impact of SMT-focused training on the professional development and employability of translation students in real-world scenarios.",
      "method": "The authors extend the Technology Acceptance Model (TAM) to specifically analyze and model student translators' intention to use machine translation tools.\n\n**Explanation:** By adapting the Technology Acceptance Model (TAM), the authors incorporate factors such as perceived usefulness, perceived ease of use, and other relevant constructs tailored to the context of student translators. This extended framework provides insights into the key determinants influencing their intention to use machine translation tools, helping educators and developers better address adoption barriers.",
      "status": "SUCCESS",
      "title": "Adapting the Technology Acceptance Model (TAM) for Statistical Machine Translation Pedagogy: A Framework for Enhanced Curriculum Integration",
      "abstract": "Background: Statistical Machine Translation (SMT) presents a significant pedagogical challenge in translation studies, with existing curricula struggling to fully integrate SMT training in ways that align with student needs and professional expectations. Current instructional tools and methodologies often fail to address student translators' acceptance and effective use of machine translation tools. Gap: There is a lack of a theoretical framework to understand and model the factors influencing students' intention to adopt SMT tools. Proposed Method: This study extends the Technology Acceptance Model (TAM) specifically for SMT pedagogy, incorporating domain-specific constructs such as perceived adequacy for professional standards, perceived relevance to real-world translation tasks, and technological competence of student translators. The adapted TAM framework is then validated through curriculum design experiments and longitudinal analysis of student outcomes. Expected Result: The proposed extension of TAM will enable a deeper understanding of adoption barriers and enablers, facilitate the development of more effective SMT-integrated teaching methods, and positively impact the employability and professional growth of translation students.",
      "modification": "The bridging variable is the incorporation of domain-specific constructs into the Technology Acceptance Model (TAM), such as perceived relevance to translation tasks, adequacy for professional standards, and technological proficiency tailored for translation students.",
      "reasoning": "To determine the compatibility of extending TAM to address the limitations in SMT integration within translation studies curricula, a thorough analysis was conducted. First, TAM's adaptability is reviewed, specifically its ability to incorporate custom constructs aligned with specific contexts, as it has historically demonstrated utility across diverse domains. This is compatible with the multidisciplinary nature of SMT training, which spans technology, linguistics, and pedagogy. However, a gap remains in tailoring the TAM framework specifically to translation training scenarios. Unlike general educational tools, SMT requires addressing unique factors such as real-world applications, professional standards, and students' translation-specific technological skills. Inspired by the Convergence Pattern from the evolutionary paths, where diverse approaches were unified into a central model (e.g., the vector space model), adapting TAM similarly converges adoption modeling with domain-level specificity. By designing a specialized TAM extension that incorporates translation-relevant variables (e.g., adequacy for professional standards, task relevance), this method can achieve holistic integration within SMT curricula. Furthermore, it also aligns with the Divergence Pattern, as it expands TAM's utility into a novel pedagogical domain with unique requirements.",
      "rationale": "Step 1 Analysis: The core properties of the Technology Acceptance Model (TAM), including its ability to analyze behavioral intentions and its extensibility through additional context-specific variables, suit the problem's constraints. SMT training involves significant variability in perceived usefulness and ease of use among student translators. TAM's mathematical and theoretical foundation is compatible with these dimensions, making it suitable for understanding and improving SMT integration. Additionally, its moderate computational complexity and flexibility for domain adaptation align well with educational contexts, where data collection and analysis often pose logistical challenges. Therefore, there is compatibility. Step 2 Analysis: The gap arises from TAM's generality; it lacks constructs that account for the unique features of translation pedagogy, such as perceived adequacy to real-world tasks and alignment with professional translator demands. To bridge this gap, the bridging variable is the introduction of domain-focused constructs into TAM. This involves extending TAM to include perceived relevance to SMT-specific tasks, adequacy to industry standards, and technological competence of translation students. The evolutionary Convergence Pattern indicates the strength of unifying frameworks (e.g., integrating diverse approaches like in the 'vector space model'), supporting this extension. Divergence also shows value in domain-specific expansions. Step 3 Analysis: By synthesizing these findings, the idea was drafted with a specific focus on extending TAM for SMT pedagogy. Learning from the evolutionary paths, this approach reflects both convergence (unifying SMT-specific needs into TAM) and divergence (adapting TAM to a new domain). The complete chain of thoughtâ€”from identifying TAM's compatibility, through defining domain-specific gaps, to devising a solutionâ€”substantiates this idea. Therefore, the final proposal addresses the SMT curriculum integration bottleneck realistically and innovatively."
    },
    {
      "limitation": "- Explore how SMT can be effectively integrated into diverse translation studies curricula, addressing the gap identified in incorporating Statistical Machine Translation holistically.\n- Develop and test new instructional methods or tools to better align SMT training with the capabilities and needs of translation students.\n- Investigate the impact of SMT-focused training on the professional development and employability of translation students in real-world scenarios.",
      "method": "Computer-assisted translation tools provide functionalities like translation memory and terminology management to streamline translation processes.\n\n**Explanation:** These tools enhance the consistency and efficiency of translations by allowing translators to reuse previously translated segments and ensure adherence to standardized terminologies, reducing the time required for manual adjustments and quality assurance.",
      "status": "SUCCESS",
      "title": "Integrated SMT Curriculum: Bridging Statistical Machine Translation and Real-world Employability",
      "abstract": "Background: Statistical Machine Translation (SMT) has become a cornerstone of modern translation workflows, yet its incorporation into translation studies curricula remains fragmented and underdeveloped. Gap: Current instructional methods and tools inadequately align with the complexities of SMT, particularly in training students for real-world professional contexts. Proposed Method: This study presents an innovative integration of computer-assisted translation (CAT) tools with SMT-focused training modules. By leveraging functionalities such as translation memory, terminology management, and quality assessment tools, the research develops a curriculum that imbibes practical SMT skill training. Expected Result: The proposed approach aims to improve students' understanding of SMT workflows, enhance skill alignment with industry needs, and empirically evaluate its influence on employability and professional growth.",
      "modification": "Adaptation of computer-assisted translation tools to function as pedagogical platforms by extending their features to include interactive SMT training modules and integration of industry-oriented datasets.",
      "reasoning": "After thorough analysis, the compatibility assessment of the candidate method (computer-assisted translation tools) indicated alignment with the requirements of the limitation. These tools already incorporate functionalities such as translation memory and terminology management, which directly relate to practical SMT workflows. However, a gap exists in their adaptation for educational purposes, particularly to align training with professional applications of SMT. Bridging this gap involves restructuring these tools to function as instructional platforms that can simulate real-world scenarios for students, thus addressing the identified limitation.",
      "rationale": "Step 1 Analysis: Compatibility was established by recognizing that computer-assisted translation (CAT) tools provide functionalities like translation memory, terminology management, and quality assurance, which directly align with the technical aspects of SMT workflows. Importantly, these functionalities already enhance translatorsâ€™ efficiency and consistency, indicating potential applicability to an educational context. However, the current state of CAT tools lacks explicit support for pedagogical applications, especially for integration into curricula or training structures.\n\nStep 2 Analysis: The gap lies in the non-educational orientation of existing CAT tools. To bridge this, the bridging variable involves the modification of CAT tools into pedagogically oriented platforms by extending their core functionalities to support interactive learning. Specifically, these adaptations would include (1) the incorporation of curriculum-aligned modules that train students in SMT principles, (2) simulation of industry-relevant tasks, and (3) use of real-world datasets and scenarios to ensure alignment with professional environments.\n\nStep 3 Analysis: The proposed idea draws from the evolutionary pattern of convergence, as seen in Evolutionary Path 1, where multiple independent methods were integrated into a unifying framework to address a research limitation. Here, CAT tools and SMT-specific training are conceptually integrated to form a holistic educational platform. This leverages the foundational strengths of CAT tools while incorporating the unmet needs of SMT-focused education and employability. 'Integrated SMT Curriculum: Bridging Statistical Machine Translation and Real-world Employability' encapsulates the goal of creating a unified pedagogical framework grounded in both theory and real-world application.\n\nDecision Chain: (1) Problem was articulated as the lack of holistic SMT integration into translation curricula. (2) Computer-assisted translation tools were identified as a candidate method for addressing this limitation. (3) Through compatibility analysis, CAT tools met the technical and theoretical requirements of integrating SMT functionalities. (4) Gap identified as the non-educational design of CAT tools, needing pedagogical adaptations. (5) Proposed a curriculum that modifies CAT tools to include SMT-specific training and real-world simulation, aligning with employability outcomes.\n\nEvidence: Learning from Evolutionary Path 1, where multiple methods were consolidated, strongly supported the integrative approach proposed here. Adapting CAT tools for training purposes follows a similar trajectory of creating a unifying, functional framework for a fragmented problem. This approach is also informed by the patterns of divergence in Evolutionary Paths 2 and 3, where original methods were extended beyond their initial scope to address new domains. Our extension of CAT tools into the educational realm mirrors this adaptive trajectory."
    },
    {
      "limitation": "- Explore how SMT can be effectively integrated into diverse translation studies curricula, addressing the gap identified in incorporating Statistical Machine Translation holistically.\n- Develop and test new instructional methods or tools to better align SMT training with the capabilities and needs of translation students.\n- Investigate the impact of SMT-focused training on the professional development and employability of translation students in real-world scenarios.",
      "method": "The authors propose a holistic Statistical Machine Translation (SMT) syllabus designed specifically for translation students, prioritizing the integration of SMT into translation education.\n\n**Explanation:** The proposed syllabus addresses the problem by systematically teaching SMT concepts, tools, and applications to translation students, ensuring they understand how to use SMT technology in translation processes. By creating a dedicated syllabus, the program ensures students gain hands-on experience and theoretical knowledge, equipping them to work effectively with SMT in professional contexts.",
      "status": "SUCCESS",
      "title": "Integrative Pedagogy for Statistical Machine Translation: Bridging Technology and Translation Education",
      "abstract": "Background: The growing relevance of Statistical Machine Translation (SMT) in professional translation workflows necessitates its systematic integration into translation studies curricula. However, current research highlights a gap in holistic instructional methods tailored to translation students, addressing their pedagogical and professional preparation needs. Gap: Existing SMT teaching methodologies prioritize either theoretical understanding or practical tool usage without aligning these two aspects holistically. Further, there is limited research on the impact of SMT training on students' professional development and employability. Proposed Method: This research proposes the development of an integrative pedagogy combining hands-on SMT tool training, theoretical instruction, and real-world application case studies. The syllabus design aligns theoretical constructs of SMT, such as probabilistic models and language pairs, with practical, immersive experiences in using SMT tools for translation tasks. The curriculum will incorporate evaluation workshops, professional skill enhancement modules, and employability-focused training. Expected Result: The proposed syllabus will bridge the educational gap by equipping translation students with both conceptual and practical SMT expertise. Empirical testing will demonstrate improvements in their professional readiness and competitiveness in the translation industry.",
      "modification": "Integrate experiential learning modules, case studies, and employability-focused workshops into the SMT syllabus, emphasizing the connection between abstract SMT concepts and real-world translation practices.",
      "reasoning": "The reasoning process starts by analyzing the compatibility of the proposed SMT-focused syllabus with the research limitation. The syllabus aims to holistically integrate SMT into translation curricula by providing conceptual and practical learning experiences. This method is compatible with the identified limitation as it explicitly targets the educational and professional development goals for translation students. However, a gap exists due to the lack of experiential learning modules that bridge abstract SMT concepts with tangible employability outcomes. Drawing on patterns observed in the evolutionary pathsâ€”particularly, the convergence model where multiple methodologies are integrated to address a limitationâ€”the syllabus can be modified to incorporate experiential learning modules and employability workshops. The modification anchors theoretical SMT education to practical outcomes, fostering a stronger alignment with industry needs. This aligned approach is inspired by broader evolutionary strategies where innovations adapt core methods to meet sector-specific challenges. Hence, the final idea builds on underlying SMT education principles and bridges gaps through an integrative, applied pedagogy that directly addresses the research limitation.",
      "rationale": "### Step 1: Compatibility Analysis\nThe proposed SMT syllabus is well-aligned with the core objectives of the limitation. It is designed specifically for translation students, aiming to provide theoretical understanding and practical experience in SMT. The mathematical and algorithmic foundations of SMT are inherently compatible, as they form the basis of probabilistic models and language pair alignments that students need to learn. The syllabus covers these areas adequately. However, a significant requirement of the limitationâ€”aligning SMT with employability and professional developmentâ€”is not fully addressed in the initial proposal. This suggests an opportunity for enhancement rather than incompatibility, supporting the conclusion that the candidate method is fundamentally compatible.\n\n### Step 2: Identify the Gap\nThe syllabus lacks explicit components that connect abstract SMT concepts to professional and employability-focused outcomes. The bridging variable, therefore, is the introduction of experiential learning elements, such as case studies, real-world translation tasks, and employability modules. Drawing from the convergence pattern seen in Evolutionary Path 1, where independent methodologies were integrated effectively, incorporating these elements refines the syllabus to not only teach SMT but also prepare students for professional application. This ensures that the syllabus meets the dual need for academic rigor and practical relevance.\n\n### Step 3: Draft the Idea\nBuilding on the identified gap, the revised syllabus is reimagined as an integrative pedagogy. The title \"Integrative Pedagogy for Statistical Machine Translation: Bridging Technology and Translation Education\" emphasizes the innovation's dual focus. The abstract clearly communicates the limitations and proposed solution, highlighting how experiential learning modules connect theoretical knowledge with employability. Drawing inspiration from evolutionary patterns of integrating multidisciplinary approaches, the method is tailored to address the new problem context holistically.\n\n### Decision Chain\nLimitation â†’ Compatibility (syllabus aligns but lacks practical-professional connection) â†’ Gap (need for experiential, employability-oriented modules) â†’ Solution (revised integrative pedagogy) â†’ Feasibility (proposed innovation directly addresses the identified gap).\n\n### Evidence\nThe notion of integrating case studies and new applications draws directly from the evolutionary convergence pattern, where multiple methodologies were harmonized to address broader challenges. This supports the feasibility and relevance of the integrative approach for translating abstract SMT concepts into professional competence development.\n\nBy following a structured reasoning process and leveraging evolutionary insight, the proposal successfully adapts the candidate method to address the research limitation comprehensively."
    },
    {
      "limitation": "- Explore how SMT can be effectively integrated into diverse translation studies curricula, addressing the gap identified in incorporating Statistical Machine Translation holistically.\n- Develop and test new instructional methods or tools to better align SMT training with the capabilities and needs of translation students.\n- Investigate the impact of SMT-focused training on the professional development and employability of translation students in real-world scenarios.",
      "method": "The introduction and use of Vector Space Models (VSMs) for semantic processing of text, which encode word semantics as mathematical structures in vector spaces.\n\n**Explanation:** VSMs address the issue by representing words or phrases as vectors in a multi-dimensional space based on their distributional properties in language. This mathematical encoding allows computers to capture and reason about semantic similarity, relationships, and implicit meanings in text, enabling tasks like following instructions, explaining actions, and processing text more effectively. By shifting the problem of language understanding into a measurable vector space, VSMs provide a scalable and interpretable framework for semantic representation.",
      "status": "SUCCESS",
      "title": "Integrating Vector Space Models into Statistical Machine Translation Curricula: Bridging Semantic Representation and Pedagogical Training",
      "abstract": "Background: Current research in translation studies highlights the need to holistically integrate Statistical Machine Translation (SMT) into curricula to better align training with the real-world needs of translation students. However, challenges persist in crafting effective instructional methods that incorporate SMT tools and enhance professional employability. Gap: The key limitation lies in enabling students to grasp advanced semantic processing concepts underpinned by SMT systems while considering their practical applications in varying translation contexts. Proposed Method: To address this gap, we propose integrating Vector Space Models (VSMs) into SMT instruction, as VSMs enable structured semantic analysis by representing linguistic elements in a measurable vector space. This approach offers a scalable and interpretable pathway for discussing SMT system mechanisms, semantic similarity, and contextual representations, connecting theoretical knowledge with practical translation tasks. Expected Results: By embedding VSM-based semantic representation insights into SMT curricula, the approach can foster deeper student understanding of underlying translation mechanisms, enhance instructional efficacy, and improve employability through professional readiness in handling advanced translation tools.",
      "modification": "Align Vector Space Models (VSMs) with SMT-specific translation tasks through the development of pedagogical frameworks that leverage semantic representation in vector spaces to illustrate SMT mechanisms within diverse translation contexts.",
      "reasoning": "Step 1: Compatibility Analysis: VSMs exhibit the capability to model word semantics quantitatively, aligning well with the core functionality of SMT systems, which depend on probabilistic models for translation generation. VSM's ability to process contextual and semantic relationships is directly compatible with SMT workflows that emphasize semantic accuracy and fluency. Furthermore, VSM scalability allows integration into classroom settings via educational tools like interactive labs or visualizations to connect theoretical principles with SMT system insights. Thus, the fundamental mathematical properties of VSMs are ideally compatible with the objectives of advancing SMT-focused curricula. Step 2: Gap Identification: The critical gap lies in adapting VSMsâ€”a general-purpose semantic representation frameworkâ€”to translational learning environments. To bridge this, creating targeted pedagogical resources such as visual simulations or analytical exercises demonstrating semantic representation in SMT workflows is necessary. The bridging variable here is the alignment of VSM functionality (vector-based semantic representation) with instructional design focused on explaining nuanced SMT operation principles. Step 3: Idea Drafting: By learning from patterns of cross-domain adaptations ('Route 1 Divergence' from Evolutionary Paths), integrating VSM concepts provides an innovative pedagogical mechanism for teaching SMT. It builds on VSM's proven ability to concretize abstract semantic relationships by extending its application to curriculum design, fulfilling the unmet need for effective instructional methods. Reasoning: The structured chain of thought has demonstrated compatibility in Step 1, identified the required modification in Step 2, and extended the method to the limitation in Step 3, substantiated by evolutionary adaptation logic like migration to new fields. This complete narrative links the SMT curriculum gap with semantic innovation from VSMs, yielding a viable solution.",
      "rationale": "The provided limitation demands a holistic solution to the integration of SMT into translation studies curricula, emphasizing the need for effective pedagogical methods and tools. Vector Space Models (VSMs), with their ability to encode semantic information in vector space, offer scalable and interpretable frameworks suitable for discussing SMT system mechanisms. Step 1 analyzed compatibility and affirmed that VSM's mathematical properties align well with SMT's semantic requirements. Step 2 identified the gap as the lack of direct educational alignment between VSM's semantic representation functions and SMT instructional needs, to which a bridging variableâ€”the alignment frameworkâ€”was proposed. Step 3 drafted a feasible idea that leverages VSM capabilities and extends their application to educational frameworks in translation. Evolutionary paths demonstrate how adaptation and extension to new domains have been successfully applied in past research endeavors, providing both inspiration and validation for transferring VSM functionalities into pedagogical applications for SMT. This reasoning pathway validates the feasibility of the proposed idea to address the limitation comprehensively."
    },
    {
      "limitation": "- Explore how SMT can be effectively integrated into diverse translation studies curricula, addressing the gap identified in incorporating Statistical Machine Translation holistically.\n- Develop and test new instructional methods or tools to better align SMT training with the capabilities and needs of translation students.\n- Investigate the impact of SMT-focused training on the professional development and employability of translation students in real-world scenarios.",
      "method": "Creating positive classroom experiences tailored to enhancing self-efficacy for computer technologies.\n\n**Explanation:** The solution leverages structured positive interactions and experiences within the classroom to boost students' belief in their ability to use computer technologies effectively. By designing these experiences intentionally, students are encouraged through supportive pedagogy, interactive learning, and low-stress challenges, which help build confidence and competence progressively. Positive reinforcement and contextual learning further ensure sustained self-efficacy growth.",
      "status": "SUCCESS",
      "title": "Redefining SMT Curricula: Enhancing Translation Pedagogy through Positive Classroom Experiences and Self-Efficacy for Computational Tools",
      "abstract": "Background: The field of translation studies faces challenges in integrating Statistical Machine Translation (SMT) into curricula, particularly in aligning instructional methods with translation students' needs and fostering their employability. Gap: Current approaches lack emphasis on building self-efficacy and situating SMT training within supportive, interactive classroom experiences designed to address both technical competencies and psychological barriers. Proposed Method: This study proposes a methodology for incorporating structured positive classroom experiences aimed at enhancing self-efficacy for computational tools in SMT training. By integrating confidence-building activities, interactive pedagogies, and context-driven learning, this approach seeks to holistically support students in acquiring SMT competencies. Expected Result: The proposed method is anticipated to bridge the integration gap via improved student engagement, competency growth, and real-world readiness for translation roles involving SMT technologies.",
      "modification": "Adapt the candidate method to focus specifically on Statistical Machine Translation training by designing task-specific positive classroom experiences. This involves tailoring experiential learning steps (e.g., interactive SMT debugging, real-world corpus translation tasks, peer-led workshops) that build both technical skills and confidence progressively.",
      "reasoning": "Step 1 Analysis: Examining the candidate method (positive classroom experiences tailored for self-efficacy improvement), its algorithmic and theoretical properties align well with the limitation. Self-efficacy in computational tools is a critical enabler of effective SMT learning, as many translation students face barriers due to limited prior exposure to advanced computational systems. The constraints of the limitation include holistic integration into curricula and practical real-world applicability, both of which can be addressed through structured classroom experiences that reinforce confidence and competence. Computational complexity is not a concern here since the method focuses on pedagogy rather than algorithmic optimization. Status here: Compatible. Step 2 Analysis: The gap involves the need for domain-specific tuning of the candidate method to address SMT training. A bridging variable is identified: experiential learning activities tailored to SMT (e.g., corpus-level translation exercises integrated into student-friendly software platforms, peer-guided collaborative debugging of SMT tools). These elements would promote both skill acquisition and psychological readiness, solving the identified instructional gap. Drawing from Evolutionary Path 1, which demonstrates successful convergence of complementary methods, the core insight is that SMT training needs both technical rigor and pedagogical user-friendlinessâ€”achieved via tailoring positive classroom experiences explicitly to SMT training contexts. Step 3 Analysis: Building on insights from Divergence patterns (Evolutionary Paths 2 and 3), the innovative adjustment involves creating task-specific learning modules that blend technical challenges with psychological empowerment strategies. Task-based workshops and gamified SMT exercises are key elements for bridging curricular integration gaps. Academic inspiration from Path 1's convergence highlights the importance of synthesizing diverse influences (psychological learning theories + SMT training contexts) into a cohesive pedagogical model.",
      "rationale": "The reasoning begins with an analysis of the candidate methodâ€”a pedagogy based on positive classroom experiences designed to enhance self-efficacy in computational tool usage. This method has clear compatibility with the limitation, given the need for students to engage with SMT tools confidently and effectively in translation curricula. The limitation's constraints, including holistic integration and real-world applicability, align well with the theoretical underpinnings of enhancing self-efficacy via structured and supportive classroom activities. Compatibility is established due to the capacity of these pedagogical interventions to address psychological and practical barriers in SMT learning without computational overhead. Moving to gap identification, the required adaptations involve situating this method within domain-specific contextsâ€”SMT trainingâ€”including designing experiential modules that gradually enhance both skills and confidence. Here, the bridging variable is the reorientation of the method from general self-efficacy improvements to specific SMT-focused experiences, incorporating interactive, low-stress, and task-specific pedagogies. Drawing from Evolutionary Path 1, convergence logic suggests that blending psychological reinforcement with SMT competency-building yields a cohesive educational pathway. Lastly, the idea synthesis pulls from convergence and divergence patterns to create a structured proposal titled \"Redefining SMT Curricula: Enhancing Translation Pedagogy through Positive Classroom Experiences and Self-Efficacy for Computational Tools.\" By embedding foundational self-efficacy pedagogies into SMT-specific training activities, the approach is poised to improve student engagement, technical competencies, and real-world readiness. The reasoning chain incorporates alignment checks, domain-specific adjustments, insights from evolutionary paths, and innovative synthesis into a well-structured research proposal."
    }
  ],
  "pools": {
    "unsolved_limitations": 27,
    "candidate_methods": 41
  }
};

                // æŒ‰ç±»å‹åˆ†ç»„è¾¹æ•°æ®
                const edgesByType = {};
                edgesData.forEach(edge => {
                    if (!edgesByType[edge.type]) {
                        edgesByType[edge.type] = [];
                    }
                    edgesByType[edge.type].push(edge);
                });

                // åˆ›å»ºèŠ‚ç‚¹è½¨è¿¹
                const nodeTrace = {
                    x: nodesData.map(n => n.x),
                    y: nodesData.map(n => n.y),
                    mode: 'markers+text',
                    marker: {
                        size: nodesData.map(n => n.size),
                        color: nodesData.map(n => n.color),
                        line: { width: 2, color: 'white' },
                        colorscale: 'Viridis'
                    },
                    text: nodesData.map(n => n.label),
                    textposition: 'middle center',
                    textfont: { size: 10, color: 'black' },
                    customdata: nodesData,
                    hovertemplate: '<b>%{customdata.title}</b><extra></extra>',
                    type: 'scatter',
                    name: 'è®ºæ–‡èŠ‚ç‚¹'
                };

                // å›¾è¡¨å¸ƒå±€é…ç½®
                const layout = {
                    title: '',
                    showlegend: false,
                    hovermode: 'closest',
                    margin: { l: 0, r: 0, b: 40, t: 0 },
                    xaxis: {
                        title: 'å‘è¡¨å¹´ä»½',
                        showgrid: true,
                        gridcolor: 'lightgray',
                        range: [1971, 2025]
                    },
                    yaxis: {
                        title: 'è®ºæ–‡åˆ†å¸ƒ',
                        showgrid: true,
                        gridcolor: 'lightgray',
                        showticklabels: false
                    },
                    plot_bgcolor: 'white',
                    paper_bgcolor: 'white'
                };

                // ========== å·¥å…·å‡½æ•° ==========
                // åˆ›å»ºè¾¹è½¨è¿¹ï¼ˆé€šç”¨å‡½æ•°ï¼Œæ¶ˆé™¤é‡å¤é€»è¾‘ï¼‰
                function createEdgeTraces(styleMap) {
                    const traces = [];
                    Object.keys(edgesByType).forEach(type => {
                        const edges = edgesByType[type];
                        const style = styleMap.get(type) || {
                            color: edges[0].color,
                            width: edges[0].width,
                            dash: 'solid'
                        };

                        const edgeX = [];
                        const edgeY = [];

                        edges.forEach(edge => {
                            const fromNode = nodesData.find(n => n.id === edge.from);
                            const toNode = nodesData.find(n => n.id === edge.to);
                            if (fromNode && toNode) {
                                edgeX.push(fromNode.x, toNode.x, null);
                                edgeY.push(fromNode.y, toNode.y, null);
                            }
                        });

                        if (edgeX.length > 0) {
                            traces.push({
                                x: edgeX,
                                y: edgeY,
                                mode: 'lines',
                                line: {
                                    width: style.width,
                                    color: style.color,
                                    dash: style.dash
                                },
                                hoverinfo: 'none',
                                showlegend: false,
                                type: 'scatter'
                            });
                        }
                    });
                    return traces;
                }

                // æ›´æ–°å›¾è¡¨ï¼ˆé€šç”¨å‡½æ•°ï¼‰
                function updateGraph(edgeTraces, nodeColors, nodeLineStyle = null) {
                    const nodeUpdate = {
                        ...nodeTrace,
                        marker: {
                            ...nodeTrace.marker,
                            color: nodeColors,
                            line: nodeLineStyle || { width: 2, color: 'white' }
                        }
                    };

                    Plotly.react('graph', [...edgeTraces, nodeUpdate], layout);
                }

                // ========== åˆå§‹åŒ–å›¾è¡¨ ==========
                // åˆ›å»ºåˆå§‹è¾¹æ ·å¼ï¼ˆä½¿ç”¨åŸå§‹å®šä¹‰çš„é¢œè‰²ï¼‰
                const initialEdgeStyle = new Map();
                Object.keys(edgesByType).forEach(type => {
                    const firstEdge = edgesByType[type][0];
                    initialEdgeStyle.set(type, {
                        color: firstEdge.original_color,
                        width: firstEdge.width,
                        dash: firstEdge.dash
                    });
                });

                const initialEdgeTraces = createEdgeTraces(initialEdgeStyle);
                updateGraph(initialEdgeTraces, nodesData.map(n => n.color));

                // ========== äº‹ä»¶å¤„ç†å™¨ ==========
                document.getElementById('graph').on('plotly_click', function(data) {
                    if (data.points?.[0]?.customdata) {
                        const node = data.points[0].customdata;
                        const nodeIndex = data.points[0].pointIndex;
                        showPaperDetails(node);
                        highlightClickedNodeAndEdges(nodeIndex, node);
                    }
                });


                // ========== åŠŸèƒ½å‡½æ•° ==========
                // æ ‡ç­¾é¡µåˆ‡æ¢å‡½æ•°
                function switchTab(event, tabId) {
                    // ç§»é™¤æ‰€æœ‰activeç±»
                    document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
                    document.querySelectorAll('.tab-content').forEach(content => content.classList.remove('active'));

                    // æ·»åŠ activeç±»åˆ°å½“å‰æ ‡ç­¾
                    event.target.classList.add('active');
                    document.getElementById(tabId).classList.add('active');

                    // åˆ‡æ¢åˆ°"è®ºæ–‡è¯¦æƒ…"æˆ–"ç§‘ç ”åˆ›æ„"æ ‡ç­¾æ—¶ï¼Œé‡ç½®å›¾è°±é«˜äº®
                    if (tabId === 'paper-tab' || tabId === 'ideas-tab') {
                        resetGraphHighlight();
                        console.log(`åˆ‡æ¢åˆ° ${tabId}ï¼Œå·²é‡ç½®å›¾è°±é«˜äº®`);
                    }

                    // æ ¹æ®æ ‡ç­¾é¡µIDåŠ è½½ç›¸åº”å†…å®¹
                    if (tabId === 'survey-tab') {
                        renderDeepSurvey();
                    } else if (tabId === 'ideas-tab') {
                        renderResearchIdeas();
                    }
                }

                // æ¸²æŸ“æ·±åº¦ç»¼è¿° (æ–°ç‰ˆæ•°æ®ç»“æ„)
                function renderDeepSurvey() {
                    const surveyTab = document.getElementById('survey-tab');

                    if (!deepSurveyData || Object.keys(deepSurveyData).length === 0) {
                        surveyTab.innerHTML = '<div class="placeholder">æš‚æ— æ·±åº¦ç»¼è¿°æ•°æ®</div>';
                        return;
                    }

                    let html = '<div style="padding:20px;">';

                    // æ‘˜è¦ä¿¡æ¯ (æ–°ç»“æ„)
                    if (deepSurveyData.summary) {
                        html += `
                            <div class="stats">
                                <h4 style="margin-top:0;">ğŸ“Š ç»¼è¿°æ‘˜è¦</h4>
                                <div class="stat-item"><span>åŸå§‹è®ºæ–‡:</span><span>${deepSurveyData.summary.original_papers || 0} ç¯‡</span></div>
                                <div class="stat-item"><span>ç­›é€‰åè®ºæ–‡:</span><span>${deepSurveyData.summary.pruned_papers || 0} ç¯‡</span></div>
                                <div class="stat-item"><span>æ¼”åŒ–æ•…äº‹çº¿:</span><span>${deepSurveyData.summary.total_threads || 0} æ¡</span></div>
                            </div>
                        `;
                    }

                    // å‰ªæç»Ÿè®¡ä¿¡æ¯
                    if (deepSurveyData.pruning_stats) {
                        const stats = deepSurveyData.pruning_stats;
                        const retentionRate = (stats.retention_rate * 100).toFixed(1);
                        html += `
                            <div class="stats" style="margin-top:15px; background:#fff3cd;">
                                <h4 style="margin-top:0;">âœ‚ï¸ å›¾è°±å‰ªæç»Ÿè®¡</h4>
                                <div class="stat-item"><span>ä¿ç•™ç‡:</span><span>${retentionRate}%</span></div>
                                <div class="stat-item"><span>ç§å­è®ºæ–‡:</span><span>${stats.seed_papers || 0} ç¯‡</span></div>
                                <div class="stat-item"><span>å¼ºå…³ç³»è¾¹:</span><span>${stats.strong_edges || 0} æ¡</span></div>
                                <div class="stat-item"><span>å‰”é™¤å¼±å…³ç³»è¾¹:</span><span>${stats.weak_edges_removed || 0} æ¡</span></div>
                            </div>
                        `;
                    }

                    // æ¼”åŒ–è·¯å¾„ (Threads)
                    const threads = deepSurveyData.survey_report?.threads || deepSurveyData.evolutionary_paths || [];
                    if (threads.length > 0) {
                        html += `
                            <div style="display:flex; justify-content:space-between; align-items:center; margin-top:20px;">
                                <h3 style="color:#2c3e50; margin:0;">ğŸ§µ å…³é”®æ¼”åŒ–æ•…äº‹çº¿</h3>
                                <button id="resetGraphBtn" onclick="resetGraphHighlight()"
                                    style="padding:5px 12px; background:#6c757d; color:white; border:none; border-radius:4px; cursor:pointer; font-size:12px;">
                                    ğŸ”„ é‡ç½®å›¾è°±
                                </button>
                            </div>
                        `;
                        threads.forEach((thread, index) => {
                            const threadTitle = thread.title || thread.thread_name || `Thread ${index + 1}`;
                            const patternType = thread.pattern_type || thread.thread_type || 'æœªçŸ¥ç±»å‹';
                            const paperCount = thread.papers ? thread.papers.length : 0;
                            const narrative = thread.narrative || 'æš‚æ— å™äº‹æ–‡æœ¬';

                            // å®šä¹‰ä¸°å¯Œçš„é¢œè‰²è°ƒè‰²æ¿ï¼ˆæŒ‰æ•…äº‹çº¿ç´¢å¼•åˆ†é…ï¼‰
                            const colorPalette = [
                                '#E74C3C',  // çº¢è‰² - Thread 0
                                '#3498DB',  // è“è‰² - Thread 1
                                '#2ECC71',  // ç»¿è‰² - Thread 2
                                '#F39C12',  // æ©™è‰² - Thread 3
                                '#9B59B6',  // ç´«è‰² - Thread 4
                                '#1ABC9C',  // é’è‰² - Thread 5
                                '#E67E22',  // æ·±æ©™è‰² - Thread 6
                                '#95A5A6',  // ç°è‰² - Thread 7
                                '#34495E',  // æ·±è“ç° - Thread 8
                                '#16A085'   // æ·±é’è‰² - Thread 9
                            ];

                            // æ ¹æ®æ•…äº‹çº¿ç´¢å¼•åˆ†é…é¢œè‰²ï¼ˆä¿è¯æ¯æ¡æ•…äº‹çº¿é¢œè‰²å”¯ä¸€ï¼‰
                            let borderColor = colorPalette[index % colorPalette.length];
                            let highlightColor = borderColor;

                            // æ”¶é›†è¯¥æ•…äº‹çº¿çš„æ‰€æœ‰è®ºæ–‡ID
                            const paperIds = thread.papers ? thread.papers.map(p => p.paper_id) : [];

                            html += `
                                <div class="epoch-card" style="border-left-color:${borderColor}; cursor:pointer; transition:all 0.3s;"
                                     onclick="highlightThread(${index}, '${highlightColor}')"
                                     onmouseover="this.style.backgroundColor='#f8f9fa'"
                                     onmouseout="this.style.backgroundColor='white'">
                                    <h4>
                                        ${threadTitle}
                                        <span style="float:right; font-size:12px; color:#666; font-weight:normal;">
                                            ${patternType}
                                        </span>
                                    </h4>
                                    <p><strong>ğŸ“š è®ºæ–‡æ•°é‡:</strong> ${paperCount} ç¯‡</p>
                                    ${thread.total_citations ? `<p><strong>ğŸ“Š æ€»å¼•ç”¨æ•°:</strong> ${thread.total_citations}</p>` : ''}
                                    <p style="font-size:11px; color:#666; margin-top:8px;">
                                        ğŸ’¡ <em>ç‚¹å‡»æ­¤å¡ç‰‡å¯åœ¨å·¦ä¾§å›¾è°±ä¸­é«˜äº®æ˜¾ç¤ºè¯¥æ•…äº‹çº¿çš„æ‰€æœ‰è®ºæ–‡</em>
                                    </p>

                                    <details style="margin-top:10px;">
                                        <summary style="cursor:pointer; color:#3498DB; font-weight:bold;">ğŸ“– æŸ¥çœ‹æ¼”åŒ–å™äº‹</summary>
                                        <div style="margin-top:10px; padding:10px; background:#f8f9fa; border-radius:5px; line-height:1.6; white-space:pre-wrap;">
                                            ${narrative}
                                        </div>
                                    </details>

                                    ${thread.papers && thread.papers.length > 0 ? `
                                        <details style="margin-top:10px;">
                                            <summary style="cursor:pointer; color:#2ECC71; font-weight:bold;">ğŸ“„ æŸ¥çœ‹è®ºæ–‡åˆ—è¡¨</summary>
                                            <ul style="margin-top:10px; padding-left:20px;">
                                                ${thread.papers.map((p, pIndex) => `
                                                    <li style="margin:5px 0; cursor:pointer; color:#2980b9; transition:color 0.2s;"
                                                        onclick="event.stopPropagation(); showPaperFromThread('${p.paper_id}');"
                                                        onmouseover="this.style.color='#3498db'; this.style.textDecoration='underline';"
                                                        onmouseout="this.style.color='#2980b9'; this.style.textDecoration='none';"
                                                        title="ç‚¹å‡»æŸ¥çœ‹è¯¥è®ºæ–‡è¯¦æƒ…å¹¶åœ¨å›¾è°±ä¸­é«˜äº®">
                                                        <strong>${p.title}</strong>
                                                        (${p.year || 'N/A'}, å¼•ç”¨: ${p.cited_by_count || 0})
                                                    </li>
                                                `).join('')}
                                            </ul>
                                        </details>
                                    ` : ''}
                                </div>
                            `;
                        });
                    }

                    // ç»¼è¿°æŠ¥å‘Šæ‘˜è¦
                    if (deepSurveyData.survey_report?.abstract) {
                        html += `
                            <div style="margin-top:20px; padding:15px; background:#e8f4f8; border-left:4px solid #3498DB; border-radius:5px;">
                                <h4 style="margin:0 0 10px 0; color:#2c3e50;">ğŸ“ ç»¼è¿°æ‘˜è¦</h4>
                                <p style="line-height:1.6; color:#333; margin:0;">${deepSurveyData.survey_report.abstract}</p>
                            </div>
                        `;
                    }

                    html += '</div>';
                    surveyTab.innerHTML = html;
                }

                // æ¸²æŸ“ç§‘ç ”åˆ›æ„
                function renderResearchIdeas() {
                    const ideasTab = document.getElementById('ideas-tab');

                    if (!researchIdeasData || Object.keys(researchIdeasData).length === 0) {
                        ideasTab.innerHTML = '<div class="placeholder">æš‚æ— ç§‘ç ”åˆ›æ„æ•°æ®</div>';
                        return;
                    }

                    let html = '<div style="padding:20px;">';

                    // ç»Ÿè®¡ä¿¡æ¯
                    html += `
                        <div class="stats">
                            <h4 style="margin-top:0;">ğŸ’¡ åˆ›æ„ç”Ÿæˆç»Ÿè®¡</h4>
                            <div class="stat-item"><span>æ€»åˆ›æ„æ•°:</span><span>${researchIdeasData.total_ideas || 0}</span></div>
                            <div class="stat-item"><span>å¯è¡Œåˆ›æ„:</span><span>${researchIdeasData.successful_ideas || 0}</span></div>
                            ${researchIdeasData.pools ? `
                                <div class="stat-item"><span>æœªè§£å†³é™åˆ¶:</span><span>${researchIdeasData.pools.unsolved_limitations || 0}</span></div>
                                <div class="stat-item"><span>å€™é€‰æ–¹æ³•:</span><span>${researchIdeasData.pools.candidate_methods || 0}</span></div>
                            ` : ''}
                        </div>
                    `;

                    // åˆ›æ„åˆ—è¡¨
                    if (researchIdeasData.ideas && researchIdeasData.ideas.length > 0) {
                        html += '<h3 style="color:#2c3e50; margin-top:20px;">ğŸ’¡ ç ”ç©¶åˆ›æ„åˆ—è¡¨</h3>';
                        researchIdeasData.ideas.forEach((idea, index) => {
                            const statusClass = idea.status === 'SUCCESS' ? 'status-success' : 'status-incompatible';
                            const statusText = idea.status === 'SUCCESS' ? 'âœ“ å¯è¡Œ' : 'âœ— ä¸å…¼å®¹';

                            html += `
                                <div class="idea-card">
                                    <h4>
                                        åˆ›æ„ ${index + 1}: ${idea.title || 'æœªå‘½ååˆ›æ„'}
                                        <span class="status-badge ${statusClass}">${statusText}</span>
                                    </h4>
                                    ${idea.abstract ? `
                                        <p style="margin:10px 0; line-height:1.6; color:#444;">
                                            <strong>æ‘˜è¦:</strong> ${idea.abstract}
                                        </p>
                                    ` : ''}
                                    ${idea.modification ? `
                                        <p style="margin:8px 0; padding:10px; background:#f8f9fa; border-radius:4px;">
                                            <strong>ğŸ”§ å…³é”®åˆ›æ–°:</strong> ${idea.modification}
                                        </p>
                                    ` : ''}
                                    ${idea.reasoning ? `
                                        <details style="margin-top:10px;">
                                            <summary style="cursor:pointer; color:#3498DB;"><strong>æŸ¥çœ‹æ¨ç†è¿‡ç¨‹</strong></summary>
                                            <p style="margin-top:8px; font-size:12px; color:#666; white-space:pre-wrap;">${idea.reasoning}</p>
                                        </details>
                                    ` : ''}
                                </div>
                            `;
                        });
                    }

                    html += '</div>';
                    ideasTab.innerHTML = html;
                }

                function showPaperDetails(node) {
                    const authorsText = node.authors.slice(0, 5).join(', ') +
                                      (node.authors.length > 5 ? ' ç­‰' : '');

                    // æ„å»ºRAGåˆ†æéƒ¨åˆ†çš„HTML
                    let ragAnalysisHTML = '';
                    if (node.rag_problem || node.rag_method || node.rag_limitation || node.rag_future_work) {
                        ragAnalysisHTML = `
                            <div class="paper-info" style="background:#e8f4f8; padding:15px; border-radius:8px; margin-top:15px;">
                                <h4 style="margin:0 0 10px 0; color:#1a73e8; font-size:15px;">ğŸ§  å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ·±åº¦åˆ†æ</h4>
                                ${node.analysis_method ? `<p style="font-size:12px; color:#666; margin-bottom:10px;"><strong>åˆ†ææ–¹æ³•:</strong> ${node.analysis_method.toUpperCase()}</p>` : ''}
                                ${node.sections_extracted ? `<p style="font-size:12px; color:#666; margin-bottom:10px;"><strong>æå–ç« èŠ‚:</strong> ${node.sections_extracted} ä¸ª</p>` : ''}
                            </div>
                            ${node.rag_problem ? `
                            <div class="paper-info" style="border-left:3px solid #FF6B6B; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#FF6B6B; font-size:14px;">ğŸ“‹ ç ”ç©¶é—®é¢˜ (Problem)</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_problem}</p>
                            </div>
                            ` : ''}
                            ${node.rag_method ? `
                            <div class="paper-info" style="border-left:3px solid #4ECDC4; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#4ECDC4; font-size:14px;">ğŸ’¡ æ ¸å¿ƒæ–¹æ³• (Method)</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_method}</p>
                            </div>
                            ` : ''}
                            ${node.rag_limitation ? `
                            <div class="paper-info" style="border-left:3px solid #FFA500; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#FFA500; font-size:14px;">âš ï¸ å±€é™æ€§ (Limitation)</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_limitation}</p>
                            </div>
                            ` : ''}
                            ${node.rag_future_work ? `
                            <div class="paper-info" style="border-left:3px solid #9B59B6; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#9B59B6; font-size:14px;">ğŸ”® æœªæ¥å·¥ä½œ (Future Work)</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_future_work}</p>
                            </div>
                            ` : ''}
                        `;
                    }

                    document.getElementById('paper-tab').innerHTML = `
                        <div class="stats">
                            <h4 style="margin-top:0;">å›¾è°±ç»Ÿè®¡</h4>
                            <div class="stat-item"><span>è®ºæ–‡æ€»æ•°:</span><span>44</span></div>
                            <div class="stat-item"><span>å¼•ç”¨å…³ç³»:</span><span>50</span></div>
                            <div class="stat-item"><span>æ—¶é—´è·¨åº¦:</span><span>1972 - 2024</span></div>
                        </div>
                        <div class="paper-info">
                            <h3>${node.title}</h3>
                            <p><strong>ä½œè€…:</strong> ${authorsText}</p>
                            <p><strong>å¹´ä»½:</strong> ${node.year}</p>
                            <p><strong>å¼•ç”¨æ•°:</strong> ${node.cited_by_count}</p>
                            <p><strong>æœŸåˆŠ/ä¼šè®®:</strong> ${node.venue || 'æœªçŸ¥'}</p>
                            <p><strong>è®ºæ–‡ID:</strong> ${node.id}</p>
                        </div>
                        ${ragAnalysisHTML}
                    `;
                }

                function highlightClickedNodeAndEdges(nodeIndex, node) {
                    // åªæ”¹å˜èŠ‚ç‚¹é¢œè‰² - è¢«ç‚¹å‡»çš„èŠ‚ç‚¹é«˜äº®
                    const nodeColors = nodesData.map((n, i) =>
                        i === nodeIndex ? '#FF4444' : n.color);

                    // è¾¹ä¿æŒåŸå§‹æ ·å¼ä¸å˜
                    updateGraph(initialEdgeTraces, nodeColors);
                }

                // ========== æ¼”åŒ–æ•…äº‹çº¿é«˜äº®åŠŸèƒ½ ==========
                function highlightThread(threadIndex, highlightColor) {
                    // è·å–çº¿ç¨‹æ•°æ®
                    const threads = deepSurveyData.survey_report?.threads || deepSurveyData.evolutionary_paths || [];
                    if (threadIndex >= threads.length) return;

                    const thread = threads[threadIndex];
                    const threadPaperIds = new Set(thread.papers?.map(p => p.paper_id) || []);

                    console.log(`é«˜äº® Thread ${threadIndex}: ${thread.title}, åŒ…å« ${threadPaperIds.size} ç¯‡è®ºæ–‡`);

                    // æ›´æ–°èŠ‚ç‚¹é¢œè‰²å’Œå¤§å°
                    const newColors = [];
                    const newSizes = [];
                    const newLineStyles = [];

                    nodesData.forEach((node, index) => {
                        if (threadPaperIds.has(node.id)) {
                            // é«˜äº®æ˜¾ç¤ºï¼šä¿æŒèŠ‚ç‚¹åŸæœ¬é¢œè‰²ï¼Œæ”¾å¤§1.5å€
                            newColors.push(node.color);
                            newSizes.push(node.size * 1.5);
                            newLineStyles.push({ width: 3, color: node.color });
                        } else {
                            // å…¶ä»–èŠ‚ç‚¹ï¼šå˜ç°ï¼Œç¼©å°åˆ°0.5å€
                            newColors.push('#D3D3D3');
                            newSizes.push(node.size * 0.5);
                            newLineStyles.push({ width: 1, color: '#CCCCCC' });
                        }
                    });

                    // æ›´æ–°å›¾è°±
                    const highlightedNodeTrace = {
                        ...nodeTrace,
                        marker: {
                            ...nodeTrace.marker,
                            size: newSizes,
                            color: newColors,
                            line: newLineStyles
                        }
                    };

                    // è¾¹ä¹Ÿè°ƒæ•´é€æ˜åº¦ï¼ˆé«˜äº®æ•…äº‹çº¿å†…çš„è¾¹ï¼‰
                    const highlightedEdgeTraces = createEdgeTracesWithHighlight(threadPaperIds, highlightColor);

                    Plotly.react('graph', [...highlightedEdgeTraces, highlightedNodeTrace], layout);

                    // æ»šåŠ¨å›¾è°±åˆ°é«˜äº®åŒºåŸŸ
                    document.getElementById('graph').scrollIntoView({ behavior: 'smooth', block: 'center' });
                }

                function createEdgeTracesWithHighlight(highlightedNodeIds, highlightColor) {
                    const traces = [];
                    Object.keys(edgesByType).forEach(type => {
                        const edges = edgesByType[type];
                        const style = initialEdgeStyle.get(type) || {
                            color: edges[0].color,
                            width: edges[0].width,
                            dash: 'solid'
                        };

                        // åˆ†ç¦»é«˜äº®è¾¹å’Œéé«˜äº®è¾¹
                        const highlightedEdgeX = [];
                        const highlightedEdgeY = [];
                        const dimmedEdgeX = [];
                        const dimmedEdgeY = [];

                        edges.forEach(edge => {
                            const fromNode = nodesData.find(n => n.id === edge.from);
                            const toNode = nodesData.find(n => n.id === edge.to);
                            if (fromNode && toNode) {
                                // æ£€æŸ¥æ˜¯å¦æ˜¯é«˜äº®æ•…äº‹çº¿çš„è¾¹
                                const isHighlighted = highlightedNodeIds.has(edge.from) && highlightedNodeIds.has(edge.to);

                                if (isHighlighted) {
                                    highlightedEdgeX.push(fromNode.x, toNode.x, null);
                                    highlightedEdgeY.push(fromNode.y, toNode.y, null);
                                } else {
                                    dimmedEdgeX.push(fromNode.x, toNode.x, null);
                                    dimmedEdgeY.push(fromNode.y, toNode.y, null);
                                }
                            }
                        });

                        // æ·»åŠ é«˜äº®è¾¹traceï¼ˆä¿æŒåŸå§‹é¢œè‰²ï¼ŒåªåŠ ç²—ï¼‰
                        if (highlightedEdgeX.length > 0) {
                            traces.push({
                                x: highlightedEdgeX,
                                y: highlightedEdgeY,
                                mode: 'lines',
                                line: {
                                    width: style.width * 1.8,
                                    color: style.color,  // ä½¿ç”¨åŸå§‹é¢œè‰²ï¼Œä¸æ”¹å˜
                                    dash: style.dash
                                },
                                opacity: 1.0,
                                hoverinfo: 'none',
                                showlegend: false,
                                type: 'scatter'
                            });
                        }

                        // æ·»åŠ å˜ç°è¾¹trace
                        if (dimmedEdgeX.length > 0) {
                            traces.push({
                                x: dimmedEdgeX,
                                y: dimmedEdgeY,
                                mode: 'lines',
                                line: {
                                    width: style.width * 0.4,
                                    color: '#E0E0E0',
                                    dash: style.dash
                                },
                                opacity: 0.2,
                                hoverinfo: 'none',
                                showlegend: false,
                                type: 'scatter'
                            });
                        }
                    });
                    return traces;
                }

                function resetGraphHighlight() {
                    console.log('é‡ç½®å›¾è°±é«˜äº®');
                    // æ¢å¤åŸå§‹é¢œè‰²å’Œå¤§å°
                    updateGraph(initialEdgeTraces, nodesData.map(n => n.color));
                }

                // ä»æ·±åº¦ç»¼è¿°çš„è®ºæ–‡åˆ—è¡¨ä¸­ç‚¹å‡»è®ºæ–‡ï¼Œæ˜¾ç¤ºè¯¦æƒ…å¹¶é«˜äº®
                function showPaperFromThread(paperId) {
                    console.log('ä»æ·±åº¦ç»¼è¿°ç‚¹å‡»è®ºæ–‡:', paperId);

                    // æŸ¥æ‰¾å¯¹åº”çš„èŠ‚ç‚¹ç´¢å¼•
                    const nodeIndex = nodesData.findIndex(n => n.id === paperId);

                    if (nodeIndex === -1) {
                        console.warn('æœªåœ¨å›¾è°±ä¸­æ‰¾åˆ°è®ºæ–‡:', paperId);
                        alert('è¯¥è®ºæ–‡ä¸åœ¨å½“å‰æ˜¾ç¤ºçš„å›¾è°±èŠ‚ç‚¹ä¸­');
                        return;
                    }

                    const node = nodesData[nodeIndex];

                    // åˆ‡æ¢åˆ°è®ºæ–‡è¯¦æƒ…æ ‡ç­¾é¡µ
                    const paperTab = document.querySelector('.tab[onclick*="paper-tab"]');
                    if (paperTab) {
                        paperTab.click();
                    }

                    // æ˜¾ç¤ºè®ºæ–‡è¯¦æƒ…
                    showPaperDetails(node);

                    // æŸ¥æ‰¾è¯¥è®ºæ–‡æ‰€å±çš„æ¼”åŒ–æ•…äº‹çº¿
                    const threads = deepSurveyData.survey_report?.threads || deepSurveyData.evolutionary_paths || [];
                    let threadIndex = -1;

                    for (let i = 0; i < threads.length; i++) {
                        const thread = threads[i];
                        const paperIds = thread.papers?.map(p => p.paper_id) || [];
                        if (paperIds.includes(paperId)) {
                            threadIndex = i;
                            break;
                        }
                    }

                    // é«˜äº®æ•´ä¸ªæ•…äº‹çº¿ï¼ˆèŠ‚ç‚¹ä¿æŒåŸè‰²ï¼‰
                    if (threadIndex !== -1) {
                        console.log(`è¯¥è®ºæ–‡å±äºæ•…äº‹çº¿ ${threadIndex}ï¼Œå°†é«˜äº®æ•´ä¸ªæ•…äº‹çº¿`);
                        highlightThread(threadIndex, null);  // ä¼ nullå› ä¸ºä¸å†éœ€è¦highlightColor
                    } else {
                        // å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•…äº‹çº¿ï¼Œå›é€€åˆ°å•èŠ‚ç‚¹é«˜äº®
                        console.log('è¯¥è®ºæ–‡ä¸å±äºä»»ä½•æ•…äº‹çº¿ï¼Œåªé«˜äº®å•ä¸ªèŠ‚ç‚¹');
                        highlightClickedNodeAndEdges(nodeIndex, node);
                    }

                    console.log('å·²æ˜¾ç¤ºè®ºæ–‡è¯¦æƒ…å¹¶é«˜äº®èŠ‚ç‚¹:', node.title);
                }


                function updateHoverPosition(event) {
                    const hoverDiv = document.getElementById('hoverTitle');
                    if (hoverDiv) {
                        hoverDiv.style.left = (event.clientX + 10) + 'px';
                        hoverDiv.style.top = (event.clientY - 30) + 'px';
                    }
                }
            </script>
            
        </body>
        </html>
        