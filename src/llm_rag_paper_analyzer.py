"""
åŸºäºLLMå¢å¼ºçš„RAGè®ºæ–‡åˆ†æå™¨ï¼ˆé‡æ„ç‰ˆï¼‰

ç»“æ„æ¸…æ™°ã€æ˜“äºç†è§£çš„ç‰ˆæœ¬ï¼š
- ä½¿ç”¨ç‹¬ç«‹çš„LLMé…ç½®ç®¡ç†å™¨
- ä½¿ç”¨ç‹¬ç«‹çš„æç¤ºè¯ç®¡ç†å™¨
- æ¸…æ™°çš„æ¨¡å—åˆ’åˆ†
"""

import logging
from typing import Dict, List, Optional
from pathlib import Path
from dataclasses import dataclass

# PDFå¤„ç†
try:
    import PyPDF2
except ImportError:
    PyPDF2 = None

# Embeddingæ¨¡å‹ - sentence-transformers
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    SentenceTransformer = None

# æ•°å€¼è®¡ç®—åº“ - numpy å’Œ sklearnï¼ˆç‹¬ç«‹å¯¼å…¥ï¼Œä¸å— sentence-transformers å½±å“ï¼‰
try:
    import numpy as np
except ImportError:
    np = None

try:
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError:
    cosine_similarity = None

# ç”¨äºæœ¬åœ°æ¨¡å‹åŠ è½½
try:
    import torch
    from transformers import AutoTokenizer, AutoModel
except ImportError:
    torch = None
    AutoTokenizer = None
    AutoModel = None

# ModelScopeï¼ˆå›½å†…é•œåƒï¼‰
try:
    from modelscope.hub.snapshot_download import snapshot_download
except ImportError:
    snapshot_download = None

# æœ¬åœ°æ¨¡å—
try:
    from llm_config import LLMClient, LLMConfig
    from prompt_manager import PromptManager
    from grobid_parser import GrobidPDFParser
except ImportError:
    # å¦‚æœç›´æ¥å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ä»srcæ¨¡å—å¯¼å…¥
    from src.llm_config import LLMClient, LLMConfig
    from src.prompt_manager import PromptManager
    from src.grobid_parser import GrobidPDFParser

logger = logging.getLogger(__name__)


@dataclass
class PaperSection:
    """è®ºæ–‡ç« èŠ‚æ•°æ®ç»“æ„"""
    title: str
    content: str
    page_num: int
    section_type: str


class LLMRAGPaperAnalyzer:
    """
    LLMå¢å¼ºçš„RAGè®ºæ–‡åˆ†æå™¨

    ä¸»è¦åŠŸèƒ½ï¼š
    1. ä»PDFæˆ–æ‘˜è¦ä¸­æå–è®ºæ–‡ç« èŠ‚
    2. ä½¿ç”¨RAGæ£€ç´¢ç›¸å…³å†…å®¹
    3. ä½¿ç”¨LLMç”Ÿæˆé«˜è´¨é‡åˆ†æ
    4. è‡ªåŠ¨æå–å››ä¸ªå…³é”®å­—æ®µï¼šProblem, Contribution, Limitation, Future Work
    """

    def __init__(
        self,
        llm_config_path: str,
        embedding_model: str = "all-MiniLM-L6-v2",
        use_modelscope: bool = True,
        prompts_dir: str = "./prompts",
        max_context_length: int = 3000,
        grobid_url: Optional[str] = None,
        local_model_path: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            llm_config_path: LLMé…ç½®æ–‡ä»¶è·¯å¾„
            embedding_model: Embeddingæ¨¡å‹åç§°
            use_modelscope: æ˜¯å¦ä½¿ç”¨ModelScopeä¸‹è½½æ¨¡å‹
            prompts_dir: æç¤ºè¯æ–‡ä»¶å¤¹è·¯å¾„
            max_context_length: LLMä¸Šä¸‹æ–‡æœ€å¤§é•¿åº¦
            grobid_url: GROBIDæœåŠ¡URLï¼ˆå¯é€‰ï¼Œå¦‚ http://localhost:8070ï¼‰
            local_model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚ ./model/sentence-transformers/all-MiniLM-L6-v2ï¼‰
        """
        logger.info("="*60)
        logger.info("åˆå§‹åŒ–LLM RAGè®ºæ–‡åˆ†æå™¨")
        logger.info("="*60)

        # åŸºæœ¬é…ç½®
        self.embedding_model_name = embedding_model
        self.use_modelscope = use_modelscope
        self.max_context_length = max_context_length
        self.grobid_url = grobid_url
        self.local_model_path = local_model_path

        # åˆå§‹åŒ–GROBIDè§£æå™¨ï¼ˆå¦‚æœæä¾›äº†URLï¼‰
        self.grobid_parser = None
        if grobid_url:
            self._init_grobid_parser()

        # åˆå§‹åŒ–embeddingæ¨¡å‹
        self.embedder = None
        self.use_embeddings = False
        self._init_embedding_model()

        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = self._init_llm_client(llm_config_path)

        # åˆå§‹åŒ–æç¤ºè¯ç®¡ç†å™¨
        self.prompt_manager = PromptManager(prompts_dir)

        # ç« èŠ‚è¯†åˆ«æ¨¡å¼
        self.section_patterns = self._get_section_patterns()

        # è¦æå–çš„å­—æ®µ
        self.extraction_fields = ['problem', 'method', 'limitation', 'future_work']

        logger.info("="*60)
        logger.info("âœ… LLM RAGè®ºæ–‡åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info("="*60)

    def _init_embedding_model(self):
        """åˆå§‹åŒ–Embeddingæ¨¡å‹"""
        # æ£€æŸ¥ sentence-transformers æ˜¯å¦å®‰è£…
        if SentenceTransformer is None:
            logger.warning("âš ï¸ sentence-transformersæœªå®‰è£…ï¼Œå°†ä½¿ç”¨çº¯å…³é”®è¯æ£€ç´¢")
            logger.warning("   å®‰è£…å‘½ä»¤: pip install sentence-transformers")
            self.use_embeddings = False
            return

        # å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹
        if not self.local_model_path:
            self.local_model_path = self._get_local_model_path(self.embedding_model_name)

        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
        if self.local_model_path:
            try:
                import os
                if os.path.exists(self.local_model_path):
                    logger.info(f"  ğŸ” æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹: {self.local_model_path}")
                    logger.info(f"  ğŸ“¦ æ­£åœ¨åŠ è½½æœ¬åœ°Embeddingæ¨¡å‹...")
                    self.embedder = SentenceTransformer(self.local_model_path)
                    self.use_embeddings = True
                    logger.info(f"  âœ… æœ¬åœ°Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ!")
                    return
                else:
                    logger.warning(f"  âš ï¸ æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.local_model_path}")
            except Exception as e:
                logger.warning(f"  âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°è¯•ä¸‹è½½...")

        # å¦‚æœæœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•ä¸‹è½½

        try:
            logger.info(f"åŠ è½½Embeddingæ¨¡å‹: {self.embedding_model_name}")

            # ä½¿ç”¨ModelScopeé•œåƒï¼ˆå›½å†…æ›´å¿«ï¼‰
            if self.use_modelscope and snapshot_download is not None:
                try:
                    logger.info("  ä½¿ç”¨ModelScopeé•œåƒ...")
                    model_dir = snapshot_download(
                        f'sentence-transformers/{self.embedding_model_name}',
                        cache_dir='./model',
                        revision='master'
                    )
                    self.embedder = SentenceTransformer(model_dir)
                    logger.info(f"  âœ… æ¨¡å‹å·²ä»ModelScopeä¸‹è½½: {model_dir}")
                except Exception as e:
                    logger.warning(f"  ModelScopeä¸‹è½½å¤±è´¥: {e}ï¼Œå°è¯•HuggingFace...")
                    self.embedder = SentenceTransformer(self.embedding_model_name)
            else:
                self.embedder = SentenceTransformer(self.embedding_model_name)

            self.use_embeddings = True
            logger.info("  âœ… Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ")

        except Exception as e:
            logger.warning(f"  âŒ Embeddingæ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨çº¯å…³é”®è¯æ£€ç´¢")
            self.use_embeddings = False

    def _get_local_model_path(self, model_name: str) -> Optional[str]:
        """
        æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚ 'all-MiniLM-L6-v2'
            
        Returns:
            æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        import os
        from pathlib import Path
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„æœ¬åœ°è·¯å¾„
        possible_paths = [
            # ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„è·¯å¾„
            Path(__file__).parent.parent / "model" / "sentence-transformers" / model_name,
            # ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
            Path(__file__).parent.parent.parent / "KGdemo" / "model" / "sentence-transformers" / model_name,
            # ç»å¯¹è·¯å¾„
            Path("/home/lexy/ä¸‹è½½/CLwithRAG/KGdemo/model/sentence-transformers") / model_name,
        ]
        
        for path in possible_paths:
            if path.exists() and (path / "modules.json").exists():
                return str(path)
        
        return None

    def _init_grobid_parser(self):
        """åˆå§‹åŒ–GROBIDè§£æå™¨"""
        try:
            logger.info(f"åˆå§‹åŒ–GROBIDè§£æå™¨: {self.grobid_url}")
            self.grobid_parser = GrobidPDFParser(self.grobid_url)
            logger.info("âœ… GROBIDè§£æå™¨å·²å¯ç”¨")
        except Exception as e:
            logger.warning(f"âš ï¸ GROBIDè§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•")
            self.grobid_parser = None

    def _init_llm_client(self, config_path: Optional[str]) -> Optional[LLMClient]:
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        # å¦‚æœé…ç½®è·¯å¾„ä¸ºNoneï¼Œåˆ™ä¸ä½¿ç”¨LLM
        if config_path is None:
            logger.info("âš ï¸ LLMé…ç½®è·¯å¾„ä¸ºNoneï¼Œè·³è¿‡LLMåˆå§‹åŒ–ï¼ˆå°†ä½¿ç”¨åŸºç¡€åˆ†ææ¨¡å¼ï¼‰")
            return None

        try:
            logger.info(f"åŠ è½½LLMé…ç½®: {config_path}")
            config = LLMConfig.from_file(config_path)

            logger.info(f"  Provider: {config.provider}")
            logger.info(f"  Model: {config.model}")

            client = LLMClient(config)
            return client

        except FileNotFoundError:
            logger.warning(f"âš ï¸ LLMé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return None
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å¤±è´¥: {e}")
            return None

    def _get_section_patterns(self) -> Dict[str, List[str]]:
        """å®šä¹‰ç« èŠ‚è¯†åˆ«çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        import re
        return {
            'abstract': [
                r'^abstract\s*$',
                r'^summary\s*$',
            ],
            'introduction': [
                r'^1\.?\s*introduction',
                r'^introduction\s*$',
            ],
            'related_work': [
                r'^2\.?\s*related\s+work',
                r'^2\.?\s*background',
            ],
            'method': [
                r'^\d+\.?\s*method',
                r'^\d+\.?\s*approach',
                r'^\d+\.?\s*model',
            ],
            'experiment': [
                r'^\d+\.?\s*experiment',
                r'^\d+\.?\s*evaluation',
            ],
            'discussion': [
                r'^\d+\.?\s*discussion',
                r'^\d+\.?\s*analysis',
            ],
            'limitation': [
                r'^\d+\.?\s*limitation',
            ],
            'conclusion': [
                r'^\d+\.?\s*conclusion',
                r'^conclusion\s*$',
            ],
            'future_work': [
                r'^\d+\.?\s*future\s+work',
            ],
            'references': [
                r'^references\s*$',
            ],
        }

    # ========== æ ¸å¿ƒåˆ†ææ–¹æ³• ==========

    def analyze_paper(self, paper: Dict, pdf_path: Optional[str] = None) -> Dict:
        """
        åˆ†æè®ºæ–‡å¹¶æå–å…³é”®ä¿¡æ¯

        è‡ªåŠ¨æå–å››ä¸ªå­—æ®µï¼šProblem, Contribution, Limitation, Future Work
        æ”¯æŒå¤šçº§é™çº§ç­–ç•¥ï¼šPDF â†’ æ‘˜è¦ â†’ æ ‡é¢˜

        Args:
            paper: è®ºæ–‡åŸºç¡€ä¿¡æ¯å­—å…¸
            pdf_path: PDFæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            åŒ…å«åˆ†æç»“æœçš„å¢å¼ºè®ºæ–‡å­—å…¸
        """
        paper_id = paper.get('id', 'unknown')
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“„ å¼€å§‹åˆ†æè®ºæ–‡: {paper_id}")
        logger.info(f"{'='*60}")

        # æ­¥éª¤1: æå–ç« èŠ‚å†…å®¹å¹¶åˆ¤æ–­æ˜¯å¦æˆåŠŸæå–PDF
        sections, pdf_extracted = self._extract_paper_sections(paper, pdf_path)

        # åˆ¤æ–­æ˜¯å¦æˆåŠŸä»PDFæå–äº†ç« èŠ‚
        if pdf_extracted:
            logger.info("  âœ… PDFæå–æˆåŠŸï¼Œä½¿ç”¨RAGæ£€ç´¢æ¨¡å¼")
        else:
            logger.info("  âš ï¸ PDFæœªæå–ï¼Œä½¿ç”¨æ‘˜è¦ç›´æ¥ç”Ÿæˆæ¨¡å¼")

        # æ­¥éª¤2: è®¡ç®—ç« èŠ‚å‘é‡ï¼ˆä»…å½“PDFæå–æˆåŠŸæ—¶ï¼‰
        section_embeddings = None
        if pdf_extracted:
            section_embeddings = self._compute_section_embeddings(sections)

        # æ­¥éª¤3: æå–æ‰€æœ‰å­—æ®µï¼ˆä¼ å…¥pdf_extractedæ ‡å¿—ï¼‰
        analysis_result = self._extract_all_fields(sections, section_embeddings, pdf_extracted, paper)

        # æ­¥éª¤4: æ„å»ºç»“æœ
        enriched_paper = paper.copy()
        enriched_paper['rag_analysis'] = analysis_result
        enriched_paper['sections_extracted'] = len(sections)
        enriched_paper['section_types'] = [s.section_type for s in sections]
        enriched_paper['pdf_extracted'] = pdf_extracted
        enriched_paper['analysis_method'] = f'llm_rag_{self.llm_client.config.provider if self.llm_client else "none"}'

        logger.info(f"âœ… è®ºæ–‡åˆ†æå®Œæˆ: {paper_id}")
        logger.info(f"   æå–å­—æ®µæ•°: {len(analysis_result)}")
        logger.info(f"   ç« èŠ‚æ•°: {len(sections)}")
        logger.info(f"   åˆ†ææ¨¡å¼: {'RAGæ£€ç´¢' if pdf_extracted else 'æ‘˜è¦ç›´æ¥ç”Ÿæˆ'}")
        logger.info(f"{'='*60}\n")

        return enriched_paper

    def _extract_paper_sections(self, paper: Dict, pdf_path: Optional[str]) -> tuple[List[PaperSection], bool]:
        """
        æå–è®ºæ–‡ç« èŠ‚ï¼ˆæ”¯æŒå¤šçº§é™çº§ï¼‰

        é™çº§ç­–ç•¥:
        1. å°è¯•ä»PDFæå–ç« èŠ‚
        2. å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨æ‘˜è¦æ„å»ºç« èŠ‚
        3. å¦‚æœè¿æ‘˜è¦éƒ½æ²¡æœ‰ï¼Œä½¿ç”¨æ ‡é¢˜

        Args:
            paper: è®ºæ–‡ä¿¡æ¯
            pdf_path: PDFè·¯å¾„

        Returns:
            (ç« èŠ‚åˆ—è¡¨, PDFæ˜¯å¦æˆåŠŸæå–çš„æ ‡å¿—)
        """
        sections = []

        # Level 1: å°è¯•PDFæå–
        if pdf_path and Path(pdf_path).exists():
            logger.info("  [1/3] å°è¯•ä»PDFæå–ç« èŠ‚...")
            sections = self._extract_sections_from_pdf(pdf_path)

            if sections:
                logger.info(f"  âœ… ä»PDFæå–äº† {len(sections)} ä¸ªç« èŠ‚")
                return sections, True  # PDFæå–æˆåŠŸ
            else:
                logger.warning("  âŒ PDFç« èŠ‚æå–å¤±è´¥")

        # Level 2: é™çº§åˆ°æ‘˜è¦
        logger.info("  [2/3] é™çº§ä½¿ç”¨æ‘˜è¦...")
        sections = self._create_sections_from_abstract(paper)

        if sections:
            logger.info(f"  âœ… ä»æ‘˜è¦æ„å»ºäº† {len(sections)} ä¸ªç« èŠ‚")
            return sections, False  # ä½¿ç”¨æ‘˜è¦,PDFæœªæå–

        # Level 3: é™çº§åˆ°æ ‡é¢˜
        logger.info("  [3/3] é™çº§ä½¿ç”¨æ ‡é¢˜...")
        if paper.get('title'):
            sections = [PaperSection(
                title='Title',
                content=paper['title'],
                page_num=0,
                section_type='title'
            )]
            logger.info("  âœ… ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæœ€å°å†…å®¹")

        return sections, False  # ä½¿ç”¨æ ‡é¢˜æˆ–ç©º,PDFæœªæå–

    def _encode_texts(self, texts):
        """
        ç»Ÿä¸€çš„æ–‡æœ¬ç¼–ç æ¥å£
        æ”¯æŒ sentence-transformers å’Œæœ¬åœ° transformers æ¨¡å‹
        """
        if self.embedder:
            # ä½¿ç”¨ sentence-transformers
            return self.embedder.encode(texts)
        elif hasattr(self, 'tokenizer') and hasattr(self, 'model'):
            # ä½¿ç”¨æœ¬åœ° transformers æ¨¡å‹
            import torch

            # Mean Pooling - å–å¹³å‡æ± åŒ–
            def mean_pooling(model_output, attention_mask):
                token_embeddings = model_output[0]
                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
                return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

            # ç¼–ç æ–‡æœ¬
            encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)

            with torch.no_grad():
                model_output = self.model(**encoded_input)

            # æ‰§è¡Œæ± åŒ–
            embeddings = mean_pooling(model_output, encoded_input['attention_mask'])

            # å½’ä¸€åŒ–
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

            return embeddings.numpy()
        else:
            return None

    def _compute_section_embeddings(self, sections: List[PaperSection]) -> Optional[any]:
        """è®¡ç®—ç« èŠ‚å‘é‡"""
        if not self.use_embeddings or not sections:
            return None

        try:
            logger.info("  ğŸ”„ è®¡ç®—ç« èŠ‚å‘é‡...")
            section_texts = [f"{s.title} {s.content}" for s in sections]
            embeddings = self._encode_texts(section_texts)

            if embeddings is not None:
                logger.info(f"  âœ… ç« èŠ‚å‘é‡è®¡ç®—å®Œæˆ ({len(embeddings)} ä¸ªå‘é‡)")
                return embeddings
            else:
                logger.warning("  âŒ ç¼–ç å™¨æœªæ­£ç¡®åˆå§‹åŒ–")
                return None

        except Exception as e:
            logger.warning(f"  âŒ ç« èŠ‚å‘é‡è®¡ç®—å¤±è´¥: {e}")
            return None

    def _extract_all_fields(
        self,
        sections: List[PaperSection],
        section_embeddings: Optional[any],
        pdf_extracted: bool,
        paper: Dict
    ) -> Dict[str, str]:
        """
        è‡ªåŠ¨æå–æ‰€æœ‰å››ä¸ªå­—æ®µ

        Args:
            sections: è®ºæ–‡ç« èŠ‚åˆ—è¡¨
            section_embeddings: ç« èŠ‚å‘é‡ï¼ˆå¯é€‰ï¼‰
            pdf_extracted: PDFæ˜¯å¦æˆåŠŸæå–
            paper: åŸå§‹è®ºæ–‡ä¿¡æ¯ï¼ˆç”¨äºè·å–æ‘˜è¦ï¼‰

        Returns:
            {field: extracted_value}
        """
        logger.info("  ğŸ” å¼€å§‹æå–å…³é”®å­—æ®µ...")

        results = {}

        for field in self.extraction_fields:
            logger.info(f"     â€¢ æå– {field}...")

            try:
                value = self._extract_single_field(field, sections, section_embeddings, pdf_extracted, paper)
                results[field] = value

            except Exception as e:
                logger.error(f"     âŒ æå– {field} å¤±è´¥: {e}")
                results[field] = f"æå–å¤±è´¥: {str(e)}"

        logger.info(f"  âœ… å­—æ®µæå–å®Œæˆï¼ŒæˆåŠŸæå– {len(results)} ä¸ªå­—æ®µ")
        return results

    def _extract_single_field(
        self,
        field: str,
        sections: List[PaperSection],
        section_embeddings: Optional[any],
        pdf_extracted: bool,
        paper: Dict
    ) -> str:
        """
        æå–å•ä¸ªå­—æ®µ

        æµç¨‹:
        - å¦‚æœPDFæå–æˆåŠŸ: ä½¿ç”¨RAGæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ â†’ LLMç”Ÿæˆ
        - å¦‚æœPDFæœªæå–: ç›´æ¥ä½¿ç”¨æ‘˜è¦ä½œä¸ºä¸Šä¸‹æ–‡ â†’ LLMç”Ÿæˆ

        Args:
            field: å­—æ®µå
            sections: ç« èŠ‚åˆ—è¡¨
            section_embeddings: ç« èŠ‚å‘é‡
            pdf_extracted: PDFæ˜¯å¦æˆåŠŸæå–
            paper: åŸå§‹è®ºæ–‡ä¿¡æ¯

        Returns:
            æå–çš„å†…å®¹
        """
        if not sections:
            return "æ— å¯ç”¨å†…å®¹"

        # æ ¹æ®PDFæ˜¯å¦æå–æˆåŠŸé€‰æ‹©ä¸åŒç­–ç•¥
        if pdf_extracted:
            # ç­–ç•¥1: PDFæå–æˆåŠŸ -> ä½¿ç”¨RAGæ£€ç´¢
            logger.info(f"       ä½¿ç”¨RAGæ£€ç´¢æ¨¡å¼æå– {field}")
            relevant_context = self._retrieve_relevant_content(
                field, sections, section_embeddings
            )

            if not relevant_context or relevant_context == "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯":
                # RAGæ£€ç´¢å¤±è´¥,é™çº§åˆ°æ‘˜è¦
                logger.warning(f"       RAGæ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯,é™çº§ä½¿ç”¨æ‘˜è¦")
                relevant_context = self._get_abstract_context(paper)
        else:
            # ç­–ç•¥2: PDFæœªæå– -> ç›´æ¥ä½¿ç”¨æ‘˜è¦
            logger.info(f"       ä½¿ç”¨æ‘˜è¦ç›´æ¥ç”Ÿæˆæ¨¡å¼æå– {field}")
            relevant_context = self._get_abstract_context(paper)

        if not relevant_context or relevant_context == "æ— æ‘˜è¦å¯ç”¨":
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

        # ä½¿ç”¨LLMç”Ÿæˆ
        if self.llm_client:
            return self._generate_with_llm(field, relevant_context)
        else:
            logger.warning("     âš ï¸ LLMæœªé…ç½®ï¼Œè¿”å›åŸå§‹æ£€ç´¢å†…å®¹")
            return relevant_context[:200]  # è¿”å›æ£€ç´¢å†…å®¹çš„å‰200å­—ç¬¦

    def _get_abstract_context(self, paper: Dict) -> str:
        """
        è·å–æ‘˜è¦ä½œä¸ºä¸Šä¸‹æ–‡

        Args:
            paper: è®ºæ–‡ä¿¡æ¯

        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        abstract = paper.get('abstract', '')
        title = paper.get('title', '')

        if not abstract:
            return "æ— æ‘˜è¦å¯ç”¨"

        # æ„å»ºä¸Šä¸‹æ–‡
        context = f"Title: {title}\n\nAbstract: {abstract}" if title else f"Abstract: {abstract}"
        return context

    # ========== RAGæ£€ç´¢ ==========

    def _retrieve_relevant_content(
        self,
        field: str,
        sections: List[PaperSection],
        section_embeddings: Optional[any]
    ) -> str:
        """
        æ£€ç´¢ä¸å­—æ®µç›¸å…³çš„å†…å®¹ï¼ˆRAGæ ¸å¿ƒï¼‰

        æ”¯æŒ:
        - ç›®æ ‡ç« èŠ‚è¿‡æ»¤
        - å…³é”®è¯æ£€ç´¢
        - è¯­ä¹‰ç›¸ä¼¼åº¦æ’åºï¼ˆå¦‚æœæœ‰embeddingsï¼‰
        - é™çº§åˆ°æ‘˜è¦ï¼ˆå¦‚æœæ£€ç´¢å¤±è´¥ï¼‰

        Args:
            field: å­—æ®µå
            sections: ç« èŠ‚åˆ—è¡¨
            section_embeddings: ç« èŠ‚å‘é‡

        Returns:
            ç›¸å…³å†…å®¹æ–‡æœ¬
        """
        # å®šä¹‰ç›®æ ‡ç« èŠ‚å’Œå…³é”®è¯
        target_sections_map = {
            'problem': ['abstract', 'introduction'],
            'method': ['abstract', 'introduction', 'method', 'conclusion'],
            'limitation': ['limitation', 'discussion', 'conclusion'],
            'future_work': ['future_work', 'conclusion', 'discussion']
        }

        keywords_map = {
            'problem': ['problem', 'challenge', 'issue', 'gap', 'limitation'],
            'method': ['propose', 'contribution', 'novel', 'method', 'introduce'],
            'limitation': ['limitation', 'weakness', 'drawback', 'shortcoming'],
            'future_work': ['future', 'next', 'further', 'improve', 'explore']
        }

        target_section_types = target_sections_map.get(field, [])
        keywords = keywords_map.get(field, [])

        # æ­¥éª¤1: è¿‡æ»¤ç›®æ ‡ç« èŠ‚
        filtered_sections = [
            s for s in sections
            if s.section_type in target_section_types
        ] if target_section_types else sections

        if not filtered_sections:
            logger.info(f"       æœªæ‰¾åˆ°ç›®æ ‡ç« èŠ‚ {target_section_types}ï¼Œä½¿ç”¨æ‰€æœ‰ç« èŠ‚")
            filtered_sections = sections

        # æ­¥éª¤2: å…³é”®è¯æ£€ç´¢
        relevant_chunks = []

        for section in filtered_sections:
            paragraphs = self._split_into_paragraphs(section.content)

            for paragraph in paragraphs:
                # è®¡ç®—å…³é”®è¯åŒ¹é…æ•°
                keyword_count = sum(
                    1 for kw in keywords
                    if kw.lower() in paragraph.lower()
                )

                if keyword_count > 0:
                    relevant_chunks.append({
                        'text': paragraph,
                        'section': section.title,
                        'keyword_count': keyword_count
                    })

        # æ­¥éª¤3: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œé™çº§åˆ°æ‘˜è¦
        if not relevant_chunks:
            logger.info(f"       å…³é”®è¯æ£€ç´¢æœªæ‰¾åˆ°åŒ¹é…ï¼Œé™çº§ä½¿ç”¨æ‘˜è¦")
            abstract_sections = [s for s in sections if s.section_type == 'abstract']

            if abstract_sections:
                abstract_text = abstract_sections[0].content
                return f"[Abstract (Fallback)]\n{abstract_text[:self.max_context_length]}"
            else:
                # ä½¿ç”¨å‰ä¸¤ä¸ªç« èŠ‚
                all_content = "\n\n".join([f"[{s.title}]\n{s.content}" for s in sections[:2]])
                return all_content[:self.max_context_length] if all_content else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

        # æ­¥éª¤4: æ’åºï¼ˆå…³é”®è¯ or è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
        if self.use_embeddings and section_embeddings is not None:
            # ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ’åº
            query_text = f"extract {field} from paper"
            chunk_texts = [c['text'] for c in relevant_chunks]

            try:
                chunk_embeddings = self._encode_texts(chunk_texts)
                query_embedding = self._encode_texts([query_text])

                if chunk_embeddings is not None and query_embedding is not None:
                    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]

                    for i, chunk in enumerate(relevant_chunks):
                        chunk['similarity'] = similarities[i]

                    # ç»¼åˆæ’åºï¼ˆå…³é”®è¯30% + è¯­ä¹‰70%ï¼‰
                    relevant_chunks.sort(
                        key=lambda x: x['keyword_count'] * 0.3 + x['similarity'] * 0.7,
                        reverse=True
                    )
                else:
                    # ç¼–ç å¤±è´¥ï¼Œé™çº§åˆ°å…³é”®è¯æ’åº
                    relevant_chunks.sort(key=lambda x: x['keyword_count'], reverse=True)
            except Exception as e:
                # é™çº§åˆ°å…³é”®è¯æ’åº
                logger.warning(f"      è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}ï¼Œä½¿ç”¨å…³é”®è¯æ’åº")
                relevant_chunks.sort(key=lambda x: x['keyword_count'], reverse=True)
        else:
            # ä»…åŸºäºå…³é”®è¯æ’åº
            relevant_chunks.sort(key=lambda x: x['keyword_count'], reverse=True)

        # æ­¥éª¤5: æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        current_length = 0

        for chunk in relevant_chunks[:5]:  # å–top 5
            chunk_text = f"[{chunk['section']}]\n{chunk['text']}"
            chunk_length = len(chunk_text)

            if current_length + chunk_length > self.max_context_length:
                break

            context_parts.append(chunk_text)
            current_length += chunk_length

        return "\n\n".join(context_parts) if context_parts else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

    # ========== LLMç”Ÿæˆ ==========

    def _generate_with_llm(self, field: str, context: str) -> str:
        """
        ä½¿ç”¨LLMç”Ÿæˆåˆ†æç»“æœ

        Args:
            field: å­—æ®µå
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡

        Returns:
            LLMç”Ÿæˆçš„åˆ†æ
        """
        if not self.llm_client:
            return "LLMæœªé…ç½®"

        # æ„å»ºå®Œæ•´æç¤ºè¯
        full_prompt = self.prompt_manager.build_full_prompt(field, context)

        # è·å–ç³»ç»Ÿæç¤ºè¯
        system_prompt = self.prompt_manager.get_system_prompt()

        # è°ƒç”¨LLM
        result = self.llm_client.generate(
            prompt=full_prompt,
            system_prompt=system_prompt
        )

        return result

    # ========== PDFå¤„ç† ==========

    def _extract_sections_from_pdf(self, pdf_path: str) -> List[PaperSection]:
        """
        ä»PDFæˆ–TXTæ–‡ä»¶ä¸­æå–ç« èŠ‚ï¼ˆæ··åˆç­–ç•¥ï¼‰

        ç­–ç•¥:
        1. å¦‚æœæ˜¯.txtæ–‡ä»¶ï¼Œç›´æ¥è¯»å–æ–‡æœ¬
        2. å¦‚æœæ˜¯PDFï¼šä¼˜å…ˆä½¿ç”¨GROBIDï¼ˆå¦‚æœå¯ç”¨ï¼‰
        3. é™çº§åˆ°PyPDF2+æ­£åˆ™è¡¨è¾¾å¼
        """
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        file_ext = Path(pdf_path).suffix.lower()

        # ç­–ç•¥0: å¦‚æœæ˜¯.txtæ–‡ä»¶ï¼Œç›´æ¥è¯»å–
        if file_ext == '.txt':
            logger.info("  æ£€æµ‹åˆ°.txtæ–‡ä»¶ï¼Œç›´æ¥è¯»å–æ–‡æœ¬...")
            return self._extract_sections_from_txt(pdf_path)

        # ç­–ç•¥1: å°è¯•GROBIDï¼ˆä»…å¯¹PDFï¼‰
        if self.grobid_parser:
            try:
                logger.info("  å°è¯•ä½¿ç”¨GROBIDè§£æPDF...")
                sections = self.grobid_parser.extract_sections_from_pdf(pdf_path)

                if sections:
                    logger.info(f"  âœ… GROBIDæˆåŠŸæå– {len(sections)} ä¸ªç« èŠ‚")
                    return sections
                else:
                    logger.warning("  âš ï¸ GROBIDæœªæå–åˆ°ç« èŠ‚ï¼Œé™çº§åˆ°æ­£åˆ™æ–¹æ³•")
            except Exception as e:
                logger.warning(f"  âš ï¸ GROBIDè§£æå¤±è´¥: {e}ï¼Œé™çº§åˆ°æ­£åˆ™æ–¹æ³•")

        # ç­–ç•¥2: é™çº§åˆ°PyPDF2+æ­£åˆ™è¡¨è¾¾å¼
        logger.info("  ä½¿ç”¨PyPDF2+æ­£åˆ™è¡¨è¾¾å¼è§£æPDF...")
        return self._extract_sections_with_pypdf2(pdf_path)

    def _extract_sections_with_pypdf2(self, pdf_path: str) -> List[PaperSection]:
        """ä½¿ç”¨PyPDF2æå–ç« èŠ‚ï¼ˆæ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•ï¼‰"""
        if PyPDF2 is None:
            logger.error("  PyPDF2æœªå®‰è£…ï¼Œæ— æ³•æå–PDF")
            return []

        sections = []

        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)

                full_text = ""
                for page_num in range(total_pages):
                    try:
                        page = pdf_reader.pages[page_num]
                        page_text = page.extract_text()
                        full_text += page_text + "\n"
                    except:
                        continue

                sections = self._identify_sections(full_text)

        except Exception as e:
            logger.error(f"  PDFå¤„ç†å¤±è´¥: {e}")

        return sections

    def _extract_sections_from_txt(self, txt_path: str) -> List[PaperSection]:
        """ä».txtæ–‡ä»¶ç›´æ¥è¯»å–å¹¶æå–ç« èŠ‚"""
        sections = []

        try:
            with open(txt_path, 'r', encoding='utf-8') as file:
                full_text = file.read()

            logger.info(f"  âœ… æˆåŠŸè¯»å–.txtæ–‡ä»¶ï¼Œå…± {len(full_text)} ä¸ªå­—ç¬¦")

            # ä½¿ç”¨ç›¸åŒçš„ç« èŠ‚è¯†åˆ«é€»è¾‘
            sections = self._identify_sections(full_text)

            if sections:
                logger.info(f"  âœ… ä».txtæ–‡ä»¶è¯†åˆ«å‡º {len(sections)} ä¸ªç« èŠ‚")
            else:
                logger.warning("  âš ï¸ æœªè¯†åˆ«åˆ°æ˜ç¡®ç« èŠ‚ï¼Œå°†æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªç« èŠ‚")
                # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ï¼Œå°†æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªç« èŠ‚
                sections = [PaperSection(
                    title='Full Text',
                    content=full_text[:10000],  # é™åˆ¶é•¿åº¦
                    page_num=0,
                    section_type='other'
                )]

        except Exception as e:
            logger.error(f"  .txtæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")

        return sections

    def _identify_sections(self, full_text: str) -> List[PaperSection]:
        """è¯†åˆ«æ–‡æœ¬ä¸­çš„ç« èŠ‚"""
        import re

        sections = []
        lines = full_text.split('\n')

        current_section = None
        current_content = []
        current_type = 'other'

        for line in lines:
            line_stripped = line.strip()
            if not line_stripped:
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
            section_type = self._match_section_type(line_stripped)

            if section_type:
                # ä¿å­˜å‰ä¸€ä¸ªç« èŠ‚
                if current_section and current_content:
                    content = '\n'.join(current_content).strip()
                    if content:
                        sections.append(PaperSection(
                            title=current_section,
                            content=content,
                            page_num=0,
                            section_type=current_type
                        ))

                # å¼€å§‹æ–°ç« èŠ‚
                current_section = line_stripped
                current_content = []
                current_type = section_type
            else:
                # æ·»åŠ åˆ°å½“å‰ç« èŠ‚
                if current_section:
                    current_content.append(line_stripped)

        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section and current_content:
            content = '\n'.join(current_content).strip()
            if content:
                sections.append(PaperSection(
                    title=current_section,
                    content=content,
                    page_num=0,
                    section_type=current_type
                ))

        return sections

    def _match_section_type(self, line: str) -> Optional[str]:
        """åŒ¹é…ç« èŠ‚ç±»å‹"""
        import re

        line_lower = line.lower().strip()

        for section_type, patterns in self.section_patterns.items():
            for pattern in patterns:
                if re.match(pattern, line_lower, re.IGNORECASE):
                    return section_type

        return None

    def _create_sections_from_abstract(self, paper: Dict) -> List[PaperSection]:
        """ä»æ‘˜è¦åˆ›å»ºç« èŠ‚"""
        sections = []

        if paper.get('title'):
            sections.append(PaperSection(
                title='Title',
                content=paper['title'],
                page_num=0,
                section_type='title'
            ))

        if paper.get('abstract'):
            sections.append(PaperSection(
                title='Abstract',
                content=paper['abstract'],
                page_num=0,
                section_type='abstract'
            ))

        return sections

    # ========== å·¥å…·æ–¹æ³• ==========

    def _split_into_paragraphs(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ®µè½"""
        import re

        paragraphs = re.split(r'\n\s*\n|\n', text)
        paragraphs = [p.strip() for p in paragraphs if len(p.strip()) > 30]

        return paragraphs

    # ========== æ‰¹é‡åˆ†æ ==========

    def batch_analyze_papers(
        self,
        papers: List[Dict],
        pdf_dir: Optional[str] = None
    ) -> List[Dict]:
        """
        æ‰¹é‡åˆ†æè®ºæ–‡

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            pdf_dir: PDFæ–‡ä»¶å¤¹è·¯å¾„

        Returns:
            å¢å¼ºçš„è®ºæ–‡åˆ—è¡¨
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“š æ‰¹é‡åˆ†æ {len(papers)} ç¯‡è®ºæ–‡")
        logger.info(f"{'='*60}\n")

        enriched_papers = []

        for i, paper in enumerate(papers):
            try:
                # æŸ¥æ‰¾PDFæ–‡ä»¶
                pdf_path = None
                if pdf_dir:
                    paper_id = paper.get('id', '')
                    pdf_dir_path = Path(pdf_dir)

                    for pdf_file in pdf_dir_path.glob(f"{paper_id}*.pdf"):
                        pdf_path = str(pdf_file)
                        break

                # åˆ†æè®ºæ–‡
                enriched_paper = self.analyze_paper(paper, pdf_path)
                enriched_papers.append(enriched_paper)

                logger.info(f"è¿›åº¦: {i+1}/{len(papers)}\n")

            except Exception as e:
                logger.error(f"åˆ†æè®ºæ–‡å¤±è´¥ {paper.get('id', 'unknown')}: {e}")

                # æ·»åŠ å¤±è´¥çš„è®ºæ–‡
                failed_paper = paper.copy()
                failed_paper['rag_analysis'] = {
                    'problem': f'åˆ†æå¤±è´¥: {str(e)}',
                    'method': f'åˆ†æå¤±è´¥: {str(e)}',
                    'limitation': f'åˆ†æå¤±è´¥: {str(e)}',
                    'future_work': f'åˆ†æå¤±è´¥: {str(e)}'
                }
                enriched_papers.append(failed_paper)

        logger.info(f"{'='*60}")
        logger.info(f"âœ… æ‰¹é‡åˆ†æå®Œæˆ")
        logger.info(f"{'='*60}\n")

        return enriched_papers


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("\n" + "="*60)
    print("æµ‹è¯•LLM RAGè®ºæ–‡åˆ†æå™¨ï¼ˆé‡æ„ç‰ˆï¼‰")
    print("="*60)

    # æµ‹è¯•è®ºæ–‡æ•°æ®
    test_paper = {
        'id': 'W2741809807',
        'title': 'Attention Is All You Need',
        'abstract': '''The dominant sequence transduction models are based on complex
        recurrent or convolutional neural networks. The problem is that these models
        are difficult to parallelize. We propose the Transformer, a model architecture
        eschewing recurrence and instead relying entirely on an attention mechanism.''',
        'year': 2017,
    }

    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = LLMRAGPaperAnalyzer(
            llm_config_path='../llm_config_ollama.json',
            prompts_dir='../prompts'
        )

        # åˆ†æè®ºæ–‡
        result = analyzer.analyze_paper(test_paper)

        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*60)
        print("åˆ†æç»“æœ:")
        print("="*60)
        for field, value in result['rag_analysis'].items():
            print(f"\n{field.upper()}:")
            print(value)

        print("\n" + "="*60)
        print("âœ… æµ‹è¯•å®Œæˆ")
        print("="*60)

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
