"""
Complete 8-Step Literature Retrieval Pipeline with Citation Network Construction

å®Œæ•´çš„8æ­¥æ–‡çŒ®æ£€ç´¢æµç¨‹ä¸å¼•ç”¨ç½‘ç»œæ„å»º

8-Step Pipeline:
---------------
æ­¥éª¤ 1+2: ç§å­æ£€ç´¢ä¸OpenAlexæ˜ å°„ (Seed Retrieval & OpenAlex Mapping - Combined)
    ç­–ç•¥ä¼˜åŒ–ï¼šæ”¾å®½æ£€ç´¢ + ç¡®ä¿æ˜ å°„
    - é˜¶æ®µ1: ä½¿ç”¨arXiv APIæ”¾å®½æ£€ç´¢æ¡ä»¶ï¼Œè·å–3å€äºç›®æ ‡æ•°é‡çš„å€™é€‰è®ºæ–‡ï¼ˆæ— å¹´ä»½é™åˆ¶ï¼‰
    - é˜¶æ®µ2: å¯¹æ‰€æœ‰å€™é€‰è®ºæ–‡æ‰¹é‡è¿›è¡ŒOpenAlexæ˜ å°„
    - é˜¶æ®µ3: åªä¿ç•™æ˜ å°„æˆåŠŸçš„è®ºæ–‡ï¼ŒæŒ‰è´¨é‡(å¼•ç”¨æ•°+ç›¸å…³æ€§)æ’åºé€‰æ‹©æœ€ä½³ç§å­
    - ä¼˜åŠ¿: ç¡®ä¿æ‰€æœ‰ç§å­éƒ½èƒ½åœ¨OpenAlexå¼•ç”¨ç½‘ç»œä¸­ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒé«˜å¬å›ç‡

æ­¥éª¤ 3: æ­£å‘æ»šé›ªçƒ (Forward Snowballing)
    - Seed -> è°å¼•ç”¨äº†Seed? -> å­èŠ‚ç‚¹
    - è·å–è¢«å¼•ç”¨è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯

æ­¥éª¤ 4: åå‘æ»šé›ªçƒ (Backward Snowballing)
    - è°è¢«Seedå¼•ç”¨äº†? <- Seed -> çˆ¶èŠ‚ç‚¹/ç¥–å…ˆ
    - è·å–å¼•ç”¨è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯

æ­¥éª¤ 5: æ¨ªå‘è¡¥å……/å…±å¼•æŒ–æ˜ (Co-citation Mining)
    - åœ¨å­èŠ‚ç‚¹å’Œçˆ¶èŠ‚ç‚¹ä¸­ï¼Œè°è¢«å¤§å®¶åå¤æåŠ?
    - å…±å¼•é˜ˆå€¼è¿‡æ»¤é«˜ä»·å€¼è®ºæ–‡

æ­¥éª¤ 6 [å¯é€‰]: ç¬¬äºŒè½®æ»šé›ªçƒ (Second-Round Snowballing)
    - å¯¹ç¬¬ä¸€è½®è®ºæ–‡å†è¿›è¡Œä¸€è½®å—æ§æ‰©å±•
    - æ§åˆ¶æ‰©å±•è§„æ¨¡

æ­¥éª¤ 7: è¡¥å……æœ€æ–°SOTA (Recent Frontiers Supplementation)
    - arXivæœ€è¿‘6-12ä¸ªæœˆè®ºæ–‡
    - ç›¸ä¼¼åº¦è¿‡æ»¤

æ­¥éª¤ 8: æ„å»ºå¼•ç”¨é—­åŒ… (Citation Closure Construction)
    - å»ºç«‹å®Œæ•´ç½‘ç»œ
    - å¡«è¡¥ç¼ºå¤±çš„å¼•ç”¨å…³ç³»è¿æ¥å¼•ç”¨

Date: 2025-12-09
Version: 2.0 (Combined Step 1+2 for better seed quality)
"""

import logging
import time
from typing import List, Dict, Set, Optional, Tuple
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path
import yaml

# å¯¼å…¥ä¾èµ–æ¨¡å—
from openalex_client import OpenAlexClient
from arxiv_seed_retriever import ArxivSeedRetriever
from cross_database_mapper import CrossDatabaseMapper

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PaperSearchPipeline:
    """
    å®Œæ•´çš„8æ­¥æ–‡çŒ®æ£€ç´¢æµç¨‹ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ­¥éª¤1+2åˆå¹¶ï¼‰

    ç­–ç•¥ä¼˜åŒ–ï¼š
    - æ­¥éª¤1+2åˆå¹¶ï¼šæ”¾å®½arXivæ£€ç´¢ â†’ æ‰¹é‡OpenAlexæ˜ å°„ â†’ ç¡®ä¿æ‰€æœ‰ç§å­å¯ç”¨
    - æ•´åˆarXivç§å­æ£€ç´¢ã€OpenAlexå¼•ç”¨æ‰©å±•ã€å…±å¼•æŒ–æ˜
    - ç¡®ä¿æ‰€æœ‰ç§å­è®ºæ–‡éƒ½åœ¨OpenAlexæœ‰æ˜ å°„ï¼Œå¯è¿›è¡Œå¼•ç”¨ç½‘ç»œæ‰©å±•
    """

    def __init__(
        self,
        openalex_client: Optional[OpenAlexClient] = None,
        config_path: str = './config/config.yaml',
        llm_client = None
    ):
        """
        åˆå§‹åŒ–æµç¨‹

        Args:
            openalex_client: OpenAlexå®¢æˆ·ç«¯
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            llm_client: LLMå®¢æˆ·ç«¯(ç”¨äºæŸ¥è¯¢ç”Ÿæˆ)
        """
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.openalex_client = openalex_client or OpenAlexClient()
        self.llm_client = llm_client

        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)

        # åˆå§‹åŒ–æ£€ç´¢å™¨
        self._init_retrievers()

        # æ•°æ®å­˜å‚¨ç»“æ„
        self.papers = {}  # paper_id -> paper_dict
        self.citation_edges = []  # [(source_id, target_id), ...]

        # å„æ­¥éª¤ç»“æœç¼“å­˜(ç”¨äºè°ƒè¯•å’Œç»Ÿè®¡)
        self.seed_papers = []  # æ­¥éª¤1: ç§å­è®ºæ–‡
        self.mapped_seeds = []  # æ­¥éª¤2: æ˜ å°„æˆåŠŸçš„ç§å­
        self.unmapped_seeds = []  # æ­¥éª¤2: æ˜ å°„å¤±è´¥çš„ç§å­
        self.forward_papers = []  # æ­¥éª¤3: æ­£å‘å¼•ç”¨è®ºæ–‡
        self.backward_papers = []  # æ­¥éª¤4: åå‘å¼•ç”¨è®ºæ–‡
        self.cocitation_papers = []  # æ­¥éª¤5: å…±å¼•è®ºæ–‡
        self.second_round_papers = []  # æ­¥éª¤6: ç¬¬äºŒè½®æ‰©å±•è®ºæ–‡ï¼ˆæ€»è®¡ï¼‰
        self.second_round_citing = []  # æ­¥éª¤6: ç¬¬äºŒè½®æ­£å‘å¼•ç”¨è®ºæ–‡
        self.second_round_ancestor = []  # æ­¥éª¤6: ç¬¬äºŒè½®åå‘å¼•ç”¨è®ºæ–‡
        self.recent_papers = []  # æ­¥éª¤7: æœ€æ–°è®ºæ–‡

        # ç»Ÿè®¡ä¿¡æ¯
        self.statistics = {
            'seed_papers': 0,
            'arxiv_mapped': 0,
            'arxiv_unmapped': 0,
            'manual_citations_built': 0,
            'first_round_citing': 0,
            'first_round_ancestor': 0,
            'first_round_cocitation': 0,
            'second_round_enabled': False,
            'second_round_citing': 0,
            'second_round_ancestor': 0,
            'recent_papers': 0,
            'total_papers': 0,
            'total_edges': 0
        }

        logger.info("="*70)
        logger.info("åˆå§‹åŒ–è®ºæ–‡æ£€ç´¢æµç¨‹")
        logger.info("="*70)
        logger.info(f"é…ç½®å‚æ•°:")
        logger.info(f"  - ç§å­æ•°é‡: {self.config['seed_count']}")
        logger.info(f"  - æ¯ç§å­å¼•ç”¨æ•°: {self.config['citations_per_seed']}")
        logger.info(f"  - å…±å¼•é˜ˆå€¼: {self.config['cocitation_threshold']}")
        logger.info(f"  - ç¬¬äºŒè½®æ‰©å±•: {'å¯ç”¨' if self.config['enable_second_round'] else 'ç¦ç”¨'}")
        logger.info(f"  - æœ€æ–°è®ºæ–‡æ•°: {self.config['recent_count']}")
        logger.info("="*70)

    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            config_file = Path(config_path)
            if not config_file.exists():
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return self._default_config()

            with open(config_file, 'r', encoding='utf-8') as f:
                full_config = yaml.safe_load(f)
                snowball_config = full_config.get('snowball', {})

                config = {
                    # ç§å­æ£€ç´¢
                    'seed_count': snowball_config.get('seed_count', 10),
                    'arxiv_years_back': snowball_config.get('arxiv_years_back', 5),

                    # å¼•ç”¨æ•°é‡
                    'citations_per_seed': snowball_config.get('citations_per_seed', 15),
                    'references_per_seed': snowball_config.get('references_per_seed', 10),

                    # å…±å¼•
                    'cocitation_threshold': snowball_config.get('cocitation_threshold', 3),
                    'max_cocitation_papers': snowball_config.get('max_cocitation_papers', 20),

                    # ç¬¬äºŒè½®æ‰©å±•
                    'enable_second_round': snowball_config.get('enable_second_round', True),
                    'second_round_limit': snowball_config.get('second_round_limit', 5),
                    'second_round_max_papers': snowball_config.get('second_round_max_papers', 50),

                    # æœ€æ–°è®ºæ–‡
                    'recent_months': snowball_config.get('recent_months', 12),
                    'recent_count': snowball_config.get('recent_count', 10),

                    # å…¶ä»–
                    'use_llm_query': snowball_config.get('use_llm_query', True),
                    'min_citation_count': snowball_config.get('min_citation_count', 5),

                    # LLMè¯­ä¹‰æ‰©å±•
                    'llm_semantic_expansion': snowball_config.get('llm_semantic_expansion', True),
                    'expansion_max_topics': snowball_config.get('expansion_max_topics', 4),
                    'expansion_max_keywords': snowball_config.get('expansion_max_keywords', 8),
                }

                logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                return config

        except Exception as e:
            logger.warning(f"åŠ è½½é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._default_config()

    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'seed_count': 10,
            'arxiv_years_back': 5,
            'citations_per_seed': 15,
            'references_per_seed': 10,
            'cocitation_threshold': 3,
            'max_cocitation_papers': 20,
            'enable_second_round': True,
            'second_round_limit': 5,
            'second_round_max_papers': 50,
            'recent_months': 12,
            'recent_count': 10,
            'use_llm_query': True,
            'min_citation_count': 5,
            'llm_semantic_expansion': True,
            'expansion_max_topics': 4,
            'expansion_max_keywords': 8,
        }

    def _init_retrievers(self):
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        # arXivç§å­æ£€ç´¢å™¨
        self.arxiv_retriever = ArxivSeedRetriever(
            max_results_per_query=self.config['seed_count'] * 2,
            years_back=self.config['arxiv_years_back'],
            min_relevance_score=0.5,
            llm_client=self.llm_client,
            use_llm_query_generation=self.config['use_llm_query'],
            enable_semantic_expansion=self.config.get('llm_semantic_expansion', True),
            expansion_max_topics=self.config.get('expansion_max_topics', 4),
            expansion_max_keywords=self.config.get('expansion_max_keywords', 8)
        )

        # è·¨åº“æ˜ å°„å™¨
        self.cross_mapper = CrossDatabaseMapper(
            client=self.openalex_client,
            min_concept_score=0.3,
            required_concepts=["Computer Science"]
        )

        logger.info("æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

    def execute_full_pipeline(
        self,
        topic: str,
        keywords: Optional[List[str]] = None,
        categories: Optional[List[str]] = None
    ) -> Dict:
        """
        æ‰§è¡Œå®Œæ•´çš„8æ­¥æ£€ç´¢æµç¨‹

        Args:
            topic: ç ”ç©¶ä¸»é¢˜
            keywords: å…³é”®è¯åˆ—è¡¨(å¯é€‰)
            categories: arXivåˆ†ç±»åˆ—è¡¨(å¯é€‰)

        Returns:
            {
                'papers': {paper_id: paper_dict},
                'citation_edges': [(source_id, target_id), ...],
                'statistics': {...}
            }
        """
        logger.info("\n" + "="*70)
        logger.info(f"å¼€å§‹æ‰§è¡Œ8æ­¥æ–‡çŒ®æ£€ç´¢æµç¨‹")
        logger.info(f"ä¸»é¢˜: {topic}")
        logger.info("="*70 + "\n")

        start_time = time.time()

        # æ­¥éª¤1: é«˜è´¨é‡ç§å­è·å–
        self._step1_seed_retrieval(topic, keywords, categories)

        # æ­¥éª¤2: è·¨åº“IDæ˜ å°„
        self._step2_cross_database_mapping()

        # æ­¥éª¤3: æ­£å‘æ»šé›ªçƒ
        self._step3_forward_snowballing()

        # æ­¥éª¤4: åå‘æ»šé›ªçƒ
        self._step4_backward_snowballing()

        # æ­¥éª¤5: å…±å¼•æŒ–æ˜
        self._step5_cocitation_mining()

        # æ­¥éª¤6: ç¬¬äºŒè½®æ‰©å±•(å¯é€‰)
        if self.config['enable_second_round']:
            self._step6_second_round_snowballing()

        # æ­¥éª¤7: è¡¥å……æœ€æ–°SOTA
        self._step7_recent_frontiers(topic, keywords, categories)

        # æ­¥éª¤8: æ„å»ºå¼•ç”¨é—­åŒ…
        self._step8_citation_closure()

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self._finalize_statistics()

        elapsed_time = time.time() - start_time
        logger.info("\n" + "="*70)
        logger.info(f"8æ­¥æ£€ç´¢æµç¨‹å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        logger.info("="*70)
        self._print_summary()

        return {
            'papers': self.papers,
            'citation_edges': self.citation_edges,
            'statistics': self.statistics
        }

    def _step1_seed_retrieval(
        self,
        topic: str,
        keywords: Optional[List[str]],
        categories: Optional[List[str]]
    ):
        """
        æ­¥éª¤1: é«˜è´¨é‡ç§å­è·å–ï¼ˆç»“åˆæ­¥éª¤2ï¼šç¡®ä¿OpenAlexæ˜ å°„ï¼‰

        ç­–ç•¥ï¼š
        1. æ”¾å®½arXivæ£€ç´¢æ¡ä»¶ï¼Œè·å–æ›´å¤šå€™é€‰è®ºæ–‡ï¼ˆæ‰©å¤§åˆå§‹æ£€ç´¢èŒƒå›´ï¼Œæ— å¹´ä»½é™åˆ¶ï¼‰
        2. å¯¹æ‰€æœ‰å€™é€‰è®ºæ–‡è¿›è¡ŒOpenAlexæ˜ å°„
        3. åªä¿ç•™æ˜ å°„æˆåŠŸçš„è®ºæ–‡ä½œä¸ºæœ€ç»ˆç§å­
        4. æŒ‰è´¨é‡æ’åºï¼ˆå¼•ç”¨æ•°+ç›¸å…³æ€§ï¼‰é€‰æ‹©æœ€ä½³ç§å­
        """
        logger.info("\n" + "="*70)
        logger.info("æ­¥éª¤1+2: ç§å­æ£€ç´¢ä¸OpenAlexæ˜ å°„ï¼ˆç»„åˆæµç¨‹ï¼‰")
        logger.info("="*70)
        logger.info("ç­–ç•¥: æ”¾å®½arXivæ£€ç´¢ â†’ æ‰¹é‡OpenAlexæ˜ å°„ â†’ ä¿ç•™æ˜ å°„æˆåŠŸçš„è®ºæ–‡")

        target_seed_count = self.config['seed_count']

        # æ”¾å®½æ£€ç´¢æ¡ä»¶ï¼šæ£€ç´¢æ›´å¤šå€™é€‰è®ºæ–‡ï¼ˆ3å€äºç›®æ ‡æ•°é‡ï¼‰
        # å› ä¸ºè€ƒè™‘åˆ°æ˜ å°„æˆåŠŸç‡ï¼Œéœ€è¦æ›´å¤šå€™é€‰
        candidate_count = target_seed_count * 3

        logger.info(f"\né˜¶æ®µ1: arXivå€™é€‰è®ºæ–‡æ£€ç´¢ï¼ˆæ”¾å®½æ¡ä»¶ï¼‰")
        logger.info(f"  - ç›®æ ‡ç§å­æ•°: {target_seed_count}")
        logger.info(f"  - å€™é€‰æ£€ç´¢æ•°: {candidate_count}")

        try:
            # ä¸´æ—¶é™ä½ç›¸å…³æ€§é˜ˆå€¼ä»¥è·å–æ›´å¤šå€™é€‰
            original_threshold = self.arxiv_retriever.min_relevance_score
            self.arxiv_retriever.min_relevance_score = 0.3  # æ”¾å®½åˆ°0.3

            # ä½¿ç”¨arXivæ£€ç´¢å™¨è·å–å€™é€‰è®ºæ–‡
            arxiv_candidates = self.arxiv_retriever.retrieve_seed_papers(
                topic=topic,
                keywords=keywords,
                categories=categories,
                max_seeds=candidate_count
            )

            # æ¢å¤åŸé˜ˆå€¼
            self.arxiv_retriever.min_relevance_score = original_threshold

            # ä¸å†è¿‡æ»¤å¹´ä»½ï¼Œæ¥å—æ‰€æœ‰å€™é€‰è®ºæ–‡
            filtered_candidates = arxiv_candidates

            logger.info(f"  âœ“ arXivæ£€ç´¢åˆ° {len(filtered_candidates)} ç¯‡å€™é€‰è®ºæ–‡ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰")

            if not filtered_candidates:
                logger.warning("  âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„arXivå€™é€‰è®ºæ–‡")
                self.seed_papers = []
                self.mapped_seeds = []
                self.unmapped_seeds = []
                return

            # é˜¶æ®µ2: æ‰¹é‡æ˜ å°„åˆ°OpenAlex
            logger.info(f"\né˜¶æ®µ2: æ‰¹é‡æ˜ å°„åˆ°OpenAlex")
            logger.info(f"  - å€™é€‰è®ºæ–‡æ•°: {len(filtered_candidates)}")

            # ä½¿ç”¨æ˜ å°„å™¨è¿›è¡ŒIDæ˜ å°„ï¼ˆç¦ç”¨æ¦‚å¿µéªŒè¯ï¼Œå› ä¸ºarXivé˜¶æ®µå·²è¿‡æ»¤ï¼‰
            mapped_papers, mapping_stats = self.cross_mapper.map_arxiv_to_openalex(
                arxiv_papers=filtered_candidates,
                verify_concepts=False  # arXivè®ºæ–‡å·²ç»è¿‡æ»¤
            )

            logger.info(f"\næ˜ å°„ç»“æœ:")
            logger.info(f"  - æ˜ å°„æˆåŠŸ: {len(mapped_papers)} ç¯‡")
            logger.info(f"  - æ˜ å°„å¤±è´¥: {mapping_stats['failed']} ç¯‡")
            logger.info(f"  - æˆåŠŸç‡: {mapping_stats.get('success_rate', 0):.1%}")

            # é˜¶æ®µ3: æŒ‰è´¨é‡æ’åºï¼Œé€‰æ‹©æœ€ä½³ç§å­
            logger.info(f"\né˜¶æ®µ3: é€‰æ‹©é«˜è´¨é‡ç§å­")

            # æŒ‰å¼•ç”¨æ•°å’Œç›¸å…³æ€§ç»¼åˆæ’åº
            for paper in mapped_papers:
                # ç»¼åˆå¾—åˆ† = å½’ä¸€åŒ–å¼•ç”¨æ•° * 0.6 + ç›¸å…³æ€§å¾—åˆ† * 0.4
                cited_count = paper.get('cited_by_count', 0)
                relevance = paper.get('relevance_score', 0.5)

                # ç®€å•å½’ä¸€åŒ–ï¼ˆlog scaleï¼‰
                normalized_citation = min(1.0, cited_count / 100.0) if cited_count > 0 else 0
                paper['quality_score'] = normalized_citation * 0.6 + relevance * 0.4

            # æ’åº
            mapped_papers.sort(key=lambda x: x.get('quality_score', 0), reverse=True)

            # é€‰æ‹©top Nä½œä¸ºæœ€ç»ˆç§å­
            final_seeds = mapped_papers[:target_seed_count]

            logger.info(f"  - é€‰æ‹©å‰ {len(final_seeds)} ç¯‡ä½œä¸ºæœ€ç»ˆç§å­")

            # æ›´æ–°ç§å­åˆ—è¡¨
            self.seed_papers = filtered_candidates  # ä¿ç•™åŸå§‹arXivæ£€ç´¢ç»“æœç”¨äºç»Ÿè®¡
            self.mapped_seeds = final_seeds  # æ˜ å°„æˆåŠŸçš„ç§å­ï¼ˆç”¨äºåç»­æµç¨‹ï¼‰
            self.unmapped_seeds = [
                p for p in filtered_candidates
                if not any(m.get('arxiv_id') == p.get('arxiv_id') for m in mapped_papers)
            ]

            # å°†æ˜ å°„æˆåŠŸçš„ç§å­è®ºæ–‡åŠ å…¥paperså­—å…¸ï¼Œå¹¶æ ‡è®°ä¸ºç§å­èŠ‚ç‚¹
            for paper in self.mapped_seeds:
                paper_id = paper['id']
                paper['is_seed'] = True  # æ·»åŠ ç§å­èŠ‚ç‚¹æ ‡è®°
                self.papers[paper_id] = paper

            logger.info(f"\næœ€ç»ˆç§å­ç»Ÿè®¡:")
            logger.info(f"  - arXivå€™é€‰æ•°: {len(filtered_candidates)} ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰")
            logger.info(f"  - OpenAlexæ˜ å°„æˆåŠŸ: {len(mapped_papers)}")
            logger.info(f"  - æœ€ç»ˆç§å­æ•°: {len(final_seeds)}")
            logger.info(f"  - æ˜ å°„å¤±è´¥(ä¸¢å¼ƒ): {len(self.unmapped_seeds)}")

            # æ‰“å°ç§å­è®ºæ–‡ä¿¡æ¯å’ŒID
            logger.info(f"\nğŸŒ± æœ€ç»ˆç§å­è®ºæ–‡ï¼ˆTop {min(5, len(final_seeds))}ï¼‰:")
            for i, paper in enumerate(final_seeds[:5], 1):
                logger.info(
                    f"  [{i}] {paper['title'][:60]}... "
                    f"({paper.get('year', 'N/A')}, "
                    f"å¼•ç”¨:{paper.get('cited_by_count', 0)}, "
                    f"è´¨é‡åˆ†:{paper.get('quality_score', 0):.2f})"
                )
                logger.info(f"      ID: {paper['id']}")
            if len(final_seeds) > 5:
                logger.info(f"  ... è¿˜æœ‰ {len(final_seeds) - 5} ç¯‡")

            # è®°å½•æ‰€æœ‰ç§å­èŠ‚ç‚¹ID
            seed_ids = [p['id'] for p in final_seeds]
            logger.info(f"\nğŸ”‘ ç§å­èŠ‚ç‚¹IDåˆ—è¡¨: {seed_ids}")

            # å¦‚æœç§å­æ•°é‡ä¸è¶³ï¼Œå‘å‡ºè­¦å‘Š
            if len(final_seeds) < target_seed_count:
                logger.warning(
                    f"\nâš ï¸ è­¦å‘Š: æœ€ç»ˆç§å­æ•°({len(final_seeds)}) "
                    f"å°‘äºç›®æ ‡æ•°({target_seed_count})ï¼Œ"
                    f"å»ºè®®æ”¾å®½æ£€ç´¢æ¡ä»¶æˆ–è°ƒæ•´å¹´ä»½é™åˆ¶"
                )

        except Exception as e:
            logger.error(f"ç§å­æ£€ç´¢ä¸æ˜ å°„å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            self.seed_papers = []
            self.mapped_seeds = []
            self.unmapped_seeds = []

    def _step2_cross_database_mapping(self):
        """
        æ­¥éª¤2: è·¨åº“IDæ˜ å°„ï¼ˆå·²æ•´åˆåˆ°æ­¥éª¤1ï¼‰

        æ³¨æ„ï¼šæ­¤æ­¥éª¤å·²ä¸æ­¥éª¤1åˆå¹¶ï¼Œæ­¤æ–¹æ³•ä»…ä½œä¸ºå ä½ç¬¦ä¿ç•™
        å®é™…æ˜ å°„é€»è¾‘å·²åœ¨ _step1_seed_retrieval ä¸­å®Œæˆ
        """
        logger.info("\n" + "-"*70)
        logger.info("æ­¥éª¤2: è·¨åº“IDæ˜ å°„ (å·²æ•´åˆåˆ°æ­¥éª¤1)")
        logger.info("-"*70)
        logger.info("âœ“ æ˜ å°„å·²åœ¨æ­¥éª¤1å®Œæˆï¼Œè·³è¿‡")

        # ç»Ÿè®¡ä¿¡æ¯å·²åœ¨æ­¥éª¤1æ›´æ–°ï¼Œè¿™é‡Œæ— éœ€é¢å¤–æ“ä½œ
        pass

    def _handle_unmapped_seeds(self):
        """å¤„ç†æ˜ å°„å¤±è´¥çš„ç§å­è®ºæ–‡"""
        manual_count = 0

        for seed in self.unmapped_seeds:
            try:
                # å°è¯•åœ¨OpenAlexä¸­æœç´¢
                title = seed.get('title', '')
                if not title:
                    continue

                search_results = self.openalex_client.search_papers(
                    topic=title,
                    max_results=1,
                    sort_by="relevance"
                )

                if search_results:
                    paper = search_results[0]
                    paper_id = paper['id']
                    self.papers[paper_id] = paper
                    manual_count += 1
                    logger.info(f"  æ‰‹åŠ¨æœç´¢æˆåŠŸ: {title[:50]}")

            except Exception as e:
                logger.debug(f"  æ‰‹åŠ¨æœç´¢å¤±è´¥: {seed.get('title', 'Unknown')[:50]} - {e}")

        self.statistics['manual_citations_built'] = manual_count
        logger.info(f"  - æ‰‹åŠ¨æœç´¢æˆåŠŸ: {manual_count} ç¯‡")

    def _step3_forward_snowballing(self):
        """
        æ­¥éª¤3: æ­£å‘æ»šé›ªçƒ
        æ‰¾åˆ°å“ªäº›è®ºæ–‡å¼•ç”¨äº†ç§å­è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
        """
        logger.info("\n" + "-"*70)
        logger.info("æ­¥éª¤3: æ­£å‘æ»šé›ªçƒ (Forward Snowballing)")
        logger.info("-"*70)
        logger.info("ç­–ç•¥: Seed -> è°å¼•ç”¨äº†Seed? -> å­èŠ‚ç‚¹")

        if not self.mapped_seeds:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„æ˜ å°„ç§å­ï¼Œè·³è¿‡æ­£å‘æ»šé›ªçƒ")
            return

        citing_papers = []
        citations_per_seed = self.config['citations_per_seed']

        logger.info(f"å¼€å§‹å¤„ç† {len(self.mapped_seeds)} ä¸ªç§å­è®ºæ–‡ï¼Œæ¯ä¸ªè·å–æœ€å¤š {citations_per_seed} ç¯‡å¼•ç”¨...")

        for i, seed in enumerate(self.mapped_seeds, 1):
            seed_id = seed['id']
            seed_title = seed.get('title', 'Unknown')

            try:
                # è·å–å¼•ç”¨è¯¥ç§å­è®ºæ–‡çš„è®ºæ–‡
                citations = self.openalex_client.get_citations(
                    paper_id=seed_id,
                    max_results=citations_per_seed
                )

                # æ·»åŠ å¼•ç”¨è®ºæ–‡
                new_papers_count = 0
                for citation in citations:
                    citation_id = citation['id']

                    # æ·»åŠ åˆ°è®ºæ–‡é›†åˆ
                    if citation_id not in self.papers:
                        self.papers[citation_id] = citation
                        citing_papers.append(citation)
                        new_papers_count += 1

                    # æ·»åŠ å¼•ç”¨è¾¹: citation -> seed
                    edge = (citation_id, seed_id)
                    if edge not in self.citation_edges:
                        self.citation_edges.append(edge)

                # ç®€åŒ–è¾“å‡ºï¼šåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯
                logger.info(f"  [{i}/{len(self.mapped_seeds)}] {seed_title[:50]}... â†’ +{new_papers_count} æ–°è®ºæ–‡ (å…±{len(citations)}ç¯‡å¼•ç”¨)")

            except Exception as e:
                logger.warning(f"  [{i}/{len(self.mapped_seeds)}] è·å–å¼•ç”¨å¤±è´¥: {seed_title[:40]}... - {e}")

        self.forward_papers = citing_papers
        logger.info(f"\nâœ… æ­£å‘æ»šé›ªçƒå®Œæˆ: æ–°å¢ {len(citing_papers)} ç¯‡å¼•ç”¨è®ºæ–‡")

    def _step4_backward_snowballing(self):
        """
        æ­¥éª¤4: åå‘æ»šé›ªçƒ
        æ‰¾åˆ°ç§å­è®ºæ–‡å¼•ç”¨äº†å“ªäº›è®ºæ–‡
        """
        logger.info("\n" + "-"*70)
        logger.info("æ­¥éª¤4: åå‘æ»šé›ªçƒ (Backward Snowballing)")
        logger.info("-"*70)
        logger.info("ç­–ç•¥: è°è¢«Seedå¼•ç”¨äº†? <- Seed -> çˆ¶èŠ‚ç‚¹/ç¥–å…ˆ")

        if not self.mapped_seeds:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„æ˜ å°„ç§å­ï¼Œè·³è¿‡åå‘æ»šé›ªçƒ")
            return

        referenced_papers = []
        references_per_seed = self.config['references_per_seed']

        logger.info(f"å¼€å§‹å¤„ç† {len(self.mapped_seeds)} ä¸ªç§å­è®ºæ–‡ï¼Œæ¯ä¸ªè·å–æœ€å¤š {references_per_seed} ç¯‡å‚è€ƒæ–‡çŒ®...")

        for i, seed in enumerate(self.mapped_seeds, 1):
            seed_id = seed['id']
            seed_title = seed.get('title', 'Unknown')

            try:
                # è·å–è¯¥ç§å­è®ºæ–‡å¼•ç”¨çš„è®ºæ–‡
                references = self.openalex_client.get_references(
                    paper_id=seed_id,
                    max_results=references_per_seed
                )

                # æ·»åŠ å¼•ç”¨è®ºæ–‡
                new_papers_count = 0
                for reference in references:
                    reference_id = reference['id']

                    # æ·»åŠ åˆ°è®ºæ–‡é›†åˆ
                    if reference_id not in self.papers:
                        self.papers[reference_id] = reference
                        referenced_papers.append(reference)
                        new_papers_count += 1

                    # æ·»åŠ å¼•ç”¨è¾¹: seed -> reference
                    edge = (seed_id, reference_id)
                    if edge not in self.citation_edges:
                        self.citation_edges.append(edge)

                # ç®€åŒ–è¾“å‡ºï¼šåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯
                logger.info(f"  [{i}/{len(self.mapped_seeds)}] {seed_title[:50]}... â†’ +{new_papers_count} æ–°è®ºæ–‡ (å…±{len(references)}ç¯‡å‚è€ƒ)")

            except Exception as e:
                logger.warning(f"  [{i}/{len(self.mapped_seeds)}] è·å–å‚è€ƒæ–‡çŒ®å¤±è´¥: {seed_title[:40]}... - {e}")

        self.backward_papers = referenced_papers
        logger.info(f"\nâœ… åå‘æ»šé›ªçƒå®Œæˆ: æ–°å¢ {len(referenced_papers)} ç¯‡ç¥–å…ˆè®ºæ–‡")

    def _step5_cocitation_mining(self):
        """
        æ­¥éª¤5: å…±å¼•æŒ–æ˜
        æ‰¾åˆ°åœ¨å­èŠ‚ç‚¹å’Œçˆ¶èŠ‚ç‚¹ä¸­è¢«åå¤æåŠçš„é«˜ä»·å€¼è®ºæ–‡
        """
        logger.info("\n" + "-"*70)
        logger.info("æ­¥éª¤5: å…±å¼•æŒ–æ˜ (Co-citation Mining)")
        logger.info("-"*70)
        logger.info("ç­–ç•¥: ç»Ÿè®¡å…±åŒå¼•ç”¨çš„è®ºæ–‡")

        # åˆå¹¶ç¬¬ä¸€è½®çš„ç§å­è®ºæ–‡ + å¼•ç”¨è®ºæ–‡
        first_round_papers = self.forward_papers + self.backward_papers

        if not first_round_papers:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„ç¬¬ä¸€è½®è®ºæ–‡ï¼Œè·³è¿‡å…±å¼•æŒ–æ˜")
            return

        # ç»Ÿè®¡è®ºæ–‡è¢«å¼•ç”¨çš„æ¬¡æ•°
        cocitation_counter = Counter()

        for paper in first_round_papers:
            paper_id = paper['id']

            try:
                # è·å–è¯¥è®ºæ–‡çš„å‚è€ƒæ–‡çŒ®
                references = self.openalex_client.get_references(
                    paper_id=paper_id,
                    max_results=20
                )

                # ç»Ÿè®¡å‚è€ƒæ–‡çŒ®è¢«å¼•ç”¨æ¬¡æ•°
                for ref in references:
                    ref_id = ref['id']
                    # åªç»Ÿè®¡ä¸åœ¨å½“å‰è®ºæ–‡é›†åˆä¸­çš„è®ºæ–‡
                    if ref_id not in self.papers:
                        cocitation_counter[ref_id] += 1

            except Exception as e:
                logger.debug(f"  è·å–å‚è€ƒæ–‡çŒ®å¤±è´¥: {paper['title'][:50]} - {e}")

        # æ ¹æ®å…±å¼•æ¬¡æ•°è¿‡æ»¤è®ºæ–‡
        threshold = self.config['cocitation_threshold']
        max_papers = self.config['max_cocitation_papers']

        cocited_paper_ids = [
            paper_id for paper_id, count in cocitation_counter.most_common()
            if count >= threshold
        ][:max_papers]

        logger.info(f"æ‰¾åˆ° {len(cocited_paper_ids)} ä¸ªå…±å¼•è®ºæ–‡(é˜ˆå€¼â‰¥{threshold})")

        # è·å–å…±å¼•è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
        cocitation_papers = []
        for paper_id in cocited_paper_ids:
            try:
                paper = self.openalex_client.get_paper_by_id(paper_id)
                if paper:
                    self.papers[paper_id] = paper
                    cocitation_papers.append(paper)

                    # æ·»åŠ å¼•ç”¨è¾¹(ä»å¼•ç”¨å…±å¼•è®ºæ–‡çš„è®ºæ–‡åˆ°å…±å¼•è®ºæ–‡)
                    for citing_paper in first_round_papers:
                        citing_id = citing_paper['id']
                        try:
                            refs = self.openalex_client.get_references(citing_id, max_results=50)
                            if any(r['id'] == paper_id for r in refs):
                                edge = (citing_id, paper_id)
                                if edge not in self.citation_edges:
                                    self.citation_edges.append(edge)
                        except:
                            pass

            except Exception as e:
                logger.debug(f"  è·å–å…±å¼•è®ºæ–‡å¤±è´¥: {paper_id} - {e}")

        self.cocitation_papers = cocitation_papers
        logger.info(f"å…±å¼•æŒ–æ˜è·å– {len(cocitation_papers)} ç¯‡å…±å¼•è®ºæ–‡")

    def _step6_second_round_snowballing(self):
        """
        æ­¥éª¤6: ç¬¬äºŒè½®æ»šé›ªçƒ(å¯é€‰)
        å¯¹ç¬¬ä¸€è½®è®ºæ–‡å†è¿›è¡Œä¸€è½®å—æ§æ‰©å±•
        """
        logger.info("\n" + "-"*70)
        logger.info("æ­¥éª¤6: ç¬¬äºŒè½®æ»šé›ªçƒ (Second-Round Snowballing)")
        logger.info("-"*70)
        logger.info("ç­–ç•¥: å¯¹ç¬¬ä¸€è½®è®ºæ–‡å†è¿›è¡Œä¸€è½®å—æ§æ‰©å±•")

        self.statistics['second_round_enabled'] = True

        # é€‰æ‹©é«˜è´¨é‡çš„ç¬¬ä¸€è½®è®ºæ–‡è¿›è¡Œæ‰©å±•
        first_round_papers = self.forward_papers + self.backward_papers + self.cocitation_papers

        # é€‰æ‹©å¼•ç”¨é‡æœ€é«˜çš„topè®ºæ–‡
        sorted_papers = sorted(
            first_round_papers,
            key=lambda p: p.get('cited_by_count', 0),
            reverse=True
        )

        max_expand = self.config['second_round_max_papers']
        papers_to_expand = sorted_papers[:max_expand]

        logger.info(f"é€‰æ‹© {len(papers_to_expand)} ç¯‡é«˜å¼•ç”¨è®ºæ–‡è¿›è¡Œç¬¬äºŒè½®æ‰©å±•")

        second_round_citing = []
        second_round_ancestor = []
        limit_per_paper = self.config['second_round_limit']

        for i, paper in enumerate(papers_to_expand, 1):
            paper_id = paper['id']
            paper_title = paper.get('title', 'Unknown')

            try:
                # æ­£å‘æ»šé›ªçƒ(è·å–å¼•ç”¨è¯¥è®ºæ–‡çš„è®ºæ–‡)
                citations = self.openalex_client.get_citations(
                    paper_id=paper_id,
                    max_results=limit_per_paper
                )

                for citation in citations:
                    citation_id = citation['id']
                    if citation_id not in self.papers:
                        self.papers[citation_id] = citation
                        second_round_citing.append(citation)

                    edge = (citation_id, paper_id)
                    if edge not in self.citation_edges:
                        self.citation_edges.append(edge)

                # åå‘æ»šé›ªçƒ(è·å–è¯¥è®ºæ–‡å¼•ç”¨çš„è®ºæ–‡)
                references = self.openalex_client.get_references(
                    paper_id=paper_id,
                    max_results=limit_per_paper
                )

                for reference in references:
                    reference_id = reference['id']
                    if reference_id not in self.papers:
                        self.papers[reference_id] = reference
                        second_round_ancestor.append(reference)

                    edge = (paper_id, reference_id)
                    if edge not in self.citation_edges:
                        self.citation_edges.append(edge)

                if i <= 5 or i % 10 == 0:
                    logger.info(f"  [{i}/{len(papers_to_expand)}] {paper_title[:50]}... -> {len(citations)}å¼•ç”¨ + {len(references)}å‚è€ƒ")

            except Exception as e:
                logger.debug(f"  ç¬¬äºŒè½®æ‰©å±•å¤±è´¥: {paper_title[:50]} - {e}")

        # ä¿å­˜ç¬¬äºŒè½®ç»“æœ
        self.second_round_citing = second_round_citing
        self.second_round_ancestor = second_round_ancestor
        self.second_round_papers = second_round_citing + second_round_ancestor

        logger.info(f"ç¬¬äºŒè½®æ»šé›ªçƒç»“æœ:")
        logger.info(f"  - æ­£å‘å­èŠ‚ç‚¹: {len(second_round_citing)} ç¯‡")
        logger.info(f"  - åå‘ç¥–å…ˆ: {len(second_round_ancestor)} ç¯‡")

    def _step7_recent_frontiers(
        self,
        topic: str,
        keywords: Optional[List[str]],
        categories: Optional[List[str]]
    ):
        """
        æ­¥éª¤7: è¡¥å……æœ€æ–°SOTA
        ä»arXivè·å–æœ€è¿‘6-12ä¸ªæœˆçš„æœ€æ–°è®ºæ–‡
        """
        logger.info("\n" + "-"*70)
        logger.info("æ­¥éª¤7: è¡¥å……æœ€æ–°SOTA (Recent Frontiers Supplementation)")
        logger.info("-"*70)
        logger.info(f"ç­–ç•¥: arXivæœ€è¿‘{self.config['recent_months']}ä¸ªæœˆè®ºæ–‡")

        try:
            # ä½¿ç”¨arXivæ£€ç´¢å™¨è·å–æœ€æ–°è®ºæ–‡
            try:
                import arxiv
            except ImportError:
                logger.warning("arxivåŒ…æœªå®‰è£…ï¼Œè·³è¿‡æœ€æ–°è®ºæ–‡è¡¥å……")
                self.recent_papers = []
                return

            # è®¡ç®—æ—¥æœŸèŒƒå›´
            months_back = self.config['recent_months']
            start_date = datetime.now() - timedelta(days=30 * months_back)

            logger.info(f"  - æ—¶é—´èŒƒå›´: >= {start_date.strftime('%Y-%m-%d')}")
            logger.info(f"  - ç›®æ ‡æ•°é‡: {self.config['recent_count']} ç¯‡")

            # ä¸´æ—¶é™ä½ç›¸å…³æ€§é˜ˆå€¼ä»¥è·å–æ›´å¤šæœ€æ–°è®ºæ–‡
            original_threshold = self.arxiv_retriever.min_relevance_score
            self.arxiv_retriever.min_relevance_score = 0.25  # è¿›ä¸€æ­¥æ”¾å®½åˆ°0.25ï¼ˆæœ€æ–°è®ºæ–‡å¯èƒ½ç›¸å…³æ€§è¯„åˆ†åä½ï¼‰

            recent_papers_raw = self.arxiv_retriever.retrieve_seed_papers(
                topic=topic,
                keywords=keywords,
                categories=categories,
                max_seeds=self.config['recent_count'] * 3,  # å¤šå–ä¸€äº›
                sort_by=arxiv.SortCriterion.SubmittedDate  # æŒ‰æäº¤æ—¥æœŸæ’åº
            )

            # æ¢å¤åŸé˜ˆå€¼
            self.arxiv_retriever.min_relevance_score = original_threshold

            logger.info(f"  â†’ arXivæ£€ç´¢åˆ° {len(recent_papers_raw)} ç¯‡å€™é€‰è®ºæ–‡")

            # è¿‡æ»¤åªä¿ç•™æœ€è¿‘æ—¶é—´çš„è®ºæ–‡
            recent_filtered = []
            for paper in recent_papers_raw:
                # ä½¿ç”¨published_dateå­—æ®µè¿›è¡Œç²¾ç¡®çš„æ—¶é—´è¿‡æ»¤
                published_date = paper.get('published_date')

                if published_date:
                    # å¦‚æœpublished_dateæ˜¯datetimeå¯¹è±¡
                    if isinstance(published_date, datetime):
                        pub_date = published_date
                    else:
                        # å°è¯•è§£æå­—ç¬¦ä¸²
                        try:
                            pub_date = datetime.fromisoformat(str(published_date).replace('Z', '+00:00'))
                        except:
                            # å›é€€åˆ°å¹´ä»½æ¯”è¾ƒ
                            pub_year = paper.get('year', 0)
                            if pub_year >= start_date.year:
                                recent_filtered.append(paper)
                            continue

                    # ç¡®ä¿pub_dateæ˜¯naive datetimeï¼ˆç§»é™¤æ—¶åŒºä¿¡æ¯ä»¥ä¾¿ä¸start_dateæ¯”è¾ƒï¼‰
                    if pub_date.tzinfo is not None:
                        pub_date = pub_date.replace(tzinfo=None)

                    # æ¯”è¾ƒå®Œæ•´çš„æ—¥æœŸ
                    if pub_date >= start_date:
                        recent_filtered.append(paper)
                        logger.debug(f"  âœ“ ä¿ç•™: {paper['title'][:50]}... ({pub_date.strftime('%Y-%m-%d')})")
                    else:
                        logger.debug(f"  Ã— è¿‡æ»¤: {paper['title'][:50]}... ({pub_date.strftime('%Y-%m-%d')}ï¼Œæ—©äº{start_date.strftime('%Y-%m-%d')})")
                else:
                    # æ²¡æœ‰æ—¥æœŸä¿¡æ¯ï¼Œä½¿ç”¨å¹´ä»½ä½œä¸ºåå¤‡
                    pub_year = paper.get('year', 0)
                    if pub_year >= start_date.year:
                        recent_filtered.append(paper)
                        logger.debug(f"  âœ“ ä¿ç•™: {paper['title'][:50]}... (å¹´ä»½:{pub_year})")

            recent_filtered = recent_filtered[:self.config['recent_count']]

            logger.info(f"  â†’ è¿‡æ»¤åä¿ç•™ {len(recent_filtered)} ç¯‡æœ€è¿‘{months_back}ä¸ªæœˆçš„è®ºæ–‡")

            # æ˜ å°„åˆ°OpenAlex
            if recent_filtered:
                logger.info(f"\nå¼€å§‹æ˜ å°„åˆ°OpenAlex...")
                mapped_recent, _ = self.cross_mapper.map_arxiv_to_openalex(
                    arxiv_papers=recent_filtered,
                    verify_concepts=False
                )

                logger.info(f"  â†’ æ˜ å°„æˆåŠŸ {len(mapped_recent)} ç¯‡")

                # æ·»åŠ åˆ°è®ºæ–‡é›†åˆ
                for paper in mapped_recent:
                    paper_id = paper['id']
                    if paper_id not in self.papers:
                        self.papers[paper_id] = paper
                        self.recent_papers.append(paper)

                        # å°è¯•è¿æ¥å¼•ç”¨å…³ç³»è®ºæ–‡
                        self._connect_recent_paper(paper)

            logger.info(f"\nâœ… è¡¥å……æœ€æ–°è®ºæ–‡: æ–°å¢ {len(self.recent_papers)} ç¯‡æœ€æ–°è®ºæ–‡")

        except Exception as e:
            logger.error(f"è¡¥å……æœ€æ–°è®ºæ–‡å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            self.recent_papers = []

    def _connect_recent_paper(self, paper: Dict):
        """å°†æœ€æ–°è®ºæ–‡è¿æ¥åˆ°å¼•ç”¨è®ºæ–‡ç½‘ç»œ"""
        paper_id = paper['id']

        try:
            # è·å–è¯¥è®ºæ–‡çš„å‚è€ƒæ–‡çŒ®
            references = self.openalex_client.get_references(
                paper_id=paper_id,
                max_results=20
            )

            # å¦‚æœå‚è€ƒæ–‡çŒ®åœ¨æˆ‘ä»¬çš„è®ºæ–‡é›†åˆä¸­ï¼Œæ·»åŠ å¼•ç”¨è¾¹
            for ref in references:
                ref_id = ref['id']
                if ref_id in self.papers:
                    edge = (paper_id, ref_id)
                    if edge not in self.citation_edges:
                        self.citation_edges.append(edge)

        except Exception as e:
            logger.debug(f"  è¿æ¥æœ€æ–°è®ºæ–‡å¤±è´¥: {paper['title'][:50]} - {e}")

    def _step8_citation_closure(self):
        """
        æ­¥éª¤8: æ„å»ºå¼•ç”¨é—­åŒ…
        å¡«è¡¥è®ºæ–‡é›†åˆä¹‹é—´ç¼ºå¤±çš„å¼•ç”¨å…³ç³»ï¼Œæ„å»ºå®Œæ•´ç½‘ç»œ
        """
        logger.info("\n" + "-"*70)
        logger.info("æ­¥éª¤8: æ„å»ºå¼•ç”¨é—­åŒ… (Citation Closure Construction)")
        logger.info("-"*70)
        logger.info("ç­–ç•¥: è¡¥å…¨è®ºæ–‡é›†åˆä¹‹é—´å¼•ç”¨å…³ç³»")

        initial_edges = len(self.citation_edges)
        paper_ids = list(self.papers.keys())

        # ä¼˜åŒ–: ä½¿ç”¨é›†åˆåŠ é€ŸæŸ¥æ‰¾
        paper_ids_set = set(paper_ids)
        citation_edges_set = set(self.citation_edges)

        logger.info(f"  å½“å‰è®ºæ–‡æ•°: {len(paper_ids)}")
        logger.info(f"  å½“å‰å¼•ç”¨è¾¹æ•°: {initial_edges}")

        # è®¡ç®—å®é™…æ£€æŸ¥æ•°é‡
        max_check = min(50, len(paper_ids))
        logger.info(f"  å°†æ£€æŸ¥å‰ {max_check} ç¯‡è®ºæ–‡çš„å¼•ç”¨å…³ç³»\n")

        # æ£€æŸ¥è®ºæ–‡ä¹‹é—´çš„å¼•ç”¨å…³ç³»ç¼ºå¤±å¼•ç”¨å…³ç³»
        new_edges = 0
        checked_papers = 0
        failed_papers = 0
        start_time = time.time()

        for i, source_id in enumerate(paper_ids[:max_check]):
            try:
                # è·å–è¯¥è®ºæ–‡çš„å‚è€ƒæ–‡çŒ®
                references = self.openalex_client.get_references(
                    paper_id=source_id,
                    max_results=50
                )

                # æ‰¹é‡æ£€æŸ¥å¼•ç”¨å…³ç³»
                for ref in references:
                    ref_id = ref['id']
                    if ref_id in paper_ids_set:
                        edge = (source_id, ref_id)
                        if edge not in citation_edges_set:
                            self.citation_edges.append(edge)
                            citation_edges_set.add(edge)
                            new_edges += 1

                checked_papers += 1

                # ä¼˜åŒ–è¾“å‡ºé¢‘ç‡: æ¯20%æˆ–æ¯10ç¯‡è¾“å‡ºä¸€æ¬¡
                if checked_papers % max(1, max_check // 5) == 0 or checked_papers % 10 == 0:
                    progress = (checked_papers / max_check) * 100
                    elapsed = time.time() - start_time
                    rate = checked_papers / elapsed if elapsed > 0 else 0
                    eta = (max_check - checked_papers) / rate if rate > 0 else 0

                    logger.info(
                        f"  è¿›åº¦: [{checked_papers}/{max_check}] {progress:.0f}% | "
                        f"æ–°å¢è¾¹: {new_edges} | "
                        f"å¤±è´¥: {failed_papers} | "
                        f"é€Ÿåº¦: {rate:.1f}ç¯‡/s | "
                        f"é¢„è®¡å‰©ä½™: {eta:.0f}s"
                    )

            except Exception as e:
                failed_papers += 1
                logger.debug(f"  è·³è¿‡è®ºæ–‡ {source_id[:20]}... : {str(e)[:50]}")

        # æœ€ç»ˆç»Ÿè®¡
        elapsed_total = time.time() - start_time
        logger.info(f"\nâœ… å¼•ç”¨é—­åŒ…æ„å»ºå®Œæˆ (è€—æ—¶ {elapsed_total:.1f}s):")
        logger.info(f"  æ£€æŸ¥è®ºæ–‡: {checked_papers}/{max_check}")
        logger.info(f"  å¤±è´¥è®ºæ–‡: {failed_papers}")
        logger.info(f"  åˆå§‹å¼•ç”¨è¾¹: {initial_edges}")
        logger.info(f"  æ–°å¢å¼•ç”¨è¾¹: {new_edges}")
        logger.info(f"  æœ€ç»ˆå¼•ç”¨è¾¹: {len(self.citation_edges)}")

        if new_edges > 0:
            logger.info(f"  å¢é•¿ç‡: +{(new_edges/initial_edges*100):.1f}%")

        # è®¡ç®—ç½‘ç»œå¯†åº¦
        if len(paper_ids) > 1:
            max_possible_edges = len(paper_ids) * (len(paper_ids) - 1)
            density = len(self.citation_edges) / max_possible_edges * 100
            logger.info(f"  ç½‘ç»œå¯†åº¦: {density:.2f}%")

    def _finalize_statistics(self):
        """æ›´æ–°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        self.statistics.update({
            'seed_papers': len(self.seed_papers),
            'arxiv_mapped': len(self.mapped_seeds),
            'arxiv_unmapped': len(self.unmapped_seeds),
            'first_round_citing': len(self.forward_papers),
            'first_round_ancestor': len(self.backward_papers),
            'first_round_cocitation': len(self.cocitation_papers),
            # ä¿®å¤ï¼šä½¿ç”¨å•ç‹¬ç¼“å­˜çš„åˆ—è¡¨
            'second_round_citing': len(self.second_round_citing),
            'second_round_ancestor': len(self.second_round_ancestor),
            'recent_papers': len(self.recent_papers),
            'total_papers': len(self.papers),
            'total_edges': len(self.citation_edges),
            # æ·»åŠ ç§å­èŠ‚ç‚¹IDåˆ—è¡¨
            'seed_ids': [p['id'] for p in self.mapped_seeds]
        })

    def _print_summary(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡æ‘˜è¦"""
        stats = self.statistics

        logger.info("\n" + "="*70)
        logger.info("8æ­¥æ£€ç´¢æµç¨‹ç»Ÿè®¡æ‘˜è¦")
        logger.info("="*70)
        logger.info(f"ç§å­è®ºæ–‡")
        logger.info(f"  - æ€»ç§å­æ•°: {stats['seed_papers']}")
        logger.info(f"  - arXivæ˜ å°„æˆåŠŸ: {stats['arxiv_mapped']}")
        logger.info(f"  - arXivæ˜ å°„å¤±è´¥: {stats['arxiv_unmapped']}")
        if stats.get('seed_ids'):
            logger.info(f"  - ç§å­èŠ‚ç‚¹ID: {stats['seed_ids'][:3]}{'...' if len(stats['seed_ids']) > 3 else ''}")
        if stats['manual_citations_built'] > 0:
            logger.info(f"  - æ‰‹åŠ¨æœç´¢è¡¥å……: {stats['manual_citations_built']}")

        logger.info(f"ç¬¬ä¸€è½®æ»šé›ªçƒ")
        logger.info(f"  - æ­£å‘å­èŠ‚ç‚¹: {stats['first_round_citing']}")
        logger.info(f"  - åå‘ç¥–å…ˆ: {stats['first_round_ancestor']}")
        logger.info(f"  - å…±å¼•è®ºæ–‡: {stats['first_round_cocitation']}")

        if stats['second_round_enabled']:
            logger.info(f"ç¬¬äºŒè½®æ»šé›ªçƒ")
            logger.info(f"  - æ­£å‘å­èŠ‚ç‚¹: {stats['second_round_citing']}")
            logger.info(f"  - åå‘ç¥–å…ˆ: {stats['second_round_ancestor']}")

        logger.info(f"æœ€æ–°SOTA")
        logger.info(f"  - æœ€æ–°è®ºæ–‡: {stats['recent_papers']}")

        logger.info(f"æœ€ç»ˆç»“æœ")
        logger.info(f"  - æ€»è®ºæ–‡æ•°: {stats['total_papers']}")
        logger.info(f"  - å¼•ç”¨å…³ç³»æ•°: {stats['total_edges']}")

        if stats['total_papers'] > 0:
            avg_degree = stats['total_edges'] / stats['total_papers']
            logger.info(f"  - å¹³å‡è¿æ¥åº¦: {avg_degree:.2f}")

        logger.info("="*70)

    def get_statistics(self) -> Dict:
        """è¿”å›ç»Ÿè®¡ä¿¡æ¯"""
        return self.statistics.copy()

    def get_papers(self) -> Dict[str, Dict]:
        """è¿”å›æ‰€æœ‰è®ºæ–‡"""
        return self.papers.copy()

    def get_citation_edges(self) -> List[Tuple[str, str]]:
        """è¿”å›æ‰€æœ‰å¼•ç”¨å…³ç³»"""
        return self.citation_edges.copy()


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

if __name__ == "__main__":
    # åˆå§‹åŒ–æµç¨‹
    pipeline = PaperSearchPipeline(
        config_path='./config/config.yaml'
    )

    # æ‰§è¡Œå®Œæ•´çš„8æ­¥æ£€ç´¢
    result = pipeline.execute_full_pipeline(
        topic="Natural Language Processing",
        keywords=["transformer", "attention", "BERT", "GPT"],
        categories=["cs.CL", "cs.AI"]
    )

    # è¾“å‡ºç»“æœ
    print("\n" + "="*70)
    print("æœ€ç»ˆæ£€ç´¢ç»“æœ:")
    print("="*70)
    print(f"æ€»è®ºæ–‡æ•°: {len(result['papers'])}")
    print(f"å¼•ç”¨å…³ç³»æ•°: {len(result['citation_edges'])}")
    print(f"å¹³å‡è¿æ¥åº¦: {len(result['citation_edges']) / len(result['papers']):.2f}")
    print("="*70)
