{
  "topic": "Natural Language Processing",
  "timestamp": "2025-12-18T20:22:05.368355",
  "pruning_stats": {
    "original_papers": 226,
    "seed_papers": 5,
    "pruned_papers": 20,
    "removed_papers": 206,
    "original_edges": 282,
    "strong_edges": 17,
    "weak_edges_removed": 5,
    "retention_rate": 0.08849557522123894,
    "relation_type_distribution": {
      "Alternative": 1,
      "Overcomes": 10,
      "Adapts_to": 5,
      "Extends": 1,
      "Baselines": 5
    }
  },
  "evolutionary_paths": [
    {
      "thread_type": "star",
      "pattern_type": "The Star (星型爆发)",
      "title": "针对 The performance of general NLP models, specifically BERT, is unsatisfactory in b 的多技术路线博弈",
      "narrative": "**焦点**:  \n中心论文《BioBERT: a pre-trained biomedical language representation model for biomedical text mining》在2019年提出了首个针对生物医学领域的预训练语言模型，其在处理生物医学文本挖掘任务上取得了显著进展。然而，该模型依赖于大量的计算资源进行预训练，这成为其广泛应用的一大瓶颈。同时，通用NLP模型如BERT在处理生物医学文本时表现仍不尽如人意，特别是在专业术语和领域专属知识的理解上。\n\n**分歧**:  \n在BioBERT之后，研究者们探索了两条改善路径。第一条演进路线强调通过在领域特定的生物医学文本上进行语言模型的预训练，以强化模型对专业术语和领域知识的理解能力，其代表作是2021年的论文《Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing》。第二条路线则从公共领域的临床文本入手，公开了一系列基于临床数据训练的特定领域BERT模型，这一方向的先驱性工作为2019年的《Publicly Available Clinical》。这种方法侧重于提升模型对临床语境的适应性。\n\n**对比**:  \n两种技术路线均致力于改善生物医学领域内语言模型的效果，但采用了不同的重点和策略。第一条路线通过利用生物医学领域的专属文本进行模型预训练，专注于提升模型对特定领域内容的掌握力；第二条路线着眼于临床文本的公开访问，以增加模型在实际应用场景中的泛化能力。同时，第一条路线在2023年进一步发展，通过引入MultiMedQA基准，整合多个医学问题回答任务以测试模型的临床知识编码能力，相较于第二条路径在指标上更具全面性。",
      "center_paper": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
      "routes_count": 2,
      "routes": [
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021,
              "cited_by_count": 1737
            },
            {
              "paper_id": "W4384071683",
              "title": "Large language models encode clinical knowledge",
              "year": 2023,
              "cited_by_count": 2248
            },
            {
              "paper_id": "W4387500346",
              "title": "The future landscape of large language models in medicine",
              "year": 2023,
              "cited_by_count": 732
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W2963716420",
              "title": "Publicly Available Clinical",
              "year": 2019,
              "cited_by_count": 1422
            },
            {
              "paper_id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021,
              "cited_by_count": 1737
            },
            {
              "paper_id": "W4384071683",
              "title": "Large language models encode clinical knowledge",
              "year": 2023,
              "cited_by_count": 2248
            },
            {
              "paper_id": "W4387500346",
              "title": "The future landscape of large language models in medicine",
              "year": 2023,
              "cited_by_count": 732
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W2911489562",
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "year": 2019,
          "cited_by_count": 6148,
          "role": "center"
        },
        {
          "paper_id": "W3046375318",
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
          "year": 2021,
          "cited_by_count": 1737
        },
        {
          "paper_id": "W4384071683",
          "title": "Large language models encode clinical knowledge",
          "year": 2023,
          "cited_by_count": 2248
        },
        {
          "paper_id": "W4387500346",
          "title": "The future landscape of large language models in medicine",
          "year": 2023,
          "cited_by_count": 732
        },
        {
          "paper_id": "W2963716420",
          "title": "Publicly Available Clinical",
          "year": 2019,
          "cited_by_count": 1422
        },
        {
          "paper_id": "W3046375318",
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
          "year": 2021,
          "cited_by_count": 1737
        },
        {
          "paper_id": "W4384071683",
          "title": "Large language models encode clinical knowledge",
          "year": 2023,
          "cited_by_count": 2248
        },
        {
          "paper_id": "W4387500346",
          "title": "The future landscape of large language models in medicine",
          "year": 2023,
          "cited_by_count": 732
        }
      ],
      "total_citations": 17004,
      "visual_structure": "Center -> 2 Routes",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019
          },
          "to_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019
          },
          "to_paper": {
            "id": "W2963716420",
            "title": "Publicly Available Clinical",
            "year": 2019
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "route_id": 2,
          "direction": "chronological"
        }
      ]
    },
    {
      "thread_type": "chain",
      "pattern_type": "The Chain (线性链条)",
      "title": "从 BioBERT, a domain-specific pre-trained language representation model, is develop 到 Introduction of MultiMedQA benchmark, which combines multiple medical question-a 的演进之路",
      "narrative": "**起源**: 在2019年，研究者们针对通用自然语言处理（NLP）模型，尤其是BERT，在生物医学领域表现不佳的问题，开发了BioBERT，这是一个专门为生物医学领域文本挖掘而预训练的语言表征模型。BioBERT通过利用领域特定的语料库进行预训练，显著提升了生物医学文本处理的效果。然而，BioBERT的一个主要局限性在于其预训练过程需要大量的计算资源，这对资源有限的研究者而言构成了挑战。\n\n**演进**: 到2021年，研究进一步推进，学者们意识到在通用语料上预训练的语言模型可能会导致生物医学领域NLP任务的负迁移。因此，他们提出了一种方法，强调仅在生物医学领域内的专用文本上进行模型的预训练。虽然这一方法有效减少了负迁移效应，但在全面的生物医学NLP基准测试上仍显局限，尚未充分展示其潜力。\n\n**最新进展**: 2023年，技术发展达到了新的阶段，研究者们着眼于评估大型语言模型在临床知识方面的表现。为解决以往评估中样本限制的问题，提出了MultiMedQA基准，该基准结合了多种医学问答任务，从而提供了更为综合的评估框架。尽管如此，这一方法在生成安全关键型医学问题的适当答案时仍然面临挑战。在这几年的技术演进中，生物医学NLP中的语言模型不断克服前人的不足，朝着更加全面和精确的方向发展。",
      "papers": [
        {
          "paper_id": "W2911489562",
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "year": 2019,
          "cited_by_count": 6148
        },
        {
          "paper_id": "W3046375318",
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
          "year": 2021,
          "cited_by_count": 1737
        },
        {
          "paper_id": "W4384071683",
          "title": "Large language models encode clinical knowledge",
          "year": 2023,
          "cited_by_count": 2248
        }
      ],
      "total_citations": 10133,
      "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019
          },
          "to_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "to_paper": {
            "id": "W4384071683",
            "title": "Large language models encode clinical knowledge",
            "year": 2023
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "direction": "chronological"
        }
      ]
    },
    {
      "thread_type": "chain",
      "pattern_type": "The Chain (线性链条)",
      "title": "从 Creation and public release of domain-specific BERT models trained on clinical t 到 Pretraining language models solely on domain-specific in-domain biomedical text  的演进之路",
      "narrative": "**起源**  \n在2019年，一篇重要的论文提出了创建并公开发布专为临床领域设计的BERT预训练模型的构想。这项研究旨在解决缺乏专用临床BERT模型的问题，以提高自然语言处理技术在临床文本分析中的性能。然而，这一方法在去标识化（de-ID）任务方面表现欠佳，尤其是在某些数据集（如i2b2 2006和i2b2 2014）上表现不尽如人意，显示出模型在特定任务上局限性。\n\n**演进**  \n同年，另一篇论文提出了BioBERT，这是一种为生物医学文本挖掘而专门开发的领域特定预训练语言表示模型。BioBERT针对一般NLP模型在生物医学领域性能不佳的问题进行了改进。这种方法虽然显著提升了领域内任务的性能，但其预训练过程需要大量计算资源，成为其应用的一个限制因素。\n\n**最新进展**  \n到2021年，最新的研究进一步推进了领域特定语言模型的预训练方法。这篇文章解决了因使用通用数据进行预训练导致生物医学领域NLP任务的负迁移问题。通过专注于领域内部生物医学文本的预训练，新方法虽然尚需在生物医学NLP中进行更多比较权威的基准测试，但它为提高领域内模型性能提供了一个新的思路，标志着该领域技术的又一次突破。",
      "papers": [
        {
          "paper_id": "W2963716420",
          "title": "Publicly Available Clinical",
          "year": 2019,
          "cited_by_count": 1422
        },
        {
          "paper_id": "W2911489562",
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "year": 2019,
          "cited_by_count": 6148
        },
        {
          "paper_id": "W3046375318",
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
          "year": 2021,
          "cited_by_count": 1737
        }
      ],
      "total_citations": 9307,
      "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2963716420",
            "title": "Publicly Available Clinical",
            "year": 2019
          },
          "to_paper": {
            "id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Overcomes",
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019
          },
          "to_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "direction": "chronological"
        }
      ]
    },
    {
      "thread_type": "chain",
      "pattern_type": "The Chain (线性链条)",
      "title": "从 Creation and public release of domain-specific BERT models trained on clinical t 到 Introduction of MultiMedQA benchmark, which combines multiple medical question-a 的演进之路",
      "narrative": "**起源**\n\n2019年，第一篇论文“Publicly Available Clinical” 开创了面向临床的BERT模型的研究方向。该研究填补了临床领域缺乏预训练BERT模型的空白，并通过创建和公开发布领域特定的BERT模型，为临床文本任务提供了新的基础。然而，该方法在去识别化任务（如i2b2 2006和i2b2 20）中表现一般，这为后续研究指出了进一步优化的方向。\n\n**演进**\n\n2019年，“Enhancing clinical concept extraction with contextual embeddings”论文紧随其后，针对临床概念提取任务中缺乏最佳实践这一问题，将大型临床语料库如MIMIC上的上下文嵌入用于该任务。这一改进在捕捉语境信息方面显示了优越性，但该方法仍面临预训练语料过拟合的问题。2021年，论文“Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing”进一步发展了领域特定语言模型的预训练方法，通过仅在生物医学域内文本进行训练，减少了跨领域的负迁移现象。然而其不足之处在于缺乏广泛的生物医学NLP基准测试，这也推动了后续对模型评价体系的改进尝试。\n\n**最新进展**\n\n2023年，论文“Large language models encode clinical knowledge”实现了该领域的重要突破。研究引入了MultiMedQA基准，将多个医学问答数据集结合，评估大型语言模型在医学知识上的表现。这不仅提供了更全面的评估工具，也回应了模型生成安全关键医学问题答案的挑战，尽管在某些安全关键问题的回答生成上，模型仍存在困难。在此基础上，未来的研究将致力于进一步优化模型在安全关键情况下的表现，从而增强其临床实用性。",
      "papers": [
        {
          "paper_id": "W2963716420",
          "title": "Publicly Available Clinical",
          "year": 2019,
          "cited_by_count": 1422
        },
        {
          "paper_id": "W2955483668",
          "title": "Enhancing clinical concept extraction with contextual embeddings",
          "year": 2019,
          "cited_by_count": 314
        },
        {
          "paper_id": "W3046375318",
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
          "year": 2021,
          "cited_by_count": 1737
        },
        {
          "paper_id": "W4384071683",
          "title": "Large language models encode clinical knowledge",
          "year": 2023,
          "cited_by_count": 2248
        }
      ],
      "total_citations": 5721,
      "visual_structure": "Paper_1 -> Paper_2 -> Paper_3 -> Paper_4",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2963716420",
            "title": "Publicly Available Clinical",
            "year": 2019
          },
          "to_paper": {
            "id": "W2955483668",
            "title": "Enhancing clinical concept extraction with contextual embeddings",
            "year": 2019
          },
          "relation_type": "Extends",
          "narrative_relation": "Extends",
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2955483668",
            "title": "Enhancing clinical concept extraction with contextual embeddings",
            "year": 2019
          },
          "to_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "to_paper": {
            "id": "W4384071683",
            "title": "Large language models encode clinical knowledge",
            "year": 2023
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "direction": "chronological"
        }
      ]
    },
    {
      "thread_type": "chain",
      "pattern_type": "The Chain (线性链条)",
      "title": "从 SCIBERT, a pretrained language model based on BERT trained on a large corpus of  到 Introduction of MultiMedQA benchmark, which combines multiple medical question-a 的演进之路",
      "narrative": "**起源**\n\n在2019年，SciBERT 的出现为科学文本的自然语言处理开辟了新的方向。该工作旨在解决科学领域中获取大规模标注数据的困难，通过在大规模科学文本语料上预训练语言模型，提升了NLP任务的表现。然而，该模型的局限在于缺乏类似于BERT-Large的版本，这在一定程度上限制了其在更复杂任务中的表现能力。\n\n**演进**\n\n到了2021年，研究者们逐步认识到通用语言模型的预训练可能导致领域内具体任务的负迁移。针对这一不足，研究者提出只在特定领域内的生物医学文本上进行预训练的方法，以更好地服务于生物医学NLP任务。尽管如此，该方法在生物医学NLP中的广泛基准测试仍显不足，需要进一步验证其全面性和可靠性。\n\n**最新进展**\n\n在2023年，研究进一步取得突破，尤其是在大规模语言模型的临床知识编码评估方面。研究人员引入了MultiMedQA基准，该基准整合了多个医学问答数据集，以更全面地评估大语言模型在临床知识领域的表现。然而，新方法在处理安全关键的医学问题时，生成合适答案的能力依然存在挑战，这表明领域内仍有许多未解难题需要探索和解决。",
      "papers": [
        {
          "paper_id": "W2970771982",
          "title": "SciBERT: A Pretrained Language Model for Scientific Text",
          "year": 2019,
          "cited_by_count": 2777
        },
        {
          "paper_id": "W3046375318",
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
          "year": 2021,
          "cited_by_count": 1737
        },
        {
          "paper_id": "W4384071683",
          "title": "Large language models encode clinical knowledge",
          "year": 2023,
          "cited_by_count": 2248
        }
      ],
      "total_citations": 6762,
      "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2970771982",
            "title": "SciBERT: A Pretrained Language Model for Scientific Text",
            "year": 2019
          },
          "to_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "to_paper": {
            "id": "W4384071683",
            "title": "Large language models encode clinical knowledge",
            "year": 2023
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "direction": "chronological"
        }
      ]
    }
  ],
  "survey_report": {
    "title": "Deep Survey: Natural Language Processing",
    "abstract": "本综述基于知识图谱分析了 Natural Language Processing 领域的演进历程。通过关系剪枝，我们从原始图谱中筛选出 20 篇高质量论文，并识别出 5 条关键演化路径。其中包括 4 条线性技术链条和 1 个星型爆发结构，完整呈现了该领域的技术演进脉络和多元化发展趋势。",
    "threads": [
      {
        "thread_id": 1,
        "thread_name": "Thread 1: The Star (星型爆发)",
        "title": "针对 The performance of general NLP models, specifically BERT, is unsatisfactory in b 的多技术路线博弈",
        "pattern_type": "The Star (星型爆发)",
        "thread_type": "star",
        "narrative": "**焦点**:  \n中心论文《BioBERT: a pre-trained biomedical language representation model for biomedical text mining》在2019年提出了首个针对生物医学领域的预训练语言模型，其在处理生物医学文本挖掘任务上取得了显著进展。然而，该模型依赖于大量的计算资源进行预训练，这成为其广泛应用的一大瓶颈。同时，通用NLP模型如BERT在处理生物医学文本时表现仍不尽如人意，特别是在专业术语和领域专属知识的理解上。\n\n**分歧**:  \n在BioBERT之后，研究者们探索了两条改善路径。第一条演进路线强调通过在领域特定的生物医学文本上进行语言模型的预训练，以强化模型对专业术语和领域知识的理解能力，其代表作是2021年的论文《Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing》。第二条路线则从公共领域的临床文本入手，公开了一系列基于临床数据训练的特定领域BERT模型，这一方向的先驱性工作为2019年的《Publicly Available Clinical》。这种方法侧重于提升模型对临床语境的适应性。\n\n**对比**:  \n两种技术路线均致力于改善生物医学领域内语言模型的效果，但采用了不同的重点和策略。第一条路线通过利用生物医学领域的专属文本进行模型预训练，专注于提升模型对特定领域内容的掌握力；第二条路线着眼于临床文本的公开访问，以增加模型在实际应用场景中的泛化能力。同时，第一条路线在2023年进一步发展，通过引入MultiMedQA基准，整合多个医学问题回答任务以测试模型的临床知识编码能力，相较于第二条路径在指标上更具全面性。",
        "papers": [
          {
            "paper_id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019,
            "cited_by_count": 6148,
            "role": "center"
          },
          {
            "paper_id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021,
            "cited_by_count": 1737
          },
          {
            "paper_id": "W4384071683",
            "title": "Large language models encode clinical knowledge",
            "year": 2023,
            "cited_by_count": 2248
          },
          {
            "paper_id": "W4387500346",
            "title": "The future landscape of large language models in medicine",
            "year": 2023,
            "cited_by_count": 732
          },
          {
            "paper_id": "W2963716420",
            "title": "Publicly Available Clinical",
            "year": 2019,
            "cited_by_count": 1422
          },
          {
            "paper_id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021,
            "cited_by_count": 1737
          },
          {
            "paper_id": "W4384071683",
            "title": "Large language models encode clinical knowledge",
            "year": 2023,
            "cited_by_count": 2248
          },
          {
            "paper_id": "W4387500346",
            "title": "The future landscape of large language models in medicine",
            "year": 2023,
            "cited_by_count": 732
          }
        ],
        "total_citations": 17004,
        "visual_structure": "Center -> 2 Routes",
        "relation_stats": {
          "total_relations": 2,
          "relation_distribution": {
            "Overcomes": 2
          },
          "dominant_relation": "Overcomes"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2911489562",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019
            },
            "to_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2911489562",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019
            },
            "to_paper": {
              "id": "W2963716420",
              "title": "Publicly Available Clinical",
              "year": 2019
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "route_id": 2,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2911489562",
              "label": "Center",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019,
              "citations": 6148,
              "role": "center"
            },
            {
              "id": "W3046375318",
              "label": "Domain-Specific Language Model...",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021,
              "citations": 1737
            },
            {
              "id": "W4384071683",
              "label": "Large language models encode c...",
              "title": "Large language models encode clinical knowledge",
              "year": 2023,
              "citations": 2248
            },
            {
              "id": "W4387500346",
              "label": "The future landscape of large ...",
              "title": "The future landscape of large language models in medicine",
              "year": 2023,
              "citations": 732
            },
            {
              "id": "W2963716420",
              "label": "Publicly Available Clinical...",
              "title": "Publicly Available Clinical",
              "year": 2019,
              "citations": 1422
            },
            {
              "id": "W3046375318",
              "label": "Domain-Specific Language Model...",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021,
              "citations": 1737
            },
            {
              "id": "W4384071683",
              "label": "Large language models encode c...",
              "title": "Large language models encode clinical knowledge",
              "year": 2023,
              "citations": 2248
            },
            {
              "id": "W4387500346",
              "label": "The future landscape of large ...",
              "title": "The future landscape of large language models in medicine",
              "year": 2023,
              "citations": 732
            }
          ],
          "edges": [
            {
              "source": "W2911489562",
              "target": "W3046375318",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W2911489562",
              "target": "W2963716420",
              "type": "Overcomes",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "箭头方向表示时间演进方向（早年份 → 晚年份）"
        }
      },
      {
        "thread_id": 2,
        "thread_name": "Thread 2: The Chain (线性链条)",
        "title": "从 BioBERT, a domain-specific pre-trained language representation model, is develop 到 Introduction of MultiMedQA benchmark, which combines multiple medical question-a 的演进之路",
        "pattern_type": "The Chain (线性链条)",
        "thread_type": "chain",
        "narrative": "**起源**: 在2019年，研究者们针对通用自然语言处理（NLP）模型，尤其是BERT，在生物医学领域表现不佳的问题，开发了BioBERT，这是一个专门为生物医学领域文本挖掘而预训练的语言表征模型。BioBERT通过利用领域特定的语料库进行预训练，显著提升了生物医学文本处理的效果。然而，BioBERT的一个主要局限性在于其预训练过程需要大量的计算资源，这对资源有限的研究者而言构成了挑战。\n\n**演进**: 到2021年，研究进一步推进，学者们意识到在通用语料上预训练的语言模型可能会导致生物医学领域NLP任务的负迁移。因此，他们提出了一种方法，强调仅在生物医学领域内的专用文本上进行模型的预训练。虽然这一方法有效减少了负迁移效应，但在全面的生物医学NLP基准测试上仍显局限，尚未充分展示其潜力。\n\n**最新进展**: 2023年，技术发展达到了新的阶段，研究者们着眼于评估大型语言模型在临床知识方面的表现。为解决以往评估中样本限制的问题，提出了MultiMedQA基准，该基准结合了多种医学问答任务，从而提供了更为综合的评估框架。尽管如此，这一方法在生成安全关键型医学问题的适当答案时仍然面临挑战。在这几年的技术演进中，生物医学NLP中的语言模型不断克服前人的不足，朝着更加全面和精确的方向发展。",
        "papers": [
          {
            "paper_id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019,
            "cited_by_count": 6148
          },
          {
            "paper_id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021,
            "cited_by_count": 1737
          },
          {
            "paper_id": "W4384071683",
            "title": "Large language models encode clinical knowledge",
            "year": 2023,
            "cited_by_count": 2248
          }
        ],
        "total_citations": 10133,
        "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
        "relation_stats": {
          "total_relations": 0,
          "relation_distribution": {},
          "dominant_relation": "Unknown"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2911489562",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019
            },
            "to_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "to_paper": {
              "id": "W4384071683",
              "title": "Large language models encode clinical knowledge",
              "year": 2023
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2911489562",
              "label": "Paper 1",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019,
              "citations": 6148
            },
            {
              "id": "W3046375318",
              "label": "Paper 2",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021,
              "citations": 1737
            },
            {
              "id": "W4384071683",
              "label": "Paper 3",
              "title": "Large language models encode clinical knowledge",
              "year": 2023,
              "citations": 2248
            }
          ],
          "edges": [
            {
              "source": "W2911489562",
              "target": "W3046375318",
              "type": "chronological_evolution",
              "label": "2019 → 2021"
            },
            {
              "source": "W3046375318",
              "target": "W4384071683",
              "type": "chronological_evolution",
              "label": "2021 → 2023"
            }
          ],
          "layout": "hierarchical",
          "direction_note": "箭头方向表示时间演进方向（早年份 → 晚年份）"
        }
      },
      {
        "thread_id": 3,
        "thread_name": "Thread 3: The Chain (线性链条)",
        "title": "从 Creation and public release of domain-specific BERT models trained on clinical t 到 Pretraining language models solely on domain-specific in-domain biomedical text  的演进之路",
        "pattern_type": "The Chain (线性链条)",
        "thread_type": "chain",
        "narrative": "**起源**  \n在2019年，一篇重要的论文提出了创建并公开发布专为临床领域设计的BERT预训练模型的构想。这项研究旨在解决缺乏专用临床BERT模型的问题，以提高自然语言处理技术在临床文本分析中的性能。然而，这一方法在去标识化（de-ID）任务方面表现欠佳，尤其是在某些数据集（如i2b2 2006和i2b2 2014）上表现不尽如人意，显示出模型在特定任务上局限性。\n\n**演进**  \n同年，另一篇论文提出了BioBERT，这是一种为生物医学文本挖掘而专门开发的领域特定预训练语言表示模型。BioBERT针对一般NLP模型在生物医学领域性能不佳的问题进行了改进。这种方法虽然显著提升了领域内任务的性能，但其预训练过程需要大量计算资源，成为其应用的一个限制因素。\n\n**最新进展**  \n到2021年，最新的研究进一步推进了领域特定语言模型的预训练方法。这篇文章解决了因使用通用数据进行预训练导致生物医学领域NLP任务的负迁移问题。通过专注于领域内部生物医学文本的预训练，新方法虽然尚需在生物医学NLP中进行更多比较权威的基准测试，但它为提高领域内模型性能提供了一个新的思路，标志着该领域技术的又一次突破。",
        "papers": [
          {
            "paper_id": "W2963716420",
            "title": "Publicly Available Clinical",
            "year": 2019,
            "cited_by_count": 1422
          },
          {
            "paper_id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019,
            "cited_by_count": 6148
          },
          {
            "paper_id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021,
            "cited_by_count": 1737
          }
        ],
        "total_citations": 9307,
        "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
        "relation_stats": {
          "total_relations": 1,
          "relation_distribution": {
            "Overcomes": 1
          },
          "dominant_relation": "Overcomes"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2963716420",
              "title": "Publicly Available Clinical",
              "year": 2019
            },
            "to_paper": {
              "id": "W2911489562",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Overcomes",
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2911489562",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019
            },
            "to_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2963716420",
              "label": "Paper 1",
              "title": "Publicly Available Clinical",
              "year": 2019,
              "citations": 1422
            },
            {
              "id": "W2911489562",
              "label": "Paper 2",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019,
              "citations": 6148
            },
            {
              "id": "W3046375318",
              "label": "Paper 3",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021,
              "citations": 1737
            }
          ],
          "edges": [
            {
              "source": "W2963716420",
              "target": "W2911489562",
              "type": "chronological_evolution",
              "label": "2019 → 2019"
            },
            {
              "source": "W2911489562",
              "target": "W3046375318",
              "type": "chronological_evolution",
              "label": "2019 → 2021"
            }
          ],
          "layout": "hierarchical",
          "direction_note": "箭头方向表示时间演进方向（早年份 → 晚年份）"
        }
      },
      {
        "thread_id": 4,
        "thread_name": "Thread 4: The Chain (线性链条)",
        "title": "从 Creation and public release of domain-specific BERT models trained on clinical t 到 Introduction of MultiMedQA benchmark, which combines multiple medical question-a 的演进之路",
        "pattern_type": "The Chain (线性链条)",
        "thread_type": "chain",
        "narrative": "**起源**\n\n2019年，第一篇论文“Publicly Available Clinical” 开创了面向临床的BERT模型的研究方向。该研究填补了临床领域缺乏预训练BERT模型的空白，并通过创建和公开发布领域特定的BERT模型，为临床文本任务提供了新的基础。然而，该方法在去识别化任务（如i2b2 2006和i2b2 20）中表现一般，这为后续研究指出了进一步优化的方向。\n\n**演进**\n\n2019年，“Enhancing clinical concept extraction with contextual embeddings”论文紧随其后，针对临床概念提取任务中缺乏最佳实践这一问题，将大型临床语料库如MIMIC上的上下文嵌入用于该任务。这一改进在捕捉语境信息方面显示了优越性，但该方法仍面临预训练语料过拟合的问题。2021年，论文“Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing”进一步发展了领域特定语言模型的预训练方法，通过仅在生物医学域内文本进行训练，减少了跨领域的负迁移现象。然而其不足之处在于缺乏广泛的生物医学NLP基准测试，这也推动了后续对模型评价体系的改进尝试。\n\n**最新进展**\n\n2023年，论文“Large language models encode clinical knowledge”实现了该领域的重要突破。研究引入了MultiMedQA基准，将多个医学问答数据集结合，评估大型语言模型在医学知识上的表现。这不仅提供了更全面的评估工具，也回应了模型生成安全关键医学问题答案的挑战，尽管在某些安全关键问题的回答生成上，模型仍存在困难。在此基础上，未来的研究将致力于进一步优化模型在安全关键情况下的表现，从而增强其临床实用性。",
        "papers": [
          {
            "paper_id": "W2963716420",
            "title": "Publicly Available Clinical",
            "year": 2019,
            "cited_by_count": 1422
          },
          {
            "paper_id": "W2955483668",
            "title": "Enhancing clinical concept extraction with contextual embeddings",
            "year": 2019,
            "cited_by_count": 314
          },
          {
            "paper_id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021,
            "cited_by_count": 1737
          },
          {
            "paper_id": "W4384071683",
            "title": "Large language models encode clinical knowledge",
            "year": 2023,
            "cited_by_count": 2248
          }
        ],
        "total_citations": 5721,
        "visual_structure": "Paper_1 -> Paper_2 -> Paper_3 -> Paper_4",
        "relation_stats": {
          "total_relations": 1,
          "relation_distribution": {
            "Extends": 1
          },
          "dominant_relation": "Extends"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2963716420",
              "title": "Publicly Available Clinical",
              "year": 2019
            },
            "to_paper": {
              "id": "W2955483668",
              "title": "Enhancing clinical concept extraction with contextual embeddings",
              "year": 2019
            },
            "relation_type": "Extends",
            "narrative_relation": "Extends",
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2955483668",
              "title": "Enhancing clinical concept extraction with contextual embeddings",
              "year": 2019
            },
            "to_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "to_paper": {
              "id": "W4384071683",
              "title": "Large language models encode clinical knowledge",
              "year": 2023
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2963716420",
              "label": "Paper 1",
              "title": "Publicly Available Clinical",
              "year": 2019,
              "citations": 1422
            },
            {
              "id": "W2955483668",
              "label": "Paper 2",
              "title": "Enhancing clinical concept extraction with contextual embeddings",
              "year": 2019,
              "citations": 314
            },
            {
              "id": "W3046375318",
              "label": "Paper 3",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021,
              "citations": 1737
            },
            {
              "id": "W4384071683",
              "label": "Paper 4",
              "title": "Large language models encode clinical knowledge",
              "year": 2023,
              "citations": 2248
            }
          ],
          "edges": [
            {
              "source": "W2963716420",
              "target": "W2955483668",
              "type": "chronological_evolution",
              "label": "2019 → 2019"
            },
            {
              "source": "W2955483668",
              "target": "W3046375318",
              "type": "chronological_evolution",
              "label": "2019 → 2021"
            },
            {
              "source": "W3046375318",
              "target": "W4384071683",
              "type": "chronological_evolution",
              "label": "2021 → 2023"
            }
          ],
          "layout": "hierarchical",
          "direction_note": "箭头方向表示时间演进方向（早年份 → 晚年份）"
        }
      },
      {
        "thread_id": 5,
        "thread_name": "Thread 5: The Chain (线性链条)",
        "title": "从 SCIBERT, a pretrained language model based on BERT trained on a large corpus of  到 Introduction of MultiMedQA benchmark, which combines multiple medical question-a 的演进之路",
        "pattern_type": "The Chain (线性链条)",
        "thread_type": "chain",
        "narrative": "**起源**\n\n在2019年，SciBERT 的出现为科学文本的自然语言处理开辟了新的方向。该工作旨在解决科学领域中获取大规模标注数据的困难，通过在大规模科学文本语料上预训练语言模型，提升了NLP任务的表现。然而，该模型的局限在于缺乏类似于BERT-Large的版本，这在一定程度上限制了其在更复杂任务中的表现能力。\n\n**演进**\n\n到了2021年，研究者们逐步认识到通用语言模型的预训练可能导致领域内具体任务的负迁移。针对这一不足，研究者提出只在特定领域内的生物医学文本上进行预训练的方法，以更好地服务于生物医学NLP任务。尽管如此，该方法在生物医学NLP中的广泛基准测试仍显不足，需要进一步验证其全面性和可靠性。\n\n**最新进展**\n\n在2023年，研究进一步取得突破，尤其是在大规模语言模型的临床知识编码评估方面。研究人员引入了MultiMedQA基准，该基准整合了多个医学问答数据集，以更全面地评估大语言模型在临床知识领域的表现。然而，新方法在处理安全关键的医学问题时，生成合适答案的能力依然存在挑战，这表明领域内仍有许多未解难题需要探索和解决。",
        "papers": [
          {
            "paper_id": "W2970771982",
            "title": "SciBERT: A Pretrained Language Model for Scientific Text",
            "year": 2019,
            "cited_by_count": 2777
          },
          {
            "paper_id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021,
            "cited_by_count": 1737
          },
          {
            "paper_id": "W4384071683",
            "title": "Large language models encode clinical knowledge",
            "year": 2023,
            "cited_by_count": 2248
          }
        ],
        "total_citations": 6762,
        "visual_structure": "Paper_1 -> Paper_2 -> Paper_3",
        "relation_stats": {
          "total_relations": 0,
          "relation_distribution": {},
          "dominant_relation": "Unknown"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2970771982",
              "title": "SciBERT: A Pretrained Language Model for Scientific Text",
              "year": 2019
            },
            "to_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "to_paper": {
              "id": "W4384071683",
              "title": "Large language models encode clinical knowledge",
              "year": 2023
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2970771982",
              "label": "Paper 1",
              "title": "SciBERT: A Pretrained Language Model for Scientific Text",
              "year": 2019,
              "citations": 2777
            },
            {
              "id": "W3046375318",
              "label": "Paper 2",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021,
              "citations": 1737
            },
            {
              "id": "W4384071683",
              "label": "Paper 3",
              "title": "Large language models encode clinical knowledge",
              "year": 2023,
              "citations": 2248
            }
          ],
          "edges": [
            {
              "source": "W2970771982",
              "target": "W3046375318",
              "type": "chronological_evolution",
              "label": "2019 → 2021"
            },
            {
              "source": "W3046375318",
              "target": "W4384071683",
              "type": "chronological_evolution",
              "label": "2021 → 2023"
            }
          ],
          "layout": "hierarchical",
          "direction_note": "箭头方向表示时间演进方向（早年份 → 晚年份）"
        }
      }
    ],
    "metadata": {
      "total_papers_analyzed": 226,
      "papers_after_pruning": 20,
      "total_threads": 5,
      "generation_date": "2025-12-18T20:22:05.368299"
    }
  },
  "summary": {
    "original_papers": 226,
    "pruned_papers": 20,
    "total_threads": 5
  }
}