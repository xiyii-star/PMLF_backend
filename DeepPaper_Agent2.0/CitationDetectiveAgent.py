"""
Citation Detective Agent (å¼•ç”¨ä¾¦æ¢)
è´Ÿè´£å¤–éƒ¨éªŒè¯ï¼Œæ‰¾å‡ºåŒè¡Œå¯¹è®ºæ–‡çš„çœŸå®è¯„ä»·

èŒè´£ï¼šè·å–å¼•ç”¨è¯¥è®ºæ–‡çš„å…¶ä»–è®ºæ–‡ï¼Œåˆ†æå¼•ç”¨ä¸Šä¸‹æ–‡ï¼Œæå–çœŸå®çš„limitations
è¾“å…¥ï¼šPaper ID (DOI, ArXiv ID, Semantic Scholar IDç­‰)
è¾“å‡ºï¼šæ¥è‡ªåç»­ç ”ç©¶çš„çœŸå®Limitationåˆ—è¡¨

ä¸»è¦APIï¼šSemantic Scholar Academic Graph API
- ç›´æ¥æä¾›å¼•ç”¨ä¸Šä¸‹æ–‡ï¼ˆcitation contextsï¼‰
- è¦†ç›–ç‡é«˜ï¼ˆ2äº¿+è®ºæ–‡ï¼Œ75%+åŒ…å«ä¸Šä¸‹æ–‡ï¼‰
- æ•°æ®è´¨é‡é«˜ï¼ˆçœŸå®çš„å¼•ç”¨æ–‡æœ¬ï¼Œéæ‘˜è¦è¿‘ä¼¼ï¼‰
"""

import json
import logging
import re
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path

# å¯¼å…¥LLMé…ç½®
import sys
sys.path.append(str(Path(__file__).parent.parent))
from src.llm_config import LLMClient, LLMConfig

# ç¬¬ä¸‰æ–¹åº“
try:
    import requests
    from xml.etree import ElementTree as ET
except ImportError:
    raise ImportError("è¯·å®‰è£…ä¾èµ–: pip install requests")

logger = logging.getLogger(__name__)


@dataclass
class CitationContext:
    """å¼•ç”¨ä¸Šä¸‹æ–‡æ•°æ®ç»“æ„"""
    citing_paper_id: str  # å¼•ç”¨è®ºæ–‡çš„ID
    citing_paper_title: str  # å¼•ç”¨è®ºæ–‡çš„æ ‡é¢˜
    citing_paper_authors: List[str]  # å¼•ç”¨è®ºæ–‡çš„ä½œè€…
    citing_paper_year: Optional[int]  # å¼•ç”¨è®ºæ–‡çš„å¹´ä»½
    citation_count: int  # å¼•ç”¨è®ºæ–‡çš„å¼•ç”¨æ•°ï¼ˆå½±å“åŠ›æŒ‡æ ‡ï¼‰
    context_text: str  # å¼•ç”¨ä¸Šä¸‹æ–‡çš„æ–‡æœ¬
    has_critical_keyword: bool  # æ˜¯å¦åŒ…å«æ‰¹åˆ¤æ€§å…³é”®è¯
    critical_keywords: List[str]  # åŒ¹é…åˆ°çš„æ‰¹åˆ¤æ€§å…³é”®è¯
    extracted_limitation: Optional[str] = None  # æå–çš„limitationæè¿°
    confidence: float = 0.0  # ç½®ä¿¡åº¦ (0-1)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


@dataclass
class CitationAnalysisResult:
    """å¼•ç”¨åˆ†æç»“æœ"""
    target_paper_id: str  # ç›®æ ‡è®ºæ–‡ID
    total_citations: int  # æ€»å¼•ç”¨æ•°
    analyzed_citations: int  # åˆ†æçš„å¼•ç”¨æ•°
    critical_citations: int  # åŒ…å«æ‰¹åˆ¤æ€§è¯„ä»·çš„å¼•ç”¨æ•°
    citation_contexts: List[CitationContext]  # å¼•ç”¨ä¸Šä¸‹æ–‡åˆ—è¡¨
    extracted_limitations: List[str]  # æå–çš„limitationåˆ—è¡¨

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "target_paper_id": self.target_paper_id,
            "total_citations": self.total_citations,
            "analyzed_citations": self.analyzed_citations,
            "critical_citations": self.critical_citations,
            "citation_contexts": [ctx.to_dict() for ctx in self.citation_contexts],
            "extracted_limitations": self.extracted_limitations
        }


class CitationDetectiveAgent:
    """
    Citation Detective Agent
    è´Ÿè´£è·å–å¼•ç”¨ä¿¡æ¯ï¼Œåˆ†æå¼•ç”¨ä¸Šä¸‹æ–‡ï¼Œæå–çœŸå®çš„peerè¯„ä»·
    """

    # æ‰¹åˆ¤æ€§å…³é”®è¯ï¼ˆè½¬æŠ˜è¯ã€è´Ÿé¢è¯„ä»·ï¼‰
    CRITICAL_KEYWORDS = [
        "however", "although", "but", "nevertheless", "nonetheless",
        "limitation", "limitations", "limited", "limits",
        "fails to", "fail to", "failed", "failure",
        "weakness", "weaknesses", "drawback", "drawbacks",
        "problem", "problems", "issue", "issues",
        "challenge", "challenges", "difficulty", "difficulties",
        "cannot", "can not", "unable to",
        "does not", "doesn't", "do not", "don't",
        "insufficient", "inadequate", "lack", "lacks", "lacking",
        "poor", "worse", "inferior",
        "unfortunately", "regrettably",
        "contradict", "contradicts", "inconsistent"
    ]

    def __init__(self, llm_client: Optional[LLMClient] = None):
        """
        åˆå§‹åŒ–å¼•ç”¨ä¾¦æ¢

        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œç”¨äºæ·±åº¦è¯­ä¹‰åˆ†æï¼‰
        """
        self.llm_client = llm_client
        self.system_prompt = self._build_system_prompt() if llm_client else None

    def _build_system_prompt(self) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆç”¨äºLLMæ·±åº¦åˆ†æï¼‰"""
        return """You are a professional expert in scientific literature analysis.

Your Task:
Extract genuine, specific limitations from citation contexts.

Input: A citation context (a text segment where a citing paper references the target paper).

Output Requirements:

Determine if the citation contains critical evaluation or points out limitations of the target paper.
If yes, extract the specific limitation description (avoid generalizations).
Summarize this limitation using clear, objective language.
Output Format (JSON):

<JSON>
{
    "has_limitation": true/false,
    "limitation": "Description of the specific limitation",
    "confidence": 0.9,
    "reasoning": "Basis for judgment"
}
Notes:

Extract only actual criticisms and limitations; do not extract positive evaluations.
Descriptions must be specific; avoid vague phrases like "has certain limitations".
If the context is a neutral citation (no criticism), return has_limitation=false.

"""

    def analyze(
        self,
        paper_id: str,
        top_k: int = 10,
        min_citation_count: int = 0,
        use_llm_analysis: bool = False
    ) -> CitationAnalysisResult:
        """
        åˆ†æè®ºæ–‡çš„å¼•ç”¨æƒ…å†µï¼Œæå–çœŸå®çš„limitations

        Args:
            paper_id: è®ºæ–‡IDï¼ˆArXiv ID, DOIç­‰ï¼‰
            top_k: åˆ†æTop Kç¯‡å¼•ç”¨è®ºæ–‡
            min_citation_count: æœ€å°å¼•ç”¨æ•°è¿‡æ»¤ï¼ˆå½±å“åŠ›è¿‡æ»¤ï¼‰
            use_llm_analysis: æ˜¯å¦ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦è¯­ä¹‰åˆ†æ

        Returns:
            å¼•ç”¨åˆ†æç»“æœ
        """
        logger.info(f"å¼€å§‹å¼•ç”¨ä¾¦æ¢åˆ†æ: {paper_id}")

        # Step 1: è·å–å¼•ç”¨è¯¥è®ºæ–‡çš„åˆ—è¡¨
        citing_papers = self._fetch_citing_papers(paper_id)
        logger.info(f"âœ… æ‰¾åˆ° {len(citing_papers)} ç¯‡å¼•ç”¨è®ºæ–‡")

        if not citing_papers:
            logger.warning("æœªæ‰¾åˆ°å¼•ç”¨ä¿¡æ¯")
            return CitationAnalysisResult(
                target_paper_id=paper_id,
                total_citations=0,
                analyzed_citations=0,
                critical_citations=0,
                citation_contexts=[],
                extracted_limitations=[]
            )

        # Step 2: æŒ‰å½±å“åŠ›æ’åºï¼Œå–Top K
        citing_papers = self._rank_by_impact(citing_papers, top_k, min_citation_count)
        logger.info(f"âœ… ç­›é€‰å‡º Top {len(citing_papers)} ç¯‡é«˜å½±å“åŠ›è®ºæ–‡")

        # Step 3: è·å–å¼•ç”¨ä¸Šä¸‹æ–‡
        citation_contexts = self._extract_citation_contexts(
            paper_id, citing_papers
        )
        logger.info(f"âœ… æå–åˆ° {len(citation_contexts)} æ¡å¼•ç”¨ä¸Šä¸‹æ–‡")

        # Step 4: è¿‡æ»¤æ‰¹åˆ¤æ€§ä¸Šä¸‹æ–‡
        critical_contexts = self._filter_critical_contexts(citation_contexts)
        logger.info(f"âœ… ç­›é€‰å‡º {len(critical_contexts)} æ¡æ‰¹åˆ¤æ€§å¼•ç”¨")

        # Step 5: æå–limitations
        if use_llm_analysis and self.llm_client:
            limitations = self._extract_limitations_with_llm(critical_contexts)
        else:
            limitations = self._extract_limitations_rule_based(critical_contexts)

        logger.info(f"âœ… æå–åˆ° {len(limitations)} æ¡çœŸå®limitations")

        # æ„å»ºç»“æœ
        result = CitationAnalysisResult(
            target_paper_id=paper_id,
            total_citations=len(citing_papers),
            analyzed_citations=len(citation_contexts),
            critical_citations=len(critical_contexts),
            citation_contexts=critical_contexts,
            extracted_limitations=limitations
        )

        return result

    def _fetch_citing_papers(self, paper_id: str) -> List[Dict[str, Any]]:
        """
        è·å–å¼•ç”¨è¯¥è®ºæ–‡çš„åˆ—è¡¨ï¼ˆä½¿ç”¨ Semantic Scholar APIï¼‰

        Args:
            paper_id: è®ºæ–‡ID (æ”¯æŒ DOI, ArXiv ID, Semantic Scholar IDç­‰)

        Returns:
            å¼•ç”¨è®ºæ–‡åˆ—è¡¨ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
        """
        return self._fetch_from_semantic_scholar(paper_id)

    def _fetch_from_semantic_scholar(self, paper_id: str) -> List[Dict[str, Any]]:
        """
        ä» Semantic Scholar API è·å–å¼•ç”¨ä¿¡æ¯ï¼ˆåŒ…å«å¼•ç”¨ä¸Šä¸‹æ–‡ï¼‰

        Args:
            paper_id: è®ºæ–‡ID (æ”¯æŒ DOI, ArXiv ID, Semantic Scholar IDç­‰)

        Returns:
            å¼•ç”¨è®ºæ–‡åˆ—è¡¨ï¼ˆåŒ…å«contextså­—æ®µï¼‰
        """
        logger.info(f"ä» Semantic Scholar è·å–å¼•ç”¨ä¿¡æ¯: {paper_id}")

        # æ ‡å‡†åŒ– paper_id æ ¼å¼
        if not paper_id.startswith(('DOI:', 'arXiv:', 'CorpusId:', 'PMID:')):
            # è‡ªåŠ¨è¯†åˆ«æ ¼å¼
            if paper_id.startswith('10.'):
                paper_id = f'DOI:{paper_id}'
            elif paper_id.replace('.', '').replace('v', '').isdigit():
                paper_id = f'arXiv:{paper_id}'
            elif paper_id.isdigit():
                paper_id = f'CorpusId:{paper_id}'
            # å¦åˆ™ä¿æŒåŸæ ·ï¼Œè®© API è‡ªåŠ¨å¤„ç†

        try:
            # Semantic Scholar API - è·å–å¼•ç”¨åˆ—è¡¨
            base_url = "https://api.semanticscholar.org/graph/v1"
            url = f"{base_url}/paper/{paper_id}/citations"

            params = {
                'fields': 'contexts,intents,isInfluential,'
                         'citingPaper.paperId,citingPaper.title,'
                         'citingPaper.year,citingPaper.citationCount,'
                         'citingPaper.authors',
                'limit': 1000  # è·å–å°½å¯èƒ½å¤šçš„å¼•ç”¨
            }

            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()

            data = response.json()
            citations = data.get('data', [])

            citing_papers = []
            for cite in citations:
                citing_paper = cite.get('citingPaper', {})

                if not citing_paper:
                    continue

                # æ„å»ºç»Ÿä¸€çš„æ•°æ®æ ¼å¼
                paper_info = {
                    "id": citing_paper.get('paperId', ''),
                    "title": citing_paper.get('title', ''),
                    "authors": [
                        author.get('name', '')
                        for author in citing_paper.get('authors', [])
                    ],
                    "year": citing_paper.get('year'),
                    "citation_count": citing_paper.get('citationCount', 0),
                    # å…³é”®ï¼šå¼•ç”¨ä¸Šä¸‹æ–‡ï¼ˆSemantic Scholar ç›´æ¥æä¾›ï¼‰
                    "contexts": cite.get('contexts', []),
                    "intents": cite.get('intents', []),
                    "is_influential": cite.get('isInfluential', False)
                }

                citing_papers.append(paper_info)

            logger.info(f"âœ… ä» Semantic Scholar è·å–åˆ° {len(citing_papers)} ç¯‡å¼•ç”¨è®ºæ–‡")
            logger.info(f"âœ… å…¶ä¸­ {sum(1 for p in citing_papers if p.get('contexts'))} ç¯‡åŒ…å«å¼•ç”¨ä¸Šä¸‹æ–‡")

            return citing_papers

        except requests.exceptions.RequestException as e:
            logger.error(f"Semantic Scholar API è°ƒç”¨å¤±è´¥: {e}")
            logger.info("ğŸ’¡ æç¤ºï¼šå¦‚æœæ˜¯ 404 é”™è¯¯ï¼Œè¯·æ£€æŸ¥ paper_id æ ¼å¼æ˜¯å¦æ­£ç¡®")
            return []
        except Exception as e:
            logger.error(f"å¤„ç† Semantic Scholar å“åº”å¤±è´¥: {e}")
            return []

    def _rank_by_impact(
        self,
        papers: List[Dict[str, Any]],
        top_k: int,
        min_citation_count: int
    ) -> List[Dict[str, Any]]:
        """
        æŒ‰å½±å“åŠ›æ’åºå¹¶è¿‡æ»¤

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            top_k: å–Top K
            min_citation_count: æœ€å°å¼•ç”¨æ•°

        Returns:
            æ’åºåçš„è®ºæ–‡åˆ—è¡¨
        """
        # è¿‡æ»¤ä½å¼•ç”¨è®ºæ–‡
        filtered = [p for p in papers if p.get("citation_count", 0) >= min_citation_count]

        # æŒ‰å¼•ç”¨æ•°æ’åº
        sorted_papers = sorted(
            filtered,
            key=lambda x: x.get("citation_count", 0),
            reverse=True
        )

        return sorted_papers[:top_k]

    def _extract_citation_contexts(
        self,
        target_paper_id: str,
        citing_papers: List[Dict[str, Any]]
    ) -> List[CitationContext]:
        """
        æå–å¼•ç”¨ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨ Semantic Scholar æä¾›çš„ contextsï¼‰

        Args:
            target_paper_id: ç›®æ ‡è®ºæ–‡IDï¼ˆä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰æ¥å£ï¼‰
            citing_papers: å¼•ç”¨è®ºæ–‡åˆ—è¡¨ï¼ˆå·²åŒ…å« contexts å­—æ®µï¼‰

        Returns:
            å¼•ç”¨ä¸Šä¸‹æ–‡åˆ—è¡¨
        """
        contexts = []

        for paper in citing_papers:
            # Semantic Scholar ç›´æ¥æä¾›å¼•ç”¨ä¸Šä¸‹æ–‡
            raw_contexts = paper.get('contexts', [])

            # å¦‚æœæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯¥å¼•ç”¨
            if not raw_contexts:
                logger.debug(f"è·³è¿‡æ— ä¸Šä¸‹æ–‡çš„å¼•ç”¨: {paper.get('title', 'Unknown')[:50]}...")
                continue

            # åˆå¹¶æ‰€æœ‰ä¸Šä¸‹æ–‡
            context_text = " ".join(raw_contexts)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰¹åˆ¤æ€§å…³é”®è¯
            has_critical, keywords = self._check_critical_keywords(context_text)

            context = CitationContext(
                citing_paper_id=paper.get("id", ""),
                citing_paper_title=paper.get("title", ""),
                citing_paper_authors=paper.get("authors", []),
                citing_paper_year=paper.get("year"),
                citation_count=paper.get("citation_count", 0),
                context_text=context_text,
                has_critical_keyword=has_critical,
                critical_keywords=keywords
            )

            contexts.append(context)

        logger.info(f"âœ… æˆåŠŸæå– {len(contexts)} æ¡å¼•ç”¨ä¸Šä¸‹æ–‡")
        logger.info(f"âœ… å…¶ä¸­ {sum(1 for c in contexts if c.has_critical_keyword)} æ¡åŒ…å«æ‰¹åˆ¤æ€§å…³é”®è¯")

        return contexts

    def _check_critical_keywords(self, text: str) -> Tuple[bool, List[str]]:
        """
        æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«æ‰¹åˆ¤æ€§å…³é”®è¯

        Args:
            text: æ–‡æœ¬

        Returns:
            (æ˜¯å¦åŒ…å«, åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨)
        """
        text_lower = text.lower()
        found_keywords = []

        for keyword in self.CRITICAL_KEYWORDS:
            if keyword in text_lower:
                found_keywords.append(keyword)

        return len(found_keywords) > 0, found_keywords

    def _filter_critical_contexts(
        self,
        contexts: List[CitationContext]
    ) -> List[CitationContext]:
        """
        è¿‡æ»¤å‡ºåŒ…å«æ‰¹åˆ¤æ€§å…³é”®è¯çš„ä¸Šä¸‹æ–‡

        Args:
            contexts: å¼•ç”¨ä¸Šä¸‹æ–‡åˆ—è¡¨

        Returns:
            æ‰¹åˆ¤æ€§ä¸Šä¸‹æ–‡åˆ—è¡¨
        """
        return [ctx for ctx in contexts if ctx.has_critical_keyword]

    def _extract_limitations_rule_based(
        self,
        contexts: List[CitationContext]
    ) -> List[str]:
        """
        åŸºäºè§„åˆ™æå–limitations

        Args:
            contexts: æ‰¹åˆ¤æ€§ä¸Šä¸‹æ–‡åˆ—è¡¨

        Returns:
            limitationåˆ—è¡¨
        """
        limitations = []

        for ctx in contexts:
            # ç®€å•è§„åˆ™ï¼šæå–åŒ…å«æ‰¹åˆ¤æ€§å…³é”®è¯çš„å¥å­
            sentences = self._split_sentences(ctx.context_text)

            for sentence in sentences:
                # æ£€æŸ¥å¥å­æ˜¯å¦åŒ…å«æ‰¹åˆ¤æ€§å…³é”®è¯
                has_critical, keywords = self._check_critical_keywords(sentence)

                if has_critical:
                    # æ„é€ limitationæè¿°
                    limitation = (
                        f"[{ctx.citing_paper_title[:50]}... "
                        f"({ctx.citing_paper_year})] "
                        f"{sentence.strip()}"
                    )
                    limitations.append(limitation)
                    ctx.extracted_limitation = sentence.strip()

        return limitations

    def _extract_limitations_with_llm(
        self,
        contexts: List[CitationContext]
    ) -> List[str]:
        """
        ä½¿ç”¨LLMæ·±åº¦æå–limitations

        Args:
            contexts: æ‰¹åˆ¤æ€§ä¸Šä¸‹æ–‡åˆ—è¡¨

        Returns:
            limitationåˆ—è¡¨
        """
        limitations = []

        for ctx in contexts:
            try:
                # æ„å»ºæç¤ºè¯
                prompt = f"""ã€å¼•ç”¨è®ºæ–‡ã€‘
æ ‡é¢˜: {ctx.citing_paper_title}
ä½œè€…: {', '.join(ctx.citing_paper_authors)}
å¹´ä»½: {ctx.citing_paper_year}

ã€å¼•ç”¨ä¸Šä¸‹æ–‡ã€‘
{ctx.context_text}

è¯·åˆ†æä»¥ä¸Šå¼•ç”¨ä¸Šä¸‹æ–‡ï¼Œåˆ¤æ–­æ˜¯å¦åŒ…å«å¯¹ç›®æ ‡è®ºæ–‡çš„æ‰¹åˆ¤æ€§è¯„ä»·æˆ–å±€é™æ€§æŒ‡å‡ºã€‚
å¦‚æœåŒ…å«ï¼Œè¯·æå–å…·ä½“çš„limitationæè¿°ã€‚
"""

                # è°ƒç”¨LLM
                response = self.llm_client.generate(
                    prompt=prompt,
                    system_prompt=self.system_prompt,
                    temperature=0.2,
                    max_tokens=500
                )

                # è§£æå“åº”
                result = self._parse_llm_response(response)

                if result.get("has_limitation"):
                    limitation = (
                        f"[{ctx.citing_paper_title[:50]}... "
                        f"({ctx.citing_paper_year})] "
                        f"{result.get('limitation', '')}"
                    )
                    limitations.append(limitation)
                    ctx.extracted_limitation = result.get("limitation")
                    ctx.confidence = result.get("confidence", 0.0)

            except Exception as e:
                logger.warning(f"LLMåˆ†æå¤±è´¥: {e}")
                continue

        return limitations

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """è§£æLLMå“åº”"""
        try:
            # æå–JSON
            json_str = self._extract_json(response)
            return json.loads(json_str)
        except Exception as e:
            logger.warning(f"LLMå“åº”è§£æå¤±è´¥: {e}")
            return {"has_limitation": False}

    def _extract_json(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–JSON"""
        if "```json" in text:
            start = text.find("```json") + 7
            end = text.find("```", start)
            if end != -1:
                return text[start:end].strip()

        start = text.find("{")
        end = text.rfind("}") + 1
        if start != -1 and end > start:
            return text[start:end]

        return text.strip()

    def _split_sentences(self, text: str) -> List[str]:
        """ç®€å•çš„å¥å­åˆ†å‰²"""
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
        sentences = re.split(r'[.!?]\s+', text)
        return [s.strip() for s in sentences if s.strip()]

    def export_results(
        self,
        result: CitationAnalysisResult,
        output_path: str,
        format: str = "json"
    ):
        """
        å¯¼å‡ºåˆ†æç»“æœ

        Args:
            result: åˆ†æç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            format: è¾“å‡ºæ ¼å¼ (json, txt, md)
        """
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        try:
            if format == "json":
                self._export_json(result, output_file)
            elif format == "txt":
                self._export_txt(result, output_file)
            elif format == "md":
                self._export_markdown(result, output_file)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")

            logger.info(f"âœ… ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")

        except Exception as e:
            logger.error(f"å¯¼å‡ºå¤±è´¥: {e}")

    def _export_json(self, result: CitationAnalysisResult, output_file: Path):
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)

    def _export_txt(self, result: CitationAnalysisResult, output_file: Path):
        """å¯¼å‡ºä¸ºæ–‡æœ¬æ ¼å¼"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("Citation Detective Report: Peer Review & Limitations\n")
            f.write("=" * 80 + "\n\n")

            f.write(f"Target Paper ID: {result.target_paper_id}\n")
            f.write(f"Total Citations: {result.total_citations}\n")
            f.write(f"Analyzed Citations: {result.analyzed_citations}\n")
            f.write(f"Critical Citations: {result.critical_citations}\n\n")

            f.write("-" * 80 + "\n")
            f.write("Extracted Limitations:\n")
            f.write("-" * 80 + "\n\n")

            for i, limitation in enumerate(result.extracted_limitations, 1):
                f.write(f"{i}. {limitation}\n\n")

            if result.citation_contexts:
                f.write("-" * 80 + "\n")
                f.write("Critical Citation Contexts:\n")
                f.write("-" * 80 + "\n\n")

                for i, ctx in enumerate(result.citation_contexts, 1):
                    f.write(f"ã€Context {i}ã€‘\n")
                    f.write(f"Citing Paper: {ctx.citing_paper_title}\n")
                    f.write(f"Authors: {', '.join(ctx.citing_paper_authors)}\n")
                    f.write(f"Year: {ctx.citing_paper_year}\n")
                    f.write(f"Citation Count: {ctx.citation_count}\n")
                    f.write(f"Keywords: {', '.join(ctx.critical_keywords)}\n\n")
                    f.write(f"Context:\n{ctx.context_text}\n\n")
                    f.write("-" * 80 + "\n\n")

    def _export_markdown(self, result: CitationAnalysisResult, output_file: Path):
        """å¯¼å‡ºä¸ºMarkdownæ ¼å¼"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# Citation Detective Report: Peer Review & Limitations\n\n")

            f.write("## Summary\n\n")
            f.write(f"- **Target Paper ID**: {result.target_paper_id}\n")
            f.write(f"- **Total Citations**: {result.total_citations}\n")
            f.write(f"- **Analyzed Citations**: {result.analyzed_citations}\n")
            f.write(f"- **Critical Citations**: {result.critical_citations}\n\n")

            f.write("---\n\n")
            f.write("## Extracted Limitations\n\n")

            for i, limitation in enumerate(result.extracted_limitations, 1):
                f.write(f"{i}. {limitation}\n\n")

            if result.citation_contexts:
                f.write("---\n\n")
                f.write("## Critical Citation Contexts\n\n")

                for i, ctx in enumerate(result.citation_contexts, 1):
                    f.write(f"### Context {i}\n\n")
                    f.write(f"**Citing Paper**: {ctx.citing_paper_title}\n\n")
                    f.write(f"**Authors**: {', '.join(ctx.citing_paper_authors)}\n\n")
                    f.write(f"**Year**: {ctx.citing_paper_year}\n\n")
                    f.write(f"**Citation Count**: {ctx.citation_count}\n\n")
                    f.write(f"**Critical Keywords**: {', '.join(ctx.critical_keywords)}\n\n")
                    f.write(f"**Context**:\n\n")
                    f.write(f"> {ctx.context_text}\n\n")
                    f.write("---\n\n")


def main():
    """æµ‹è¯•ä»£ç """
    import argparse
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    parser = argparse.ArgumentParser(description="Citation Detective Agent - å¼•ç”¨ä¾¦æ¢")
    parser.add_argument("--paper-id", required=True, help="è®ºæ–‡ID (ArXiv IDæˆ–DOI)")
    parser.add_argument("--top-k", type=int, default=10, help="åˆ†æTop Kç¯‡å¼•ç”¨è®ºæ–‡")
    parser.add_argument("--min-citations", type=int, default=0, help="æœ€å°å¼•ç”¨æ•°è¿‡æ»¤")
    parser.add_argument("--use-llm", action="store_true", help="ä½¿ç”¨LLMæ·±åº¦åˆ†æ")
    parser.add_argument("--config", help="LLMé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨LLMæ—¶å¿…éœ€ï¼‰")
    parser.add_argument("--output", default="citation_analysis_results.json", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--format", choices=["json", "txt", "md"], default="json", help="è¾“å‡ºæ ¼å¼")

    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–agent
        llm_client = None
        if args.use_llm:
            if not args.config:
                raise ValueError("ä½¿ç”¨LLMåˆ†ææ—¶å¿…é¡»æä¾›é…ç½®æ–‡ä»¶ (--config)")
            config = LLMConfig.from_file(args.config)
            llm_client = LLMClient(config)

        agent = CitationDetectiveAgent(llm_client)

        # æ‰§è¡Œåˆ†æ
        result = agent.analyze(
            paper_id=args.paper_id,
            top_k=args.top_k,
            min_citation_count=args.min_citations,
            use_llm_analysis=args.use_llm
        )

        # å¯¼å‡ºç»“æœ
        agent.export_results(result, args.output, format=args.format)

        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 80)
        print(f"Citation Analysis Complete")
        print("=" * 80)
        print(f"Total Citations: {result.total_citations}")
        print(f"Analyzed: {result.analyzed_citations}")
        print(f"Critical: {result.critical_citations}")
        print(f"Extracted Limitations: {len(result.extracted_limitations)}")
        print("\n" + "-" * 80)
        print("Top Limitations:")
        print("-" * 80)

        for i, limitation in enumerate(result.extracted_limitations[:5], 1):
            print(f"\n{i}. {limitation}")

    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
