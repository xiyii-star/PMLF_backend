"""
é«˜çº§è®ºæ–‡æ£€ç´¢æ¨¡å— - åŸºäºæ»šé›ªçƒæ–¹æ³•çš„å¤šé˜¶æ®µæ£€ç´¢ç­–ç•¥

å®ç°ä¼˜åŒ–çš„å…­æ­¥æ£€ç´¢æµç¨‹ï¼ˆåŸºäºarXivç§å­ + OpenAlexæ‰©å±•ï¼‰ï¼š
1. é«˜è´¨é‡ç§å­è·å– (High-Quality Seed Retrieval) - ä½¿ç”¨arXiv API + Categoriesè¿‡æ»¤
2. è·¨åº“IDæ˜ å°„ (ID Mapping) - arXiv -> OpenAlexï¼Œä¸¥æ ¼ConceptéªŒè¯
3. æ­£å‘æ»šé›ªçƒ (Forward Snowballing) - Seed -> è°å¼•ç”¨äº†Seed? -> å¾—åˆ°å­èŠ‚ç‚¹
4. åå‘æ»šé›ªçƒ (Backward Snowballing) - è°è¢«Seedå¼•ç”¨äº†? <- Seed -> å¾—åˆ°çˆ¶èŠ‚ç‚¹/ç¥–å…ˆ
5. æ¨ªå‘è¡¥å……/å…±å¼•æŒ–æ˜ (Co-citation Mining) - åœ¨å­èŠ‚ç‚¹å’Œçˆ¶èŠ‚ç‚¹ä¸­ï¼Œè°è¢«å¤§å®¶åå¤æåŠä½†è¿˜ä¸åœ¨åº“é‡Œ?
6. è¡¥å……SOTA (Add Recent Frontiers) - arXivæœ€æ–°è®ºæ–‡ï¼ˆ6-12ä¸ªæœˆï¼‰+ ç›¸ä¼¼åº¦è¿‡æ»¤
7. æ„å»ºé—­åŒ… (Closure Construction) - å»ºç«‹è¿æ¥
"""

import logging
from typing import List, Dict, Set, Optional, Tuple
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path
import yaml
from openalex_client import OpenAlexClient
from arxiv_seed_retriever import ArxivSeedRetriever
from cross_database_mapper import CrossDatabaseMapper

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SnowballRetrieval:
    """
    æ»šé›ªçƒè®ºæ–‡æ£€ç´¢ç³»ç»Ÿ
    åŸºäºå¼•ç”¨å…³ç³»çš„å¤šé˜¶æ®µè®ºæ–‡å‘ç°å’Œå…³ç³»æ„å»º
    """

    def __init__(
        self,
        client: Optional[OpenAlexClient] = None,
        seed_count: Optional[int] = None,
        citations_per_seed: Optional[int] = None,
        recent_count: Optional[int] = None,
        seed_keywords: Optional[List[str]] = None,
        enable_second_round: Optional[bool] = None,
        second_round_limit: Optional[int] = None,
        use_arxiv_seeds: Optional[bool] = None,
        arxiv_years_back: Optional[int] = None,
        llm_client = None,
        config_path: str = './config/config.yaml'
    ):
        """
        åˆå§‹åŒ–æ»šé›ªçƒæ£€ç´¢ç³»ç»Ÿ

        ä¼˜å…ˆçº§ï¼šä¼ å…¥å‚æ•° > config.yamlé…ç½® > é»˜è®¤å€¼

        Args:
            client: OpenAlex APIå®¢æˆ·ç«¯
            seed_count: åŸºçŸ³ç§å­è®ºæ–‡æ•°é‡ï¼ˆé»˜è®¤: 5ï¼‰
            citations_per_seed: æ¯ä¸ªç§å­è®ºæ–‡é€‰å–çš„å¼•ç”¨è®ºæ–‡æ•°é‡ï¼ˆé»˜è®¤: 8ï¼‰
            recent_count: æœ€æ–°è®ºæ–‡æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰
            seed_keywords: ç§å­å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºç›¸å…³æ€§è¿‡æ»¤ï¼ˆé»˜è®¤: []ï¼‰
            enable_second_round: æ˜¯å¦å¯ç”¨ç¬¬äºŒè½®æ»šé›ªçƒï¼ˆé»˜è®¤: Trueï¼‰
            second_round_limit: ç¬¬äºŒè½®æ¯ç¯‡è®ºæ–‡çš„æ‰©å±•æ•°é‡é™åˆ¶ï¼ˆé»˜è®¤: 3ï¼‰
            use_arxiv_seeds: æ˜¯å¦ä½¿ç”¨arXivç§å­æ£€ç´¢ï¼ˆé»˜è®¤: Trueï¼‰
            arxiv_years_back: arXivç§å­å›æº¯å¹´æ•°ï¼ˆé»˜è®¤: 5ï¼‰
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆç”¨äºæ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆï¼‰
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: './config/config.yaml'ï¼‰
        """
        # åŠ è½½é…ç½®æ–‡ä»¶
        snowball_config = self._load_config(config_path)

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = client or OpenAlexClient()
        self.llm_client = llm_client

        # å‚æ•°ä¼˜å…ˆçº§ï¼šä¼ å…¥å‚æ•° > config.yaml > é»˜è®¤å€¼
        self.seed_count = seed_count if seed_count is not None else snowball_config.get('seed_count', 5)
        self.citations_per_seed = citations_per_seed if citations_per_seed is not None else snowball_config.get('citations_per_seed', 8)
        self.recent_count = recent_count if recent_count is not None else snowball_config.get('recent_count', 10)
        self.seed_keywords = seed_keywords if seed_keywords is not None else snowball_config.get('seed_keywords', [])
        self.enable_second_round = enable_second_round if enable_second_round is not None else snowball_config.get('enable_second_round', True)
        self.second_round_limit = second_round_limit if second_round_limit is not None else snowball_config.get('second_round_limit', 3)

        # æ–°å¢ï¼šarXivç§å­æ£€ç´¢å‚æ•°
        self.use_arxiv_seeds = use_arxiv_seeds if use_arxiv_seeds is not None else snowball_config.get('use_arxiv_seeds', True)
        self.arxiv_years_back = arxiv_years_back if arxiv_years_back is not None else snowball_config.get('arxiv_years_back', 5)

        # åˆå§‹åŒ–arXivæ£€ç´¢å™¨å’Œè·¨åº“æ˜ å°„å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_arxiv_seeds:
            self.arxiv_retriever = ArxivSeedRetriever(
                max_results_per_query=self.seed_count * 2,  # å¤šå–ä¸€äº›ï¼Œæ˜ å°„åå¯èƒ½ä¼šå‡å°‘
                years_back=self.arxiv_years_back,
                min_relevance_score=0.6,
                llm_client=self.llm_client,  # ä¼ é€’LLMå®¢æˆ·ç«¯
                use_llm_query_generation=True  # å¯ç”¨LLMæŸ¥è¯¢ç”Ÿæˆ
            )
            self.cross_mapper = CrossDatabaseMapper(
                client=self.client,
                min_concept_score=0.3,
                required_concepts=["Computer Science"]
            )
        else:
            self.arxiv_retriever = None
            self.cross_mapper = None

        # å­˜å‚¨æ£€ç´¢åˆ°çš„è®ºæ–‡
        self.seed_papers: List[Dict] = []
        self.citing_papers: List[Dict] = []  # å­èŠ‚ç‚¹ï¼šå¼•ç”¨ç§å­çš„è®ºæ–‡
        self.ancestor_papers: List[Dict] = []  # çˆ¶èŠ‚ç‚¹ï¼šè¢«ç§å­å¼•ç”¨çš„è®ºæ–‡
        self.cocitation_papers: List[Dict] = []  # å…±å¼•è®ºæ–‡ï¼šè¢«åå¤æåŠçš„è®ºæ–‡
        self.recent_papers: List[Dict] = []
        self.all_papers: Dict[str, Dict] = {}  # paper_id -> paper_data

        # å­˜å‚¨å¼•ç”¨å…³ç³»
        self.citation_edges: Set[Tuple[str, str]] = set()  # (citing_id, cited_id)

        # ç¬¬äºŒè½®ç»Ÿè®¡ï¼ˆç”¨äºæŠ¥å‘Šç”Ÿæˆï¼‰
        self.first_round_citing_count: int = 0
        self.first_round_ancestor_count: int = 0
        self.first_round_cocitation_count: int = 0
        self.second_round_citing_count: int = 0
        self.second_round_ancestor_count: int = 0
        self.second_round_cocitation_count: int = 0

        logger.info("æ»šé›ªçƒæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  é…ç½®æ¥æº: {config_path if Path(config_path).exists() else 'é»˜è®¤å€¼'}")
        logger.info(f"  ç§å­æ£€ç´¢æ¨¡å¼: {'arXivä¼˜å…ˆ' if self.use_arxiv_seeds else 'OpenAlexç›´æ¥æœç´¢'}")
        logger.info(f"  seed_count={self.seed_count}, citations_per_seed={self.citations_per_seed}")
        logger.info(f"  recent_count={self.recent_count}, enable_second_round={self.enable_second_round}")
        if self.enable_second_round:
            logger.info(f"  second_round_limit={self.second_round_limit}")
        if self.use_arxiv_seeds:
            logger.info(f"  arxiv_years_back={self.arxiv_years_back}")

    def _load_config(self, config_path: str) -> Dict:
        """
        ä»é…ç½®æ–‡ä»¶åŠ è½½æ»šé›ªçƒæ£€ç´¢é…ç½®

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„

        Returns:
            æ»šé›ªçƒé…ç½®å­—å…¸ï¼ˆsnowballéƒ¨åˆ†ï¼‰
        """
        config_file = Path(config_path)

        if not config_file.exists():
            logger.debug(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
            return {}

        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                full_config = yaml.safe_load(f)

            snowball_config = full_config.get('snowball', {}) if full_config else {}
            logger.debug(f"æˆåŠŸåŠ è½½æ»šé›ªçƒé…ç½®: {config_path}")
            return snowball_config

        except Exception as e:
            logger.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
            return {}

    def execute_full_pipeline(
        self,
        topic: str,
        content_keyword: str,
        seed_year_threshold: int = 2023
    ) -> Dict:
        """
        æ‰§è¡Œå®Œæ•´çš„å…­æ­¥æ£€ç´¢æµç¨‹

        Args:
            topic: ä¸»é¢˜å…³é”®è¯
            content_keyword: å†…å®¹å…³é”®è¯
            seed_year_threshold: ç§å­è®ºæ–‡çš„å¹´ä»½é˜ˆå€¼ï¼ˆå°äºæ­¤å¹´ä»½ï¼‰

        Returns:
            åŒ…å«æ‰€æœ‰è®ºæ–‡å’Œå¼•ç”¨å…³ç³»çš„å­—å…¸
        """
        logger.info(f"å¼€å§‹æ‰§è¡Œå®Œæ•´æ£€ç´¢æµç¨‹: topic='{topic}', content='{content_keyword}'")

        # ç¬¬ä¸€æ­¥ï¼šåŸºçŸ³ç§å­
        logger.info("\n" + "=" * 60)
        logger.info("ç¬¬ä¸€æ­¥ï¼šåŸºçŸ³ç§å­ (Seed Papers)")
        logger.info("=" * 60)
        self.seed_papers = self._select_seed_papers(
            topic=topic,
            content_keyword=content_keyword,
            year_threshold=seed_year_threshold
        )

        # ç¬¬äºŒæ­¥ï¼šæ­£å‘æ»šé›ªçƒ - æ‰¾å­èŠ‚ç‚¹
        logger.info("\n" + "=" * 60)
        logger.info("ç¬¬äºŒæ­¥ï¼šæ­£å‘æ»šé›ªçƒ (Forward Snowballing)")
        logger.info("Seed -> è°å¼•ç”¨äº†Seed? -> å¾—åˆ°å­èŠ‚ç‚¹")
        logger.info("=" * 60)
        self.citing_papers = self._forward_snowballing(self.seed_papers)

        # ç¬¬ä¸‰æ­¥ï¼šåå‘æ»šé›ªçƒ - æ‰¾çˆ¶èŠ‚ç‚¹/ç¥–å…ˆ
        logger.info("\n" + "=" * 60)
        logger.info("ç¬¬ä¸‰æ­¥ï¼šåå‘æ»šé›ªçƒ (Backward Snowballing)")
        logger.info("è°è¢«Seedå¼•ç”¨äº†? <- Seed -> å¾—åˆ°çˆ¶èŠ‚ç‚¹/ç¥–å…ˆ")
        logger.info("=" * 60)
        self.ancestor_papers = self._backward_snowballing(self.seed_papers)

        # ç¬¬å››æ­¥ï¼šæ¨ªå‘è¡¥å……/å…±å¼•æŒ–æ˜
        logger.info("\n" + "=" * 60)
        logger.info("ç¬¬å››æ­¥ï¼šæ¨ªå‘è¡¥å……/å…±å¼•æŒ–æ˜ (Co-citation Mining)")
        logger.info("åœ¨å­èŠ‚ç‚¹å’Œçˆ¶èŠ‚ç‚¹ä¸­ï¼Œè°è¢«å¤§å®¶åå¤æåŠä½†è¿˜ä¸åœ¨åº“é‡Œ?")
        logger.info("=" * 60)
        self.cocitation_papers = self._cocitation_mining(
            self.citing_papers,
            self.ancestor_papers
        )

        # è®°å½•ç¬¬ä¸€è½®ç»Ÿè®¡
        self.first_round_citing_count = len(self.citing_papers)
        self.first_round_ancestor_count = len(self.ancestor_papers)
        self.first_round_cocitation_count = len(self.cocitation_papers)

        # ç¬¬äºŒè½®æ»šé›ªçƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_second_round:
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ”„ å¼€å§‹ç¬¬äºŒè½®æ»šé›ªçƒæ‰©å±•")
            logger.info("=" * 80)
            self._execute_second_round_snowballing()

        # ç¬¬äº”æ­¥ï¼šè¡¥å……æœ€æ–°SOTA
        logger.info("\n" + "=" * 60)
        logger.info("ç¬¬äº”æ­¥ï¼šè¡¥å……æœ€æ–°SOTA (Recent Frontiers)")
        logger.info("=" * 60)
        self.recent_papers = self._add_recent_frontiers(
            topic=topic,
            content_keyword=content_keyword,
            year_threshold=seed_year_threshold
        )

        # ç¬¬å…­æ­¥ï¼šæ„å»ºé—­åŒ…
        logger.info("\n" + "=" * 60)
        logger.info("ç¬¬å…­æ­¥ï¼šæ„å»ºå¼•ç”¨é—­åŒ… (Closure Construction)")
        logger.info("=" * 60)
        self._build_closure()

        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        result = self._generate_report()
        logger.info("\næ£€ç´¢æµç¨‹å®Œæˆï¼")
        return result

    def _select_seed_papers(
        self,
        topic: str,
        content_keyword: str,
        year_threshold: int
    ) -> List[Dict]:
        """
        ç¬¬ä¸€æ­¥ï¼šé€‰å®šåŸºçŸ³ç§å­è®ºæ–‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

        ç­–ç•¥ï¼š
        - å¦‚æœå¯ç”¨arXivï¼šä½¿ç”¨arXiv Categories + å…³é”®è¯ -> æ˜ å°„åˆ°OpenAlex -> ConceptéªŒè¯
        - å¦‚æœä¸å¯ç”¨ï¼šç›´æ¥åœ¨OpenAlexæœç´¢ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰

        Args:
            topic: ä¸»é¢˜å…³é”®è¯
            content_keyword: å†…å®¹å…³é”®è¯
            year_threshold: å¹´ä»½é˜ˆå€¼ï¼ˆarXivæ¨¡å¼ä¸‹ä¼šè¢«è¦†ç›–ï¼‰

        Returns:
            ç§å­è®ºæ–‡åˆ—è¡¨
        """
        if self.use_arxiv_seeds:
            logger.info("ğŸ¯ ä½¿ç”¨arXivä¼˜å…ˆç§å­æ£€ç´¢ç­–ç•¥")
            return self._select_seeds_from_arxiv(topic, content_keyword)
        else:
            logger.info("ğŸ” ä½¿ç”¨OpenAlexç›´æ¥æœç´¢ç­–ç•¥")
            return self._select_seeds_from_openalex(topic, content_keyword, year_threshold)

    def _select_seeds_from_arxiv(
        self,
        topic: str,
        content_keyword: str
    ) -> List[Dict]:
        """
        ä»arXivæ£€ç´¢ç§å­è®ºæ–‡å¹¶æ˜ å°„åˆ°OpenAlex

        æµç¨‹ï¼š
        1. arXivæ£€ç´¢ï¼ˆä½¿ç”¨Categories + å…³é”®è¯ï¼‰
        2. è·¨åº“æ˜ å°„ï¼ˆarXiv -> OpenAlexï¼‰
        3. Conceptè¿‡æ»¤ï¼ˆç¡®ä¿CS/AIé¢†åŸŸï¼‰

        Args:
            topic: ä¸»é¢˜
            content_keyword: å†…å®¹å…³é”®è¯

        Returns:
            æ˜ å°„åçš„ç§å­è®ºæ–‡åˆ—è¡¨ï¼ˆOpenAlexæ ¼å¼ï¼‰
        """
        logger.info("æ­¥éª¤1a: ä»arXivæ£€ç´¢é«˜è´¨é‡ç§å­")
        logger.info(f"  ä¸»é¢˜: '{topic}', å…³é”®è¯: '{content_keyword}'")
        logger.info(f"  å›æº¯å¹´æ•°: {self.arxiv_years_back}å¹´")

        # 1. æ£€ç´¢arXivè®ºæ–‡
        keywords = [content_keyword] + self.seed_keywords if self.seed_keywords else [content_keyword]
        arxiv_papers = self.arxiv_retriever.retrieve_seed_papers(
            topic=topic,
            keywords=keywords,
            max_seeds=self.seed_count * 2  # å¤šå–ä¸€äº›ï¼Œæ˜ å°„åä¼šå‡å°‘
        )

        logger.info(f"  âœ“ arXivæ£€ç´¢åˆ° {len(arxiv_papers)} ç¯‡å€™é€‰è®ºæ–‡")

        # 2. æ˜ å°„åˆ°OpenAlex
        logger.info("\næ­¥éª¤1b: è·¨åº“æ˜ å°„ï¼ˆarXiv -> OpenAlexï¼‰")
        mapped_papers, stats = self.cross_mapper.map_arxiv_to_openalex(
            arxiv_papers,
            verify_concepts=False  # arXivé˜¶æ®µå·²é€šè¿‡ç±»åˆ«è¿‡æ»¤ï¼Œæ— éœ€å†éªŒè¯æ¦‚å¿µ
        )

        logger.info(f"  âœ“ æˆåŠŸæ˜ å°„ {len(mapped_papers)} ç¯‡è®ºæ–‡")
        if stats.get('filtered_by_concept', 0) > 0:
            logger.info(f"  â„¹ï¸ æ¦‚å¿µè¿‡æ»¤: {stats.get('filtered_by_concept', 0)} ç¯‡ï¼ˆå·²ç¦ç”¨ï¼‰")

        # 3. å­˜å‚¨åˆ°all_papers
        seeds = []
        for paper in mapped_papers[:self.seed_count]:
            self.all_papers[paper['id']] = paper
            seeds.append(paper)
            logger.info(
                f"  âœ“ [{paper['year']}] {paper['title'][:60]}... "
                f"(å¼•ç”¨æ•°: {paper['cited_by_count']}, arXiv: {paper.get('arxiv_id', 'N/A')})"
            )

        logger.info(f"\nâœ… å…±é€‰å®š {len(seeds)} ç¯‡é«˜è´¨é‡ç§å­è®ºæ–‡ï¼ˆarXivéªŒè¯ï¼‰")
        return seeds

    def _select_seeds_from_openalex(
        self,
        topic: str,
        content_keyword: str,
        year_threshold: int
    ) -> List[Dict]:
        """
        ä»OpenAlexç›´æ¥æ£€ç´¢ç§å­è®ºæ–‡ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰

        Args:
            topic: ä¸»é¢˜å…³é”®è¯
            content_keyword: å†…å®¹å…³é”®è¯
            year_threshold: å¹´ä»½é˜ˆå€¼

        Returns:
            ç§å­è®ºæ–‡åˆ—è¡¨
        """
        query = f"{topic} {content_keyword}"
        logger.info(f"æœç´¢æŸ¥è¯¢: '{query}'")
        logger.info(f"ç­›é€‰æ¡ä»¶: publication_year < {year_threshold}, sorted by citations")

        params = {
            'search': query,
            'per-page': self.seed_count,
            'sort': 'cited_by_count:desc',
            'filter': f'publication_year:<{year_threshold},cited_by_count:>50'
        }

        try:
            data = self.client._make_request('works', params)
            results = data.get('results', [])

            seeds = []
            for result in results[:self.seed_count]:
                paper = self.client._parse_paper(result)
                seeds.append(paper)
                self.all_papers[paper['id']] = paper
                logger.info(
                    f"  âœ“ [{paper['year']}] {paper['title'][:60]}... "
                    f"(å¼•ç”¨æ•°: {paper['cited_by_count']})"
                )

            logger.info(f"å…±æ‰¾åˆ° {len(seeds)} ç¯‡åŸºçŸ³è®ºæ–‡")
            return seeds

        except Exception as e:
            logger.error(f"é€‰å®šåŸºçŸ³è®ºæ–‡å¤±è´¥: {e}")
            return []

    def _deduplicate_and_log(
        self,
        new_papers: List[Dict],
        existing_dict: Dict[str, Dict],
        paper_type: str
    ) -> Tuple[List[Dict], int, int]:
        """
        å»é‡å¹¶è®°å½•ç»Ÿè®¡ä¿¡æ¯

        Args:
            new_papers: æ–°æ£€ç´¢åˆ°çš„è®ºæ–‡åˆ—è¡¨
            existing_dict: å·²å­˜åœ¨çš„è®ºæ–‡å­—å…¸ {paper_id: paper_data}
            paper_type: è®ºæ–‡ç±»å‹æè¿°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            (å»é‡åçš„æ–°è®ºæ–‡åˆ—è¡¨, åŸå§‹æ•°é‡, é‡å¤æ•°é‡)
        """
        original_count = len(new_papers)
        duplicates = 0
        deduplicated = []

        for paper in new_papers:
            paper_id = paper['id']
            if paper_id not in existing_dict:
                deduplicated.append(paper)
                existing_dict[paper_id] = paper
            else:
                duplicates += 1

        final_count = len(deduplicated)

        logger.info(f"ğŸ“Š {paper_type}å»é‡ç»Ÿè®¡:")
        logger.info(f"   åŸæœ¬æ•°é‡: {original_count} ç¯‡")
        logger.info(f"   æ£€æµ‹åˆ°é‡å¤: {duplicates} ç¯‡")
        logger.info(f"   å»é‡åè¿›å…¥ä¸‹ä¸€æ­¥: {final_count} ç¯‡")

        return deduplicated, original_count, duplicates

    def _forward_snowballing(
        self,
        seed_papers: List[Dict],
        max_per_paper: Optional[int] = None
    ) -> List[Dict]:
        """
        ç¬¬äºŒæ­¥ï¼šæ­£å‘æ»šé›ªçƒ - æ‰¾ç»§æ‰¿è€…
        æ‰¾å‡ºè°å¼•ç”¨äº†è¿™äº›åŸºçŸ³è®ºæ–‡

        Args:
            seed_papers: ç§å­è®ºæ–‡åˆ—è¡¨
            max_per_paper: æ¯ç¯‡è®ºæ–‡æœ€å¤šæ‰©å±•çš„æ•°é‡ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼ï¼‰

        Returns:
            å¼•ç”¨è®ºæ–‡åˆ—è¡¨
        """
        if max_per_paper is None:
            max_per_paper = self.citations_per_seed

        citing_papers_list = []  # æ”¶é›†æ‰€æœ‰è®ºæ–‡

        for seed in seed_papers:
            seed_id = seed['id']
            seed_year = seed['year']

            logger.info(f"\nå¤„ç†ç§å­è®ºæ–‡: {seed['title'][:50]}...")
            logger.info(f"  ç§å­ID: {seed_id}, å¹´ä»½: {seed_year}")

            # è·å–å¼•ç”¨æ­¤è®ºæ–‡çš„æ‰€æœ‰æ–‡çŒ®
            citing = self._get_filtered_citations(
                work_id=seed_id,
                min_year=seed_year,
                keywords=self.seed_keywords,
                max_results=max_per_paper
            )

            logger.info(f"  æ‰¾åˆ° {len(citing)} ç¯‡ç›¸å…³å¼•ç”¨è®ºæ–‡")

            for paper in citing:
                citing_papers_list.append(paper)
                # è®°å½•å¼•ç”¨å…³ç³»
                self.citation_edges.add((paper['id'], seed_id))

        # ç»Ÿä¸€å»é‡
        result, _, _ = self._deduplicate_and_log(
            citing_papers_list,
            self.all_papers,
            "æ­£å‘æ»šé›ªçƒ"
        )

        logger.info(f"\næ­£å‘æ»šé›ªçƒå®Œæˆï¼Œå…±æ”¶é›† {len(result)} ç¯‡ç»§æ‰¿è€…è®ºæ–‡ï¼ˆå»é‡åï¼‰")
        return result

    def _backward_snowballing(
        self,
        seed_papers: List[Dict],
        max_per_paper: Optional[int] = None
    ) -> List[Dict]:
        """
        ç¬¬ä¸‰æ­¥ï¼šåå‘æ»šé›ªçƒ - æ‰¾çˆ¶èŠ‚ç‚¹/ç¥–å…ˆ
        æ‰¾å‡ºè¿™äº›åŸºçŸ³è®ºæ–‡å¼•ç”¨äº†è°ï¼ˆå®ƒä»¬çš„å‚è€ƒæ–‡çŒ®ï¼‰

        Args:
            seed_papers: ç§å­è®ºæ–‡åˆ—è¡¨
            max_per_paper: æ¯ç¯‡è®ºæ–‡æœ€å¤šæ‰©å±•çš„æ•°é‡ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼ï¼‰

        Returns:
            ç¥–å…ˆè®ºæ–‡åˆ—è¡¨
        """
        if max_per_paper is None:
            max_per_paper = self.citations_per_seed

        ancestor_papers_list = []  # æ”¶é›†æ‰€æœ‰è®ºæ–‡

        for seed in seed_papers:
            seed_id = seed['id']
            logger.info(f"\nå¤„ç†ç§å­è®ºæ–‡: {seed['title'][:50]}...")
            logger.info(f"  ç§å­ID: {seed_id}")

            # è·å–æ­¤è®ºæ–‡å¼•ç”¨çš„å‚è€ƒæ–‡çŒ®
            references = self._get_references(
                work_id=seed_id,
                max_results=max_per_paper
            )

            logger.info(f"  æ‰¾åˆ° {len(references)} ç¯‡å‚è€ƒæ–‡çŒ®ï¼ˆçˆ¶èŠ‚ç‚¹ï¼‰")

            for ref in references:
                ancestor_papers_list.append(ref)
                # è®°å½•å¼•ç”¨å…³ç³»ï¼šç§å­å¼•ç”¨äº†ç¥–å…ˆ
                self.citation_edges.add((seed_id, ref['id']))

        # ç»Ÿä¸€å»é‡
        result, _, _ = self._deduplicate_and_log(
            ancestor_papers_list,
            self.all_papers,
            "åå‘æ»šé›ªçƒ"
        )

        logger.info(f"\nåå‘æ»šé›ªçƒå®Œæˆï¼Œå…±æ”¶é›† {len(result)} ç¯‡çˆ¶èŠ‚ç‚¹/ç¥–å…ˆè®ºæ–‡ï¼ˆå»é‡åï¼‰")
        return result

    def _cocitation_mining(
        self,
        citing_papers: List[Dict],
        ancestor_papers: List[Dict]
    ) -> List[Dict]:
        """
        ç¬¬å››æ­¥ï¼šæ¨ªå‘è¡¥å……/å…±å¼•æŒ–æ˜
        åœ¨å­èŠ‚ç‚¹å’Œçˆ¶èŠ‚ç‚¹ä¸­ï¼Œæ‰¾å‡ºè¢«å¤§å®¶åå¤æåŠä½†è¿˜ä¸åœ¨åº“é‡Œçš„è®ºæ–‡

        Args:
            citing_papers: å­èŠ‚ç‚¹è®ºæ–‡åˆ—è¡¨
            ancestor_papers: çˆ¶èŠ‚ç‚¹è®ºæ–‡åˆ—è¡¨

        Returns:
            å…±å¼•è®ºæ–‡åˆ—è¡¨
        """
        reference_counter = Counter()  # ç»Ÿè®¡æ¯ç¯‡å‚è€ƒæ–‡çŒ®è¢«å¼•ç”¨çš„æ¬¡æ•°
        all_references = []

        # åˆå¹¶å­èŠ‚ç‚¹å’Œçˆ¶èŠ‚ç‚¹ï¼ˆå»é‡ï¼‰
        seen_ids = set()
        all_nodes = []
        for paper in citing_papers + ancestor_papers:
            if paper['id'] not in seen_ids:
                all_nodes.append(paper)
                seen_ids.add(paper['id'])

        logger.info(f"åˆ†æ {len(all_nodes)} ç¯‡è®ºæ–‡çš„å…±å¼•æ¨¡å¼ï¼ˆå·²å»é‡ï¼‰...")

        # æ”¶é›†æ‰€æœ‰è®ºæ–‡çš„å‚è€ƒæ–‡çŒ®
        analysis_limit = min(30, len(all_nodes))  # é™åˆ¶æ•°é‡ä»¥æ§åˆ¶APIè°ƒç”¨
        for i, paper in enumerate(all_nodes[:analysis_limit], 1):
            logger.info(f"  [{i}/{analysis_limit}] åˆ†æ: {paper['title'][:40]}...")

            refs = self._get_references(paper['id'], max_results=10)
            for ref in refs:
                ref_id = ref['id']
                all_references.append(ref)
                reference_counter[ref_id] += 1

        # æ‰¾å‡ºè¢«å¤šæ¬¡å¼•ç”¨çš„è®ºæ–‡
        cocitation_papers_list = []
        threshold = 3  # è‡³å°‘è¢«3ç¯‡è®ºæ–‡å¼•ç”¨

        logger.info(f"\nå…±å¼•åˆ†æ: æ‰¾å‡ºè¢«é¢‘ç¹æåŠçš„è®ºæ–‡ï¼ˆé˜ˆå€¼: {threshold}æ¬¡ï¼‰")
        for ref_id, count in reference_counter.most_common(30):
            if count >= threshold:
                # æ‰¾åˆ°å¯¹åº”çš„è®ºæ–‡è¯¦æƒ…
                ref_paper = next((r for r in all_references if r['id'] == ref_id), None)
                if ref_paper:
                    cocitation_papers_list.append(ref_paper)
                    logger.info(
                        f"  âœ“ å€™é€‰å…±å¼•è®ºæ–‡: {ref_paper['title'][:50]}... "
                        f"(è¢«å¼•ç”¨{count}æ¬¡, æ€»å¼•ç”¨æ•°: {ref_paper['cited_by_count']})"
                    )

        # ç»Ÿä¸€å»é‡
        result, _, _ = self._deduplicate_and_log(
            cocitation_papers_list,
            self.all_papers,
            "å…±å¼•æŒ–æ˜"
        )

        logger.info(f"\nå…±å¼•æŒ–æ˜å®Œæˆï¼Œæ‰¾åˆ° {len(result)} ç¯‡è¢«åå¤æåŠçš„è®ºæ–‡ï¼ˆå»é‡åï¼‰")
        return result

    def _execute_second_round_snowballing(self):
        """
        æ‰§è¡Œç¬¬äºŒè½®æ»šé›ªçƒï¼šå¯¹ç¬¬ä¸€è½®å¾—åˆ°çš„è®ºæ–‡å†è¿›è¡Œä¸€è½®æ‰©å±•
        åŒ…æ‹¬ï¼šciting_papers, ancestor_papers, cocitation_papers
        """
        # åˆå¹¶ç¬¬ä¸€è½®å¾—åˆ°çš„æ‰€æœ‰è®ºæ–‡ï¼ˆå»é‡ï¼‰
        seen_ids = set()
        first_round_papers = []
        for paper in self.citing_papers + self.ancestor_papers + self.cocitation_papers:
            if paper['id'] not in seen_ids:
                first_round_papers.append(paper)
                seen_ids.add(paper['id'])

        logger.info(f"ç¬¬ä¸€è½®å…±å¾—åˆ° {len(first_round_papers)} ç¯‡è®ºæ–‡ï¼ˆå·²å»é‡ï¼‰")
        logger.info(f"æ¯ç¯‡è®ºæ–‡æœ€å¤šæ‰©å±• {self.second_round_limit} ä¸ªå¼•ç”¨")

        # ç¬¬äºŒè½®æ­£å‘æ»šé›ªçƒ
        logger.info("\n" + "-" * 60)
        logger.info("ç¬¬äºŒè½®æ­£å‘æ»šé›ªçƒï¼šä»ç¬¬ä¸€è½®è®ºæ–‡æ‰¾å­èŠ‚ç‚¹")
        logger.info("-" * 60)

        # ä½¿ç”¨ç»Ÿä¸€çš„æ–¹æ³•ï¼Œä¼ å…¥é™åˆ¶å‚æ•°
        second_citing = self._forward_snowballing(
            first_round_papers,
            max_per_paper=self.second_round_limit
        )
        self.second_round_citing_count = len(second_citing)

        # åˆå¹¶åˆ°ç¬¬ä¸€è½®ï¼ˆä½¿ç”¨å­—å…¸å»é‡ï¼‰
        citing_dict = {p['id']: p for p in self.citing_papers}
        before_merge = len(citing_dict)
        citing_dict.update({p['id']: p for p in second_citing})
        after_merge = len(citing_dict)
        self.citing_papers = list(citing_dict.values())

        logger.info(f"ğŸ“Š ç¬¬äºŒè½®æ­£å‘æ»šé›ªçƒä¸ç¬¬ä¸€è½®åˆå¹¶ç»Ÿè®¡:")
        logger.info(f"   ç¬¬ä¸€è½®å­èŠ‚ç‚¹: {len(self.citing_papers) - len(second_citing)} ç¯‡")
        logger.info(f"   ç¬¬äºŒè½®æ–°å¢: {self.second_round_citing_count} ç¯‡")
        logger.info(f"   åˆå¹¶æ—¶é‡å¤: {before_merge + self.second_round_citing_count - after_merge} ç¯‡")
        logger.info(f"   åˆå¹¶åæ€»è®¡: {len(self.citing_papers)} ç¯‡")

        # ç¬¬äºŒè½®åå‘æ»šé›ªçƒ
        logger.info("\n" + "-" * 60)
        logger.info("ç¬¬äºŒè½®åå‘æ»šé›ªçƒï¼šä»ç¬¬ä¸€è½®è®ºæ–‡æ‰¾çˆ¶èŠ‚ç‚¹")
        logger.info("-" * 60)

        second_ancestor = self._backward_snowballing(
            first_round_papers,
            max_per_paper=self.second_round_limit
        )
        self.second_round_ancestor_count = len(second_ancestor)

        # åˆå¹¶åˆ°ç¬¬ä¸€è½®
        ancestor_dict = {p['id']: p for p in self.ancestor_papers}
        before_merge = len(ancestor_dict)
        ancestor_dict.update({p['id']: p for p in second_ancestor})
        after_merge = len(ancestor_dict)
        self.ancestor_papers = list(ancestor_dict.values())

        logger.info(f"ğŸ“Š ç¬¬äºŒè½®åå‘æ»šé›ªçƒä¸ç¬¬ä¸€è½®åˆå¹¶ç»Ÿè®¡:")
        logger.info(f"   ç¬¬ä¸€è½®çˆ¶èŠ‚ç‚¹: {len(self.ancestor_papers) - len(second_ancestor)} ç¯‡")
        logger.info(f"   ç¬¬äºŒè½®æ–°å¢: {self.second_round_ancestor_count} ç¯‡")
        logger.info(f"   åˆå¹¶æ—¶é‡å¤: {before_merge + self.second_round_ancestor_count - after_merge} ç¯‡")
        logger.info(f"   åˆå¹¶åæ€»è®¡: {len(self.ancestor_papers)} ç¯‡")

        # ç¬¬äºŒè½®å…±å¼•æŒ–æ˜
        logger.info("\n" + "-" * 60)
        logger.info("ç¬¬äºŒè½®å…±å¼•æŒ–æ˜ï¼šåˆ†æç¬¬äºŒè½®è®ºæ–‡çš„å…±å¼•æ¨¡å¼")
        logger.info("-" * 60)

        second_cocitation = self._cocitation_mining(
            second_citing,
            second_ancestor
        )
        self.second_round_cocitation_count = len(second_cocitation)

        # åˆå¹¶åˆ°ç¬¬ä¸€è½®
        cocitation_dict = {p['id']: p for p in self.cocitation_papers}
        before_merge = len(cocitation_dict)
        cocitation_dict.update({p['id']: p for p in second_cocitation})
        after_merge = len(cocitation_dict)
        self.cocitation_papers = list(cocitation_dict.values())

        logger.info(f"ğŸ“Š ç¬¬äºŒè½®å…±å¼•æŒ–æ˜ä¸ç¬¬ä¸€è½®åˆå¹¶ç»Ÿè®¡:")
        logger.info(f"   ç¬¬ä¸€è½®å…±å¼•: {len(self.cocitation_papers) - len(second_cocitation)} ç¯‡")
        logger.info(f"   ç¬¬äºŒè½®æ–°å¢: {self.second_round_cocitation_count} ç¯‡")
        logger.info(f"   åˆå¹¶æ—¶é‡å¤: {before_merge + self.second_round_cocitation_count - after_merge} ç¯‡")
        logger.info(f"   åˆå¹¶åæ€»è®¡: {len(self.cocitation_papers)} ç¯‡")

        logger.info("\n" + "=" * 80)
        logger.info(f"âœ… ç¬¬äºŒè½®æ»šé›ªçƒå®Œæˆï¼ˆå·²ä¸ç¬¬ä¸€è½®å»é‡åˆå¹¶ï¼‰")
        logger.info(f"   æœ€ç»ˆè®ºæ–‡æ€»æ•°ï¼ˆå»é‡åï¼‰:")
        logger.info(f"     - å­èŠ‚ç‚¹: {len(self.citing_papers)} ç¯‡")
        logger.info(f"     - çˆ¶èŠ‚ç‚¹: {len(self.ancestor_papers)} ç¯‡")
        logger.info(f"     - å…±å¼•è®ºæ–‡: {len(self.cocitation_papers)} ç¯‡")
        logger.info(f"     - åˆè®¡: {len(self.citing_papers) + len(self.ancestor_papers) + len(self.cocitation_papers)} ç¯‡")
        logger.info("=" * 80)

    def _add_recent_frontiers(
        self,
        topic: str,
        content_keyword: str,
        year_threshold: int
    ) -> List[Dict]:
        """
        ç¬¬äº”æ­¥ï¼šè¡¥å……æœ€æ–°SOTAè®ºæ–‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

        ç­–ç•¥ï¼š
        - å¦‚æœå¯ç”¨arXivï¼šä½¿ç”¨arXivæ£€ç´¢æœ€æ–°è®ºæ–‡ï¼ˆ6-12ä¸ªæœˆï¼‰-> æ˜ å°„åˆ°OpenAlex
        - å¦‚æœä¸å¯ç”¨ï¼šä½¿ç”¨OpenAlexæœç´¢æœ€æ–°è®ºæ–‡

        Args:
            topic: ä¸»é¢˜å…³é”®è¯
            content_keyword: å†…å®¹å…³é”®è¯
            year_threshold: å¹´ä»½é˜ˆå€¼ï¼ˆå¤§äºç­‰äºæ­¤å¹´ä»½ï¼‰

        Returns:
            æœ€æ–°è®ºæ–‡åˆ—è¡¨
        """
        if self.use_arxiv_seeds:
            logger.info("ğŸ¯ ä½¿ç”¨arXivæ£€ç´¢æœ€æ–°SOTAè®ºæ–‡")
            return self._add_recent_from_arxiv(topic, content_keyword)
        else:
            logger.info("ğŸ” ä½¿ç”¨OpenAlexæ£€ç´¢æœ€æ–°è®ºæ–‡")
            return self._add_recent_from_openalex(topic, content_keyword, year_threshold)

    def _add_recent_from_arxiv(
        self,
        topic: str,
        content_keyword: str
    ) -> List[Dict]:
        """
        ä»arXivæ£€ç´¢æœ€æ–°è®ºæ–‡å¹¶æ˜ å°„åˆ°OpenAlex

        Args:
            topic: ä¸»é¢˜
            content_keyword: å†…å®¹å…³é”®è¯

        Returns:
            æœ€æ–°è®ºæ–‡åˆ—è¡¨ï¼ˆOpenAlexæ ¼å¼ï¼‰
        """
        logger.info("  ä»arXivæ£€ç´¢æœ€æ–°6-12ä¸ªæœˆçš„å‰æ²¿è®ºæ–‡")

        # 1. æ£€ç´¢arXivæœ€æ–°è®ºæ–‡
        keywords = [content_keyword] + self.seed_keywords if self.seed_keywords else [content_keyword]
        arxiv_recent = self.arxiv_retriever.retrieve_recent_papers(
            topic=topic,
            keywords=keywords,
            max_results=self.recent_count * 2,  # å¤šå–ä¸€äº›
            months_back=12
        )

        logger.info(f"  âœ“ arXivæ£€ç´¢åˆ° {len(arxiv_recent)} ç¯‡æœ€æ–°è®ºæ–‡")

        # 2. æ˜ å°„åˆ°OpenAlexï¼ˆä¸å¼ºåˆ¶ConceptéªŒè¯ï¼Œæ–°è®ºæ–‡å¯èƒ½è¿˜æœªè¢«æ ‡æ³¨ï¼‰
        mapped_recent, stats = self.cross_mapper.map_arxiv_to_openalex(
            arxiv_recent,
            verify_concepts=False  # æœ€æ–°è®ºæ–‡æ”¾å®½éªŒè¯
        )

        logger.info(f"  âœ“ æˆåŠŸæ˜ å°„ {len(mapped_recent)} ç¯‡æœ€æ–°è®ºæ–‡")

        # 3. å»é‡å¹¶å­˜å‚¨
        recent_papers_list = []
        for paper in mapped_recent[:self.recent_count]:
            recent_papers_list.append(paper)
            logger.info(
                f"  âœ“ æœ€æ–°: [{paper.get('published_date', paper['year'])}] "
                f"{paper['title'][:60]}... (arXiv: {paper.get('arxiv_id', 'N/A')})"
            )

        # å»é‡
        result, _, _ = self._deduplicate_and_log(
            recent_papers_list,
            self.all_papers,
            "æœ€æ–°SOTAè®ºæ–‡ï¼ˆarXivï¼‰"
        )

        logger.info(f"âœ… å…±æ·»åŠ  {len(result)} ç¯‡æœ€æ–°SOTAè®ºæ–‡ï¼ˆarXivï¼‰")
        return result

    def _add_recent_from_openalex(
        self,
        topic: str,
        content_keyword: str,
        year_threshold: int
    ) -> List[Dict]:
        """
        ä»OpenAlexæ£€ç´¢æœ€æ–°è®ºæ–‡ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰

        Args:
            topic: ä¸»é¢˜å…³é”®è¯
            content_keyword: å†…å®¹å…³é”®è¯
            year_threshold: å¹´ä»½é˜ˆå€¼ï¼ˆå¤§äºç­‰äºæ­¤å¹´ä»½ï¼‰

        Returns:
            æœ€æ–°è®ºæ–‡åˆ—è¡¨
        """
        query = f"{topic} {content_keyword}"
        logger.info(f"æœç´¢æœ€æ–°è®ºæ–‡: '{query}'")
        logger.info(f"ç­›é€‰æ¡ä»¶: publication_year >= {year_threshold}")

        params = {
            'search': query,
            'per-page': self.recent_count,
            'sort': 'cited_by_count:desc',  # åœ¨æœ€æ–°è®ºæ–‡ä¸­é€‰å¼•ç”¨æ•°é«˜çš„
            'filter': f'publication_year:>{year_threshold},cited_by_count:>5'
        }

        try:
            data = self.client._make_request('works', params)
            results = data.get('results', [])

            recent_papers_list = []
            for result in results[:self.recent_count]:
                paper = self.client._parse_paper(result)
                recent_papers_list.append(paper)
                logger.info(
                    f"  âœ“ å€™é€‰æœ€æ–°è®ºæ–‡: [{paper['year']}] {paper['title'][:60]}... "
                    f"(å¼•ç”¨æ•°: {paper['cited_by_count']})"
                )

            # ç»Ÿä¸€å»é‡
            result, _, _ = self._deduplicate_and_log(
                recent_papers_list,
                self.all_papers,
                "æœ€æ–°SOTAè®ºæ–‡"
            )

            logger.info(f"å…±æ·»åŠ  {len(result)} ç¯‡æœ€æ–°SOTAè®ºæ–‡ï¼ˆå»é‡åï¼‰")
            return result

        except Exception as e:
            logger.error(f"è¡¥å……æœ€æ–°è®ºæ–‡å¤±è´¥: {e}")
            return []

    def _build_closure(self):
        """
        ç¬¬äº”æ­¥ï¼šæ„å»ºå¼•ç”¨é—­åŒ…
        ä¸ºæ‰€æœ‰è®ºæ–‡æ„å»ºå®Œæ•´çš„å¼•ç”¨å…³ç³»ç½‘ç»œ
        """
        paper_ids = list(self.all_papers.keys())
        total_papers = len(paper_ids)

        logger.info(f"å¼€å§‹ä¸º {total_papers} ç¯‡è®ºæ–‡æ„å»ºå¼•ç”¨é—­åŒ…...")

        # ä¸ºæ¯ç¯‡è®ºæ–‡è·å–å…¶å¼•ç”¨å…³ç³»
        for i, paper_id in enumerate(paper_ids, 1):
            paper = self.all_papers[paper_id]
            logger.info(
                f"  [{i}/{total_papers}] å¤„ç†: {paper['title'][:40]}..."
            )

            # è·å–è¯¥è®ºæ–‡å¼•ç”¨çš„å…¶ä»–è®ºæ–‡ï¼ˆåœ¨æˆ‘ä»¬çš„é›†åˆä¸­ï¼‰
            cited_papers = self._get_references(paper_id, max_results=20)

            for cited in cited_papers:
                cited_id = cited['id']
                # åªè®°å½•é›†åˆå†…çš„å¼•ç”¨å…³ç³»
                if cited_id in self.all_papers and cited_id != paper_id:
                    edge = (paper_id, cited_id)
                    if edge not in self.citation_edges:
                        self.citation_edges.add(edge)
                        logger.debug(f"    æ·»åŠ è¾¹: {paper_id} -> {cited_id}")

        logger.info(f"å¼•ç”¨é—­åŒ…æ„å»ºå®Œæˆï¼å…±å»ºç«‹ {len(self.citation_edges)} æ¡å¼•ç”¨å…³ç³»")

    def _get_filtered_citations(
        self,
        work_id: str,
        min_year: int,
        keywords: List[str],
        max_results: int
    ) -> List[Dict]:
        """
        è·å–ç»è¿‡è¿‡æ»¤çš„å¼•ç”¨è®ºæ–‡

        Args:
            work_id: è®ºæ–‡ID
            min_year: æœ€å°å¹´ä»½
            keywords: å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºç›¸å…³æ€§è¿‡æ»¤ï¼‰
            max_results: æœ€å¤§ç»“æœæ•°

        Returns:
            è¿‡æ»¤åçš„å¼•ç”¨è®ºæ–‡åˆ—è¡¨
        """
        if not work_id.startswith('W'):
            work_id = f"W{work_id}"

        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filters = [
            f'cites:{work_id}',
            f'publication_year:>{min_year}'
        ]

        params = {
            'filter': ','.join(filters),
            'per-page': max_results * 2,  # å¤šå–ä¸€äº›ï¼Œåç»­å†è¿‡æ»¤
            'sort': 'cited_by_count:desc'
        }

        try:
            data = self.client._make_request('works', params)
            results = data.get('results', [])

            # è§£æå¹¶è¿‡æ»¤è®ºæ–‡
            filtered = []
            for result in results:
                paper = self.client._parse_paper(result)

                # å¦‚æœæœ‰å…³é”®è¯è¦æ±‚ï¼Œè¿›è¡Œç›¸å…³æ€§è¿‡æ»¤
                if keywords and not self._is_relevant(paper, keywords):
                    continue

                filtered.append(paper)
                if len(filtered) >= max_results:
                    break

            return filtered

        except Exception as e:
            logger.error(f"è·å–è¿‡æ»¤åçš„å¼•ç”¨å¤±è´¥: {e}")
            return []

    def _get_references(self, work_id: str, max_results: int = 10) -> List[Dict]:
        """è·å–è®ºæ–‡çš„å‚è€ƒæ–‡çŒ®"""
        if not work_id.startswith('W'):
            work_id = f"W{work_id}"

        params = {
            'filter': f'cited_by:{work_id}',
            'per-page': max_results,
            'sort': 'cited_by_count:desc'
        }

        try:
            data = self.client._make_request('works', params)
            results = data.get('results', [])

            references = []
            for result in results:
                ref = self.client._parse_paper(result)
                references.append(ref)

            return references

        except Exception as e:
            logger.error(f"è·å–å‚è€ƒæ–‡çŒ®å¤±è´¥: {e}")
            return []

    def _is_relevant(self, paper: Dict, keywords: List[str]) -> bool:
        """
        æ£€æŸ¥è®ºæ–‡æ˜¯å¦ä¸å…³é”®è¯ç›¸å…³

        Args:
            paper: è®ºæ–‡æ•°æ®
            keywords: å…³é”®è¯åˆ—è¡¨

        Returns:
            æ˜¯å¦ç›¸å…³
        """
        # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡ŒåŒ¹é…
        text = (paper.get('title', '') + ' ' + paper.get('abstract', '')).lower()

        # è‡³å°‘åŒ¹é…ä¸€ä¸ªå…³é”®è¯
        return any(keyword.lower() in text for keyword in keywords)

    def _generate_report(self) -> Dict:
        """
        ç”Ÿæˆæ£€ç´¢æŠ¥å‘Š

        Returns:
            åŒ…å«æ‰€æœ‰æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        report = {
            'statistics': {
                'total_papers': len(self.all_papers),
                'seed_papers': len(self.seed_papers),
                'citing_papers': len(self.citing_papers),
                'ancestor_papers': len(self.ancestor_papers),
                'cocitation_papers': len(self.cocitation_papers),
                'recent_papers': len(self.recent_papers),
                'total_edges': len(self.citation_edges),
                # ç¬¬ä¸€è½®ç»Ÿè®¡
                'first_round_citing': self.first_round_citing_count,
                'first_round_ancestor': self.first_round_ancestor_count,
                'first_round_cocitation': self.first_round_cocitation_count,
                # ç¬¬äºŒè½®ç»Ÿè®¡
                'second_round_citing': self.second_round_citing_count,
                'second_round_ancestor': self.second_round_ancestor_count,
                'second_round_cocitation': self.second_round_cocitation_count,
                # ç¬¬äºŒè½®æ˜¯å¦å¯ç”¨
                'second_round_enabled': self.enable_second_round
            },
            'papers': self.all_papers,
            'citation_edges': list(self.citation_edges),
            'seed_ids': [p['id'] for p in self.seed_papers],
            'citing_ids': [p['id'] for p in self.citing_papers],
            'ancestor_ids': [p['id'] for p in self.ancestor_papers],
            'cocitation_ids': [p['id'] for p in self.cocitation_papers],
            'recent_ids': [p['id'] for p in self.recent_papers]
        }

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        logger.info("\n" + "=" * 60)
        logger.info("æ£€ç´¢ç»Ÿè®¡æŠ¥å‘Š")
        logger.info("=" * 60)
        logger.info(f"æ€»è®ºæ–‡æ•°: {report['statistics']['total_papers']}")
        logger.info(f"  1. åŸºçŸ³ç§å­: {report['statistics']['seed_papers']}")
        logger.info(f"  2. å­èŠ‚ç‚¹(å¼•ç”¨ç§å­): {report['statistics']['citing_papers']}")
        logger.info(f"     - ç¬¬ä¸€è½®: {report['statistics']['first_round_citing']}")
        if self.enable_second_round:
            logger.info(f"     - ç¬¬äºŒè½®: {report['statistics']['second_round_citing']}")
        logger.info(f"  3. çˆ¶èŠ‚ç‚¹(è¢«ç§å­å¼•ç”¨): {report['statistics']['ancestor_papers']}")
        logger.info(f"     - ç¬¬ä¸€è½®: {report['statistics']['first_round_ancestor']}")
        if self.enable_second_round:
            logger.info(f"     - ç¬¬äºŒè½®: {report['statistics']['second_round_ancestor']}")
        logger.info(f"  4. å…±å¼•è®ºæ–‡(æ¨ªå‘è¡¥å……): {report['statistics']['cocitation_papers']}")
        logger.info(f"     - ç¬¬ä¸€è½®: {report['statistics']['first_round_cocitation']}")
        if self.enable_second_round:
            logger.info(f"     - ç¬¬äºŒè½®: {report['statistics']['second_round_cocitation']}")
        logger.info(f"  5. æœ€æ–°SOTA: {report['statistics']['recent_papers']}")
        logger.info(f"æ€»å¼•ç”¨å…³ç³»æ•°: {report['statistics']['total_edges']}")
        logger.info(f"å¹³å‡æ¯ç¯‡è®ºæ–‡çš„è¿æ¥æ•°: {report['statistics']['total_edges'] / max(report['statistics']['total_papers'], 1):.2f}")
        logger.info("=" * 60)

        return report

    def export_to_graph_format(self) -> Dict:
        """
        å¯¼å‡ºä¸ºå›¾æ•°æ®æ ¼å¼ï¼ˆä¾¿äºå¯è§†åŒ–ï¼‰

        Returns:
            åŒ…å«èŠ‚ç‚¹å’Œè¾¹çš„å­—å…¸
        """
        nodes = []
        for paper_id, paper in self.all_papers.items():
            # ç¡®å®šèŠ‚ç‚¹ç±»å‹ï¼ˆä¼˜å…ˆçº§é¡ºåºï¼‰
            if paper_id in [p['id'] for p in self.seed_papers]:
                node_type = 'seed'
            elif paper_id in [p['id'] for p in self.ancestor_papers]:
                node_type = 'ancestor'
            elif paper_id in [p['id'] for p in self.citing_papers]:
                node_type = 'citing'
            elif paper_id in [p['id'] for p in self.cocitation_papers]:
                node_type = 'cocitation'
            elif paper_id in [p['id'] for p in self.recent_papers]:
                node_type = 'recent'
            else:
                node_type = 'other'

            nodes.append({
                'id': paper_id,
                'label': paper['title'][:50],
                'type': node_type,
                'year': paper['year'],
                'citations': paper['cited_by_count'],
                'authors': paper['authors']
            })

        edges = [
            {'source': source, 'target': target}
            for source, target in self.citation_edges
        ]

        return {
            'nodes': nodes,
            'edges': edges
        }


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logger.info("å¼€å§‹æµ‹è¯•æ»šé›ªçƒæ£€ç´¢ç³»ç»Ÿ...")

    # åˆ›å»ºæ£€ç´¢ç³»ç»Ÿ
    retrieval = SnowballRetrieval(
        seed_count=5,
        citations_per_seed=6,
        recent_count=10,
        seed_keywords=["reasoning", "chain of thought", "prompting", "thinking"]
    )

    # æ‰§è¡Œå®Œæ•´æµç¨‹
    result = retrieval.execute_full_pipeline(
        topic="Large Language Models",
        content_keyword="Reasoning",
        seed_year_threshold=2023
    )

    # å¯¼å‡ºå›¾æ•°æ®
    graph_data = retrieval.export_to_graph_format()
    logger.info(f"\nå›¾æ•°æ®å¯¼å‡ºå®Œæˆï¼š{len(graph_data['nodes'])} ä¸ªèŠ‚ç‚¹ï¼Œ{len(graph_data['edges'])} æ¡è¾¹")

    # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
    logger.info("\nç¤ºä¾‹è®ºæ–‡ï¼ˆå‰5ç¯‡ï¼‰ï¼š")
    for i, (paper_id, paper) in enumerate(list(result['papers'].items())[:5], 1):
        logger.info(f"{i}. [{paper['year']}] {paper['title']}")
        logger.info(f"   å¼•ç”¨æ•°: {paper['cited_by_count']}, ä½œè€…: {', '.join(paper['authors'])}")
