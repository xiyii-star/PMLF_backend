"""
Future Work Extractor (æœªæ¥å·¥ä½œæå–å™¨)
ä½¿ç”¨ç« èŠ‚å®šä½æ–¹æ³•æå–è®ºæ–‡çš„æœªæ¥å·¥ä½œæ–¹å‘

å·¥ä½œæµç¨‹:
1. ä½¿ç”¨ SectionLocatorAgent å®šä½ future work ç« èŠ‚
2. ä»å®šä½çš„ç« èŠ‚ä¸­æå– future work
3. ğŸ†• ä½¿ç”¨ CriticAgent å®¡æŸ¥å¹¶è‡ªåŠ¨é‡è¯•æå–
"""

import json
import logging
import re
from typing import List, Dict, Optional, Any
from pathlib import Path

# å¯¼å…¥æ•°æ®ç»“æ„
from data_structures import PaperDocument, ExtractionResult, FieldType, SectionScope, CriticFeedback

# å¯¼å…¥å…¶ä»–Agent
from SectionLocatorAgent import SectionLocatorAgent
from critic_agent import CriticAgent

# å¯¼å…¥LLMé…ç½®
import sys
sys.path.append(str(Path(__file__).parent.parent))
from src.llm_config import LLMClient, LLMConfig

logger = logging.getLogger(__name__)


class FutureWorkExtractor:
    """
    Future Work æå–å™¨
    ä½¿ç”¨ç« èŠ‚å®šä½æ–¹æ³•æå–æœªæ¥å·¥ä½œæ–¹å‘
    """

    def __init__(
        self,
        llm_client: LLMClient,
        use_critic: bool = True,
        max_context_length: int = 3000,
        max_iterations: int = 3
    ):
        """
        åˆå§‹åŒ– Future Work æå–å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            use_critic: æ˜¯å¦ä½¿ç”¨CriticAgentè¿›è¡Œè´¨é‡æ£€æŸ¥å’Œé‡è¯•
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            max_iterations: æœ€å¤§é‡è¯•æ¬¡æ•°(å½“use_critic=Trueæ—¶)
        """
        self.llm_client = llm_client
        self.use_critic = use_critic
        self.max_context_length = max_context_length

        # åˆå§‹åŒ–å­ç»„ä»¶
        self.locator = SectionLocatorAgent(llm_client)

        # ğŸ†• åˆå§‹åŒ– CriticAgent
        if use_critic:
            self.critic = CriticAgent(llm_client, max_iterations=max_iterations)
        else:
            self.critic = None

        self.system_prompt = self._build_system_prompt()

    def _build_system_prompt(self) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        return """You are a professional expert in scientific paper analysis.

Your Task: Extract the future work directions proposed in this paper.

Identification Techniques:

Look for keywords: future, next, further, explore, plan, will, could, would.
Focus on explicit mentions of "future work".
Look for improvement directions inferred from acknowledged limitations.
Pay attention to the outlook at the end of the Conclusion or Discussion sections.
Output Requirements:

List 2-4 specific future work directions.
Use 1-2 sentences to explain each direction.
Use bullet points format.
Output the content directly, avoiding meta-talk (e.g., do not say "According to the paragraph...").

"""

    def extract(self, paper: PaperDocument, feedback: Optional[CriticFeedback] = None) -> ExtractionResult:
        """
        æå–è®ºæ–‡çš„æœªæ¥å·¥ä½œæ–¹å‘

        Args:
            paper: è®ºæ–‡æ–‡æ¡£
            feedback: CriticAgentåé¦ˆï¼ˆç”¨äºé‡è¯•ï¼Œå¯é€‰ï¼‰

        Returns:
            ExtractionResult: æå–ç»“æœ
        """
        logger.info("ğŸ“‹ å¼€å§‹æå– Future Work...")

        # æ­¥éª¤1: å®šä½ç« èŠ‚ (å¦‚æœæœ‰åé¦ˆ,ä½¿ç”¨å»ºè®®çš„ç« èŠ‚)
        if feedback and feedback.suggested_sections:
            logger.info(f"  â†’ ä½¿ç”¨Criticå»ºè®®çš„ç« èŠ‚: {feedback.suggested_sections}")
            scope = SectionScope(
                field=FieldType.FUTURE_WORK,
                target_sections=feedback.suggested_sections,
                section_titles=[paper.sections[i].title for i in feedback.suggested_sections if i < len(paper.sections)],
                reasoning="Based on Critic feedback"
            )
        else:
            logger.info("  ğŸ” å®šä½ç›¸å…³ç« èŠ‚...")
            scope = self.locator.locate(paper, FieldType.FUTURE_WORK)

        if not scope.target_sections:
            logger.warning("  âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ç« èŠ‚")
            initial_result = ExtractionResult(
                field=FieldType.FUTURE_WORK,
                content="æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
                evidence=[],
                extraction_method="section_locator",
                confidence=0.0,
                iterations=1
            )

            # å¦‚æœå¯ç”¨criticä¸”æ˜¯é¦–æ¬¡æå–ï¼Œä»ç„¶å°è¯•è‡ªåŠ¨é‡è¯•
            if self.use_critic and self.critic and not feedback:
                return self._apply_critic(initial_result, paper, scope)
            return initial_result

        # æ­¥éª¤2: æå–ç›¸å…³æ®µè½
        logger.info("  ğŸ“– æå–ç›¸å…³æ®µè½...")
        relevant_chunks = self._extract_relevant_chunks(paper, scope)

        if not relevant_chunks:
            logger.warning("  âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ®µè½")
            initial_result = ExtractionResult(
                field=FieldType.FUTURE_WORK,
                content="æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
                evidence=[],
                extraction_method="section_locator",
                confidence=0.0,
                iterations=1
            )

            if self.use_critic and self.critic and not feedback:
                return self._apply_critic(initial_result, paper, scope)
            return initial_result

        # æ­¥éª¤3: ä½¿ç”¨LLMæå–
        logger.info("  ğŸ¤– ä½¿ç”¨LLMæå–...")
        content, evidence = self._extract_with_llm(relevant_chunks, paper.title, feedback)

        logger.info(f"  âœ… Future Workåˆæ­¥æå–å®Œæˆ")
        logger.info(f"     â†’ æœ€ç»ˆå†…å®¹: {content[:100]}...")

        initial_result = ExtractionResult(
            field=FieldType.FUTURE_WORK,
            content=content,
            evidence=evidence,
            extraction_method="section_locator",
            confidence=0.8 if content and "æœªæ‰¾åˆ°" not in content else 0.3,
            iterations=1
        )

        # ğŸ†• ä½¿ç”¨ CriticAgent è¿›è¡Œè´¨é‡æ£€æŸ¥å’Œè‡ªåŠ¨é‡è¯•
        if self.use_critic and self.critic and not feedback:
            return self._apply_critic(initial_result, paper, scope)

        return initial_result

    def _apply_critic(self, initial_result: ExtractionResult, paper: PaperDocument, scope: SectionScope) -> ExtractionResult:
        """åº”ç”¨CriticAgentè¿›è¡Œè´¨é‡æ£€æŸ¥å’Œé‡è¯•"""
        logger.info("\n  ğŸ” å¯åŠ¨ CriticAgent è´¨é‡æ£€æŸ¥...")

        # å®šä¹‰é‡æ–°æå–å‡½æ•°
        def retry_extract(paper_doc, critic_feedback):
            return self.extract(paper_doc, critic_feedback)

        # è°ƒç”¨ critique_and_retry
        final_result = self.critic.critique_and_retry(
            extraction=initial_result,
            paper=paper,
            extractor_func=retry_extract,
            scope=scope,
            evaluation_level="both"
        )

        return final_result

    def _extract_relevant_chunks(
        self,
        paper: PaperDocument,
        scope: SectionScope
    ) -> List[Dict]:
        """
        æå–ç›¸å…³æ®µè½ï¼ˆä½¿ç”¨å…³é”®è¯åŒ¹é…ï¼‰

        ğŸ†• ä¼˜åŒ–ç­–ç•¥:
        1. æ‰©å±•å…³é”®è¯åˆ—è¡¨ï¼ˆåŒ…æ‹¬æ›´å¤šfuture workçš„è¡¨è¿°ï¼‰
        2. è€ƒè™‘æ®µè½ä½ç½®ï¼ˆé¦–æ®µ/å°¾æ®µæƒé‡æ›´é«˜ï¼‰
        3. ç»“åˆå…³é”®è¯æƒé‡
        4. æ™ºèƒ½é™çº§ï¼šå½“åŒ¹é…å°‘æ—¶è‡ªåŠ¨æ‰©å±•åˆ°æ‰€æœ‰æ®µè½
        """
        # ğŸ†• æ‰©å±•å…³é”®è¯åˆ—è¡¨
        keywords = [
            # åŸºç¡€future workè¯
            'future', 'next', 'further', 'improve', 'extend',
            'explore', 'investigate', 'plan', 'ongoing',
            'will', 'could', 'would', 'intend',
            'remain to be', 'open question', 'direction',
            'outlook', 'prospect',
            # ğŸ†• æ›´å¤šfuture workçš„è¡¨è¿°
            'future work', 'future direction', 'future research',
            'future study', 'future effort', 'next step',
            'in the future', 'going forward', 'moving forward',
            'to be explored', 'to be investigated', 'to be addressed',
            'worth exploring', 'worth investigating',
            'potential direction', 'potential improvement',
            'promising direction', 'open problem'
        ]

        relevant_chunks = []

        for section_idx in scope.target_sections:
            section = paper.sections[section_idx]
            paragraphs = self._split_into_paragraphs(section.content)

            for para_idx, para in enumerate(paragraphs):
                # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
                keyword_count = sum(
                    1 for kw in keywords
                    if kw.lower() in para.lower()
                )

                if keyword_count > 0:
                    # ğŸ†• è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆè€ƒè™‘ä½ç½®å’Œå…³é”®è¯æƒé‡ï¼‰
                    score = keyword_count

                    # ğŸ†• ä½ç½®æƒé‡ï¼šé¦–æ®µå’Œå°¾æ®µæƒé‡é«˜
                    if para_idx == 0:  # é¦–æ®µ
                        score += 1.5
                    elif para_idx >= len(paragraphs) - 2:  # æœ«å°¾ä¸¤æ®µ
                        score += 2.5  # future workæ›´å¸¸å‡ºç°åœ¨æœ«å°¾

                    # ğŸ†• å…³é”®"future"è¯æƒé‡
                    future_indicators = ['future work', 'future direction', 'future research']
                    if any(fi in para.lower() for fi in future_indicators):
                        score += 2.0
                    elif 'future' in para.lower():
                        score += 1.5

                    # ğŸ†• "å±•æœ›"ç±»è¯æƒé‡
                    outlook_words = ['outlook', 'prospect', 'promising', 'potential']
                    if any(ow in para.lower() for ow in outlook_words):
                        score += 1.0

                    relevant_chunks.append({
                        'section': section.title,
                        'text': para,
                        'page': section.page_num,
                        'keyword_count': keyword_count,
                        'score': score,
                        'position': para_idx
                    })

        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        relevant_chunks.sort(key=lambda x: x['score'], reverse=True)

        # ğŸ†• æ™ºèƒ½é™çº§ï¼šå¦‚æœchunkså¾ˆå°‘(<3)ï¼Œæ‰©å±•åˆ°ç›®æ ‡ç« èŠ‚çš„æ‰€æœ‰æ®µè½
        max_chunks = 8  # å¢åŠ åˆ°8ä¸ª
        if len(relevant_chunks) < 3:
            logger.info(f"     â†’ å…³é”®è¯åŒ¹é…chunksè¾ƒå°‘({len(relevant_chunks)})ï¼Œæ‰©å±•åˆ°ç›®æ ‡ç« èŠ‚çš„æ‰€æœ‰æ®µè½")
            all_chunks = []
            for section_idx in scope.target_sections:
                section = paper.sections[section_idx]
                paragraphs = self._split_into_paragraphs(section.content)
                # ğŸ†• é‡ç‚¹æå–æ¯ä¸ªç« èŠ‚çš„æœ«å°¾æ®µè½ï¼ˆfuture worké€šå¸¸åœ¨è¿™é‡Œï¼‰
                for para_idx, para in enumerate(paragraphs):
                    # ä¼˜å…ˆé€‰æ‹©æœ«å°¾æ®µè½
                    if para_idx >= len(paragraphs) - 3 or para_idx < 2:
                        all_chunks.append({
                            'section': section.title,
                            'text': para,
                            'page': section.page_num,
                            'keyword_count': 0,
                            'score': 0.5 if para_idx >= len(paragraphs) - 3 else 0.3,
                            'position': para_idx
                        })
            # æŒ‰ä½ç½®å¾—åˆ†æ’åº
            all_chunks.sort(key=lambda x: x['score'], reverse=True)
            return all_chunks[:max_chunks]

        return relevant_chunks[:max_chunks]

    def _split_into_paragraphs(self, text: str) -> List[str]:
        """åˆ†å‰²æ®µè½"""
        paragraphs = re.split(r'\n\s*\n|\n', text)
        paragraphs = [p.strip() for p in paragraphs if len(p.strip()) > 30]
        return paragraphs

    def _extract_with_llm(
        self,
        chunks: List[Dict],
        paper_title: str,
        feedback: Optional[CriticFeedback] = None
    ) -> tuple:
        """
        ä½¿ç”¨LLMä»chunksä¸­æå–æœªæ¥å·¥ï¿½ï¿½

        Args:
            chunks: ç›¸å…³æ®µè½åˆ—è¡¨
            paper_title: è®ºæ–‡æ ‡é¢˜
            feedback: Criticåé¦ˆ(ç”¨äºå¢å¼ºæç¤º)

        Returns:
            (content, evidence)
        """
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, chunk in enumerate(chunks):
            context_parts.append(
                f"[Evidence {i+1}] From section '{chunk['section']}':\n{chunk['text']}"
            )
        context = "\n\n".join(context_parts)

        # ğŸ†• æ™ºèƒ½æˆªæ–­ï¼šä¼˜å…ˆä¿ç•™å¾—åˆ†æœ€é«˜çš„chunks
        if len(context) > self.max_context_length:
            # æŒ‰å¾—åˆ†é‡æ–°æ’åºï¼Œä¿ç•™æœ€ç›¸å…³çš„
            sorted_chunks = sorted(chunks, key=lambda x: x.get('score', 0), reverse=True)
            top_chunks = sorted_chunks[:5]  # ä¿ç•™å‰5ä¸ª
            context_parts = []
            for i, chunk in enumerate(top_chunks):
                context_parts.append(
                    f"[Evidence {i+1}] From section '{chunk['section']}':\n{chunk['text']}"
                )
            context = "\n\n".join(context_parts)
            logger.info(f"     â†’ ä¸Šä¸‹æ–‡è¿‡é•¿,ä¿ç•™å‰{len(top_chunks)}ä¸ªæœ€ç›¸å…³æ®µè½")
            chunks = top_chunks  # æ›´æ–°chunksç”¨äºåç»­evidenceæ„å»º

        # æ„å»ºæç¤ºè¯
        prompt = f"""è®ºæ–‡æ ‡é¢˜: {paper_title}

ä»»åŠ¡: æå–è®ºæ–‡ä¸­æå‡ºçš„æœªæ¥å·¥ä½œæ–¹å‘

ç›¸å…³æ®µè½:
{context}

è¾“å‡ºè¦æ±‚:
1. åˆ—ä¸¾2-4ä¸ªå…·ä½“çš„æœªæ¥å·¥ä½œæ–¹å‘
2. æ¯ä¸ªæ–¹å‘ç”¨1-2å¥è¯è¯´æ˜
3. ä½¿ç”¨bullet pointsæ ¼å¼ (ä»¥ "- " å¼€å¤´)
4. å…³æ³¨ä½œè€…æ˜ç¡®æåˆ°çš„future work
5. ä¹Ÿå¯ä»¥ä»limitationæ¨æ–­å‡ºæ”¹è¿›æ–¹å‘
6. âš ï¸ é‡è¦: å³ä½¿ä¿¡æ¯ä¸å®Œæ•´,ä¹Ÿè¦å°½é‡ä»æ®µè½ä¸­æå–ç›¸å…³å†…å®¹
7. åªæœ‰åœ¨æ®µè½å®Œå…¨ä¸ç›¸å…³æ—¶æ‰è¾“å‡º"æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°"
"""

        # ğŸ†• å¦‚æœæœ‰Criticåé¦ˆ,æ·»åŠ åˆ°æç¤ºä¸­
        if feedback and feedback.retry_prompt:
            prompt += f"\nâš ï¸ Criticåé¦ˆ:\n{feedback.retry_prompt}\n"

        prompt += "\nè¾“å‡º:"

        try:
            response = self.llm_client.generate(
                prompt=prompt,
                system_prompt=self.system_prompt,
                temperature=0.3,
                max_tokens=1000
            )

            # è®°å½•åŸå§‹å“åº”,ä¾¿äºè°ƒè¯•
            logger.debug(f"     â†’ LLMåŸå§‹å“åº”: {response[:200]}...")

            # è§£æå“åº”
            future_works = self._parse_llm_response(response)

            # ğŸ†• æ£€æµ‹LLMæ˜¯å¦è¿‡åº¦è°¨æ…è¿”å›"æœªæ‰¾åˆ°"
            if self._is_llm_being_too_cautious(future_works, chunks):
                logger.warning(f"     âš ï¸ LLMè¿”å›ä¸ºç©ºæˆ–è¿‡çŸ­,ä½†æœ‰{len(chunks)}æ¡è¯æ® - ä½¿ç”¨é™çº§ç­–ç•¥")
                fallback_content = self._fallback_extraction(chunks)
                content = fallback_content
            elif future_works:
                content = "\n".join([f"- {fw}" for fw in future_works])
            else:
                content = "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°"

            # æ„å»ºevidence
            evidence = [
                {
                    'section': chunk['section'],
                    'text': chunk['text'],
                    'page': chunk['page']
                }
                for chunk in chunks
            ]

            return content, evidence

        except Exception as e:
            logger.error(f"     âŒ LLMæå–å¤±è´¥: {e}")
            # é™çº§ï¼šä½¿ç”¨è§„åˆ™æå–
            fallback_content = self._fallback_extraction(chunks)
            evidence = [{'section': chunks[0]['section'], 'text': chunks[0]['text'], 'page': chunks[0]['page']}] if chunks else []
            return fallback_content, evidence

    def _parse_llm_response(self, response: str) -> List[str]:
        """
        è§£æLLMå“åº”ï¼Œæå–æœªæ¥å·¥ä½œåˆ—è¡¨

        è¿”å›: future_worksåˆ—è¡¨
        """
        future_works = []
        lines = response.strip().split('\n')

        for line in lines:
            line = line.strip()
            # åŒ¹é…bullet points
            if line.startswith('- ') or line.startswith('â€¢ ') or line.startswith('* '):
                future_work = line[2:].strip()
                if future_work and len(future_work) > 10:
                    future_works.append(future_work)
            # åŒ¹é…æ•°å­—åˆ—è¡¨
            elif re.match(r'^\d+[\.\)]\s+', line):
                future_work = re.sub(r'^\d+[\.\)]\s+', '', line).strip()
                if future_work and len(future_work) > 10:
                    future_works.append(future_work)

        return future_works

    def _fallback_extraction(self, chunks: List[Dict]) -> str:
        """
        é™çº§æå–ç­–ç•¥

        ğŸ†• æ”¹è¿›ç‰ˆæœ¬ï¼š
        1. æå–åŒ…å«å…³é”®è¯çš„å®Œæ•´å¥å­
        2. è¿‡æ»¤è¿‡é•¿/è¿‡çŸ­çš„å¥å­
        3. é™åˆ¶è¿”å›æ•°é‡
        """
        if not chunks:
            return "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°"

        future_works = []
        # é™å®šå…³é”®è¯ï¼šç¡®ä¿æå–çš„æ˜¯future workç›¸å…³å†…å®¹
        key_indicators = [
            'future', 'next', 'explore', 'plan', 'will',
            'could', 'would', 'further', 'improve', 'extend',
            'investigate', 'direction', 'outlook'
        ]

        for chunk in chunks[:4]:  # æ‰©å±•åˆ°å‰4ä¸ªchunks
            text = chunk['text'].strip()
            # ç®€å•æå–åŒ…å«å…³é”®è¯çš„å¥å­
            sentences = text.split('. ')
            for sentence in sentences:
                if any(kw in sentence.lower() for kw in key_indicators):
                    # è¿‡æ»¤é•¿åº¦
                    if 20 < len(sentence) < 300:
                        # æ¸…ç†å¥å­
                        cleaned = sentence.strip()
                        if cleaned and cleaned not in future_works:
                            future_works.append(cleaned)

        if future_works:
            return "\n".join([f"- {fw}" for fw in future_works[:4]])  # æœ€å¤šè¿”å›4æ¡
        else:
            return "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°"

    def _is_llm_being_too_cautious(self, future_works: List[str], chunks: List[Dict]) -> bool:
        """
        ğŸ†• æ£€æµ‹LLMæ˜¯å¦è¿‡åº¦è°¨æ…è¿”å›"æœªæ‰¾åˆ°"

        åˆ¤æ–­é€»è¾‘ï¼ˆå‚è€ƒDeepPaper_Agentï¼‰:
        - å¦‚æœfuture_worksä¸ºç©ºæˆ–åªæœ‰"æœªæ‰¾åˆ°"ç±»çš„å›ç­”
        - ä½†chunksæ•°é‡ >= 3 (è¯´æ˜æœ‰ç›¸å…³è¯æ®)
        - åˆ™è®¤ä¸ºLLMè¿‡åº¦è°¨æ…

        Args:
            future_works: LLMæå–çš„future worksåˆ—è¡¨
            chunks: ç›¸å…³æ®µè½åˆ—è¡¨

        Returns:
            bool: Trueè¡¨ç¤ºLLMè¿‡åº¦è°¨æ…
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if not future_works or len(future_works) == 0:
            return len(chunks) >= 3

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¡ç›®éƒ½æ˜¯"æœªæ‰¾åˆ°"ç±»çš„å›ç­”
        empty_indicators = [
            "æœªæ‰¾åˆ°æ˜ç¡®çš„æœªæ¥å·¥ä½œæè¿°",
            "æœªæ‰¾åˆ°",
            "æ²¡æœ‰æ‰¾åˆ°",
            "æ— ç›¸å…³å†…å®¹",
            "not found",
            "no relevant",
            "no information",
            "no future work",
            "no clear future"
        ]

        all_empty = all(
            any(indicator in fw.lower() for indicator in empty_indicators)
            for fw in future_works
        )

        if all_empty and len(chunks) >= 3:
            return True

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¡ç›®éƒ½å¤ªçŸ­ï¼ˆ<20å­—ç¬¦ï¼‰
        all_too_short = all(len(fw.strip()) < 20 for fw in future_works)
        if all_too_short and len(chunks) >= 2:
            return True

        return False


def main():
    """æµ‹è¯•ä»£ç """
    import argparse
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    parser = argparse.ArgumentParser(description="Future Work Extractor - æœªæ¥å·¥ä½œæå–")
    parser.add_argument("--config", required=True, help="LLMé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--paper", required=True, help="è®ºæ–‡æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰")
    parser.add_argument("--output", default="future_work_results.json", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        config = LLMConfig.from_file(args.config)
        llm_client = LLMClient(config)

        # è¯»å–è®ºæ–‡æ–‡æ¡£
        with open(args.paper, 'r', encoding='utf-8') as f:
            paper_data = json.load(f)

        # è½¬æ¢ä¸ºPaperDocumentå¯¹è±¡
        from data_structures import PaperSection
        sections = [
            PaperSection(**section) for section in paper_data.get('sections', [])
        ]
        paper = PaperDocument(
            paper_id=paper_data.get('paper_id', 'unknown'),
            title=paper_data.get('title', ''),
            abstract=paper_data.get('abstract', ''),
            authors=paper_data.get('authors', []),
            year=paper_data.get('year'),
            sections=sections,
            metadata=paper_data.get('metadata')
        )

        # åˆ›å»ºæå–å™¨
        extractor = FutureWorkExtractor(llm_client=llm_client)

        # æ‰§è¡Œæå–
        result = extractor.extract(paper)

        # ä¿å­˜ç»“æœ
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)

        # æ‰“å°ç»“æœ
        print("\n" + "=" * 80)
        print("Future Work Extraction Results")
        print("=" * 80)
        print(f"\nContent:\n{result.content}")
        print(f"\nEvidence Count: {len(result.evidence)}")
        print(f"Confidence: {result.confidence:.2f}")
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
