"""
Hypothesis Generator for Scientific Research
Uses Chain of Thought reasoning to generate feasible research ideas
"""

import json
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import networkx as nx

try:
    from langchain_openai import ChatOpenAI
    try:
        # Try new langchain structure (v0.1.0+)
        from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
        from langchain_core.output_parsers import PydanticOutputParser
    except ImportError:
        # Fallback to old langchain structure
        from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
        from langchain.output_parsers import PydanticOutputParser
    from pydantic import BaseModel, Field
except ImportError as e:
    raise ImportError(
        "Required packages not installed. Run: pip install langchain langchain-openai langchain-core pydantic"
    ) from e

logger = logging.getLogger(__name__)


class IdeaStatus(str, Enum):
    """Status of the generated idea"""
    SUCCESS = "SUCCESS"
    INCOMPATIBLE = "INCOMPATIBLE"


class InnovationIdea(BaseModel):
    """Structured output for generated research ideas"""
    status: IdeaStatus = Field(description="Whether the method is compatible with the limitation")
    title: Optional[str] = Field(default=None, description="Catchy academic title")
    abstract: Optional[str] = Field(
        default=None,
        description="Standard academic abstract (Background -> Gap -> Proposed Method -> Expected Result)"
    )
    modification: Optional[str] = Field(
        default=None,
        description="The specific modification needed (the 'Bridging Variable')"
    )
    reasoning: Optional[str] = Field(
        default=None,
        description="Chain of thought reasoning showing the analysis process"
    )


@dataclass
class IdeaFragment:
    """Research fragment (limitation or method)"""
    content: str
    paper_id: str = ""
    paper_title: str = ""
    year: int = 0
    cited_count: int = 0


class HypothesisGenerator:
    """
    Hypothesis Generator using Chain of Thought reasoning

    Process:
    1. Analyze Compatibility: Check mathematical/theoretical compatibility
    2. Identify the Gap: Determine what modification is needed
    3. Draft the Idea: Generate structured research proposal
    """

    def __init__(
        self,
        model_name: str = "gpt-4o",
        temperature: float = 0.3,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        use_step_structure: bool = True
    ):
        """
        Initialize the Hypothesis Generator

        Args:
            model_name: OpenAI model name (e.g., "gpt-4", "gpt-3.5-turbo")
            temperature: Sampling temperature (lower = more focused, higher = more creative)
            api_key: OpenAI API key (optional, defaults to OPENAI_API_KEY env variable)
            base_url: Optional base URL for API (useful for proxies or custom endpoints)
            use_step_structure: If True, use Step 1/2/3 structure in prompts (default: True)
        """
        llm_kwargs = {
            "model": model_name,
            "temperature": temperature,
        }

        if api_key:
            llm_kwargs["api_key"] = api_key
        if base_url:
            llm_kwargs["base_url"] = base_url

        self.llm = ChatOpenAI(**llm_kwargs)
        self.output_parser = PydanticOutputParser(pydantic_object=InnovationIdea)
        self.use_step_structure = use_step_structure

        # Build the prompt template
        self.prompt_template = self._build_prompt_template()

        logger.info(f"HypothesisGenerator initialized with model: {model_name}, use_step_structure: {use_step_structure}")

    def _build_prompt_template(self) -> ChatPromptTemplate:
        """Build the Chain of Thought prompt template"""

        if self.use_step_structure:
            # å®Œæ•´çš„ä¸‰æ­¥ç»“æ„åŒ–æç¤ºè¯ï¼ˆbaselineï¼‰
            system_message = SystemMessagePromptTemplate.from_template(
                """You are a **Senior Principal Researcher** with deep expertise in analyzing research problems and generating innovative solutions.

Your task is to evaluate whether a candidate method can solve a given research limitation, following a rigorous Chain of Thought reasoning process.

**Your reasoning must follow these three steps:**

**Step 1: Analyze Compatibility**
- Examine the mathematical, algorithmic, and theoretical properties of the method
- Check if these properties align with the constraints and requirements of the limitation
- Consider: computational complexity, applicability domain, underlying assumptions
- If fundamentally incompatible, output status="INCOMPATIBLE" and stop

**Step 2: Identify the Gap**
- Determine what specific modifications are needed to bridge the gap
- Identify the "Bridging Variable" - the key innovation that makes the connection work
- Ask: What needs to change in the method to address this new problem context?

**Step 3: Draft the Idea**
- Create a catchy, academic title
- Write a structured abstract following: Background â†’ Gap â†’ Proposed Method â†’ Expected Result
- Clearly state the core innovation in one sentence

{format_instructions}

Be rigorous and honest. If something won't work, say INCOMPATIBLE. Only output SUCCESS for truly feasible ideas."""
            )

            human_message = HumanMessagePromptTemplate.from_template(
                """**LIMITATION (Current Research Bottleneck):**
{limitation}

**CANDIDATE METHOD (Potential Solution):**
{method}

Now, follow the three-step Chain of Thought process:

1. **Compatibility Analysis**: Are the mathematical/algorithmic properties of this method suitable for the limitation's constraints?

2. **Gap Identification**: What specific modification or adaptation is needed?

3. **Idea Drafting**: If feasible, create the title, abstract, and describe the core innovation.

Provide your complete reasoning and final output in the specified JSON format."""
            )
        else:
            # ç®€åŒ–ç‰ˆæç¤ºè¯ï¼ˆç”¨äºæ¶ˆèå®éªŒ - ä¸åŒ…å«Step 1/2/3ç»“æ„ï¼‰
            system_message = SystemMessagePromptTemplate.from_template(
                """You are a **Senior Principal Researcher** with deep expertise in analyzing research problems and generating innovative solutions.

Your task is to evaluate whether a candidate method can solve a given research limitation.

**Your analysis should consider:**
- Mathematical, algorithmic, and theoretical compatibility
- Specific modifications needed to bridge the gap
- The key innovation that makes the connection work
- A catchy academic title and structured abstract

{format_instructions}

Be rigorous and honest. If something won't work, say INCOMPATIBLE. Only output SUCCESS for truly feasible ideas."""
            )

            human_message = HumanMessagePromptTemplate.from_template(
                """**LIMITATION (Current Research Bottleneck):**
{limitation}

**CANDIDATE METHOD (Potential Solution):**
{method}

Analyze whether this method can address the limitation. Consider compatibility, required modifications, and the core innovation. Provide your reasoning and final output in the specified JSON format."""
            )

        return ChatPromptTemplate.from_messages([system_message, human_message])

    def generate_innovation_idea(
        self,
        limitation: str,
        method: str,
        verbose: bool = False
    ) -> Dict:
        """
        Generate a research innovation idea from a limitation and candidate method

        Args:
            limitation: Description of the research bottleneck/limitation
            method: Description of the candidate method
            verbose: If True, print detailed reasoning

        Returns:
            Dictionary with structure:
            {
                "status": "SUCCESS" or "INCOMPATIBLE",
                "title": "...",
                "abstract": "...",
                "modification": "...",
                "reasoning": "..."
            }
        """
        try:
            # Format the prompt
            formatted_prompt = self.prompt_template.format_messages(
                limitation=limitation,
                method=method,
                format_instructions=self.output_parser.get_format_instructions()
            )

            if verbose:
                logger.info("=" * 80)
                logger.info("Generating innovation idea...")
                logger.info(f"Limitation: {limitation[:100]}...")
                logger.info(f"Method: {method[:100]}...")

            # Invoke the LLM
            response = self.llm.invoke(formatted_prompt)

            # Parse the structured output
            idea = self.output_parser.parse(response.content)

            if verbose:
                logger.info(f"Status: {idea.status}")
                if idea.status == IdeaStatus.SUCCESS:
                    logger.info(f"Title: {idea.title}")
                    logger.info(f"Modification: {idea.modification}")
                logger.info("=" * 80)

            # Convert to dictionary
            result = {
                "status": idea.status,
                "title": idea.title,
                "abstract": idea.abstract,
                "modification": idea.modification,
                "reasoning": idea.reasoning
            }

            return result

        except Exception as e:
            logger.error(f"Error generating innovation idea: {e}")
            return {
                "status": "ERROR",
                "title": None,
                "abstract": None,
                "modification": None,
                "reasoning": f"Error during generation: {str(e)}"
            }

    def batch_generate(
        self,
        unsolved_limitations: List[str],
        candidate_methods: List[str],
        max_ideas: int = 10,
        verbose: bool = False
    ) -> List[Dict]:
        """
        Generate multiple ideas by pairing limitations with methods

        Args:
            unsolved_limitations: List of limitation descriptions
            candidate_methods: List of method descriptions
            max_ideas: Maximum number of ideas to generate
            verbose: If True, print progress

        Returns:
            List of generated ideas (only successful ones)
        """
        ideas = []
        count = 0

        for limitation in unsolved_limitations:
            if count >= max_ideas:
                break

            for method in candidate_methods:
                if count >= max_ideas:
                    break

                if verbose:
                    logger.info(f"\nGenerating idea {count + 1}/{max_ideas}...")

                idea = self.generate_innovation_idea(limitation, method, verbose=False)

                # Only keep successful ideas
                if idea["status"] == "SUCCESS":
                    ideas.append({
                        "limitation": limitation,
                        "method": method,
                        **idea
                    })
                    count += 1

                    if verbose:
                        logger.info(f"âœ“ SUCCESS: {idea['title']}")
                else:
                    if verbose:
                        logger.info(f"âœ— INCOMPATIBLE: Method not suitable")

        return ideas


class KnowledgeGraphExtractor:
    """
    çŸ¥è¯†å›¾è°±æ•°æ®æå–å™¨

    ä»åŒ…å«è®ºæ–‡ä¿¡æ¯çš„çŸ¥è¯†å›¾è°±ä¸­æå–ç ”ç©¶å±€é™æ€§(Limitations)å’Œå€™é€‰æ–¹æ³•(Methods)ã€‚
    è¿™äº›æå–çš„å†…å®¹å°†ç”¨äºç ”ç©¶åˆ›æ„ç”Ÿæˆ,é€šè¿‡å°†æœªè§£å†³çš„å±€é™æ€§ä¸å€™é€‰æ–¹æ³•è¿›è¡Œç»„åˆæ¥äº§ç”Ÿæ–°çš„ç ”ç©¶æ–¹å‘ã€‚

    å·¥ä½œåŸç†:
        1. éå†çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
        2. ä»èŠ‚ç‚¹å±æ€§ä¸­æå– limitations (ç ”ç©¶ç“¶é¢ˆ/éœ€è¦è§£å†³çš„é—®é¢˜)
        3. ä»èŠ‚ç‚¹å±æ€§ä¸­æå– methods (æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆ/è´¡çŒ®)
        4. å¯¹æå–çš„å†…å®¹è¿›è¡Œè¿‡æ»¤å’Œå»é‡

    ä½¿ç”¨åœºæ™¯:
        - åœ¨ç”Ÿæˆç ”ç©¶åˆ›æ„ä¹‹å‰,ä»å·²æ„å»ºçš„æ–‡çŒ®çŸ¥è¯†å›¾è°±ä¸­æå–åŸææ–™
        - ä¸º HypothesisGenerator å‡†å¤‡è¾“å…¥æ•°æ®
    """

    @staticmethod
    def extract_from_graph(
        graph: nx.Graph,
        min_text_length: int = 50
    ) -> tuple[List[str], List[str]]:
        """
        ä» NetworkX çŸ¥è¯†å›¾è°±ä¸­æå–å±€é™æ€§å’Œæ–¹æ³•ï¼ˆåŸºäºå¼•ç”¨å…³ç³»ç±»å‹çš„ç¢ç‰‡æ± åŒ–ï¼‰

        ğŸ”Œ ç¢ç‰‡æ± åŒ–ç­–ç•¥ (Fragment Pooling)ï¼š
        é€šè¿‡åˆ†æå¼•ç”¨å…³ç³»ç±»å‹ï¼ˆSocket Matching çš„ç»“æœï¼‰ï¼Œæ™ºèƒ½ç­›é€‰é«˜è´¨é‡çš„ç ”ç©¶ç¢ç‰‡ã€‚

        ğŸ“¦ å››å¤§ç¢ç‰‡æ± ï¼š
        - Pool A (Unsolved Limitations): æœªè¢« Overcomes çš„ Limitation
          â†’ è¿™äº›æ˜¯å°šæœªè§£å†³çš„ç ”ç©¶ç“¶é¢ˆï¼Œæœ€å€¼å¾—æ”»å…‹
        - Pool B (Successful Methods): è¢« Extends å¤šæ¬¡çš„ Method
          â†’ è¿™äº›æ–¹æ³•è¢«å¤šæ¬¡æ‰©å±•ï¼Œè¯æ˜æ˜¯æˆç†Ÿå¯é çš„åŸºç¡€æŠ€æœ¯
        - Pool C (Cross-Domain Methods): æ¥è‡ª Adapts_to æºå¤´çš„ Method
          â†’ è¿™äº›æ–¹æ³•å·²è¯æ˜å…·æœ‰è·¨é¢†åŸŸè¿ç§»èƒ½åŠ›ï¼Œé€‚åˆæ–°åœºæ™¯
        - Pool D (Unrealized Future Work): æœªè¢« Realizes çš„ Future Work
          â†’ è¿™äº›æ˜¯å‰äººè®¾æƒ³ä½†å°šæœªå®ç°çš„ç ”ç©¶æ–¹å‘

        ğŸ”— Limitation æ¥æºï¼šPool A + Pool D
        ğŸ”§ Method æ¥æºï¼šPool B + Pool C

        Args:
            graph: NetworkX å›¾å¯¹è±¡ï¼ŒèŠ‚ç‚¹åŒ…å«è®ºæ–‡ä¿¡æ¯ï¼Œè¾¹åŒ…å«å¼•ç”¨å…³ç³»ç±»å‹
                   é¢„æœŸçš„èŠ‚ç‚¹å±æ€§:
                   - rag_limitation (str): RAG æå–çš„å±€é™æ€§
                   - rag_future_work (str): RAG æå–çš„æœªæ¥å·¥ä½œ
                   - rag_method (str): RAG æå–çš„è´¡çŒ®/æ–¹æ³•
                   é¢„æœŸçš„è¾¹å±æ€§:
                   - edge_type (str): å¼•ç”¨å…³ç³»ç±»å‹ (Overcomes, Realizes, Extends, etc.)
            min_text_length: æœ‰æ•ˆæ–‡æœ¬çš„æœ€å°é•¿åº¦ï¼Œé»˜è®¤ 50 å­—ç¬¦

        Returns:
            tuple[List[str], List[str]]:
                - unsolved_limitations: é«˜è´¨é‡çš„æœªè§£å†³å±€é™æ€§åˆ—è¡¨ï¼ˆPool A + Pool Dï¼‰
                - candidate_methods: é«˜è´¨é‡çš„å€™é€‰æ–¹æ³•åˆ—è¡¨ï¼ˆPool B + Pool Cï¼‰

        Example:
            >>> G = nx.Graph()
            >>> G.add_node('W1', rag_limitation='High complexity', rag_method='Method A')
            >>> G.add_node('W2', rag_method='Method B')
            >>> G.add_edge('W2', 'W1', edge_type='Extends')
            >>> limitations, methods = KnowledgeGraphExtractor.extract_from_graph(G)
        """
        logger.info("ğŸ”Œ å¼€å§‹ç¢ç‰‡æ± åŒ–æå– (Fragment Pooling based on Socket Matching)")

        # ===== ç»Ÿè®¡è¾¹ç±»å‹ä¿¡æ¯ =====
        # ç»Ÿè®¡æ¯ä¸ªèŠ‚ç‚¹è¢«å“ªäº›ç±»å‹çš„è¾¹æŒ‡å‘ï¼Œä»¥åŠæŒ‡å‘å“ªäº›èŠ‚ç‚¹
        node_incoming_edges = {}  # èŠ‚ç‚¹è¢«å“ªäº›è¾¹æŒ‡å‘ {node_id: [(source, edge_type), ...]}
        node_outgoing_edges = {}  # èŠ‚ç‚¹æŒ‡å‘å“ªäº›è¾¹ {node_id: [(target, edge_type), ...]}

        for source, target, edge_data in graph.edges(data=True):
            edge_type = edge_data.get('edge_type', 'Unknown')

            # è®°å½• target è¢« source é€šè¿‡ edge_type å¼•ç”¨
            if target not in node_incoming_edges:
                node_incoming_edges[target] = []
            node_incoming_edges[target].append((source, edge_type))

            # è®°å½• source é€šè¿‡ edge_type å¼•ç”¨äº† target
            if source not in node_outgoing_edges:
                node_outgoing_edges[source] = []
            node_outgoing_edges[source].append((target, edge_type))

        # ===== Pool A: Unsolved Limitations (æœªè¢« Overcomes çš„ Limitation) =====
        pool_a_limitations = []

        for node_id, node_data in graph.nodes(data=True):
            # æå– limitation
            limitation_text = node_data.get('rag_limitation', '')
            if not isinstance(limitation_text, str) or len(limitation_text.strip()) <= min_text_length:
                continue

            # æ£€æŸ¥æ˜¯å¦è¢« Overcomes
            incoming_edges = node_incoming_edges.get(node_id, [])
            is_overcome = any(edge_type == 'Overcomes' for _, edge_type in incoming_edges)

            if not is_overcome:
                # æœªè¢«è§£å†³çš„ limitation
                pool_a_limitations.append(limitation_text.strip())

        logger.info(f"ğŸ“¦ Pool A (Unsolved Limitations): {len(pool_a_limitations)} æ¡")

        # ===== Pool D: Unrealized Future Work (æœªè¢« Realizes çš„ Future Work) =====
        pool_d_limitations = []

        for node_id, node_data in graph.nodes(data=True):
            # æå– future_work
            future_work_text = node_data.get('rag_future_work', '')
            if not isinstance(future_work_text, str) or len(future_work_text.strip()) <= min_text_length:
                continue

            # æ£€æŸ¥æ˜¯å¦è¢« Realizes
            incoming_edges = node_incoming_edges.get(node_id, [])
            is_realized = any(edge_type == 'Realizes' for _, edge_type in incoming_edges)

            if not is_realized:
                # æœªå®ç°çš„ future work
                pool_d_limitations.append(future_work_text.strip())

        logger.info(f"ğŸ“¦ Pool D (Unrealized Future Work): {len(pool_d_limitations)} æ¡")

        # ===== Pool B: Successful Methods (è¢« Extends å¤šæ¬¡çš„ Method) =====
        pool_b_methods = []
        extends_threshold = 2  # è‡³å°‘è¢« Extends 2 æ¬¡æ‰ç®—æˆç†Ÿæ–¹æ³•

        for node_id, node_data in graph.nodes(data=True):
            # æå– contribution (method)
            contribution_text = node_data.get('rag_method', '')
            if not isinstance(contribution_text, str) or len(contribution_text.strip()) <= min_text_length:
                continue

            # ç»Ÿè®¡è¢« Extends çš„æ¬¡æ•°
            incoming_edges = node_incoming_edges.get(node_id, [])
            extends_count = sum(1 for _, edge_type in incoming_edges if edge_type == 'Extends')

            if extends_count >= extends_threshold:
                # è¢«å¤šæ¬¡æ‰©å±•çš„æˆç†Ÿæ–¹æ³•
                pool_b_methods.append(contribution_text.strip())

        logger.info(f"ğŸ“¦ Pool B (Successful Methods, Extendsâ‰¥{extends_threshold}): {len(pool_b_methods)} æ¡")

        # ===== Pool C: Cross-Domain Methods (æ¥è‡ª Adapts_to æºå¤´çš„ Method) =====
        pool_c_methods = []

        # æ‰¾å‡ºæ‰€æœ‰ Adapts_to è¾¹çš„æºèŠ‚ç‚¹
        adapts_to_sources = set()
        for source, target, edge_data in graph.edges(data=True):
            if edge_data.get('edge_type') == 'Adapts_to':
                adapts_to_sources.add(target)  # target æ˜¯è¢«è¿ç§»çš„æºè®ºæ–‡

        # æå–è¿™äº›æºèŠ‚ç‚¹çš„ method
        for node_id in adapts_to_sources:
            node_data = graph.nodes[node_id]
            contribution_text = node_data.get('rag_method', '')
            if isinstance(contribution_text, str) and len(contribution_text.strip()) > min_text_length:
                pool_c_methods.append(contribution_text.strip())

        logger.info(f"ğŸ“¦ Pool C (Cross-Domain Methods from Adapts_to): {len(pool_c_methods)} æ¡")

        # ===== åˆå¹¶æ± åŒ–ç»“æœ =====
        # Limitations = Pool A + Pool D
        unsolved_limitations = pool_a_limitations + pool_d_limitations
        # Methods = Pool B + Pool C
        candidate_methods = pool_b_methods + pool_c_methods

        # å»é‡
        unsolved_limitations = list(set(unsolved_limitations))
        candidate_methods = list(set(candidate_methods))

        # ===== é™çº§ç­–ç•¥ï¼šå¦‚æœç¢ç‰‡æ± åŒ–ç»“æœä¸è¶³ï¼Œè¡¥å……ä¼ ç»Ÿæ–¹æ³• =====
        if len(unsolved_limitations) < 3 or len(candidate_methods) < 3:
            logger.warning("âš ï¸ ç¢ç‰‡æ± åŒ–ç»“æœä¸è¶³ï¼Œå¯ç”¨é™çº§ç­–ç•¥ï¼ˆè¡¥å……ä¼ ç»Ÿæå–ï¼‰")
            fallback_limitations, fallback_methods = KnowledgeGraphExtractor._fallback_extract(
                graph, min_text_length
            )

            # è¡¥å……åˆ°ç°æœ‰æ± ä¸­
            unsolved_limitations.extend(fallback_limitations)
            candidate_methods.extend(fallback_methods)

            # å†æ¬¡å»é‡
            unsolved_limitations = list(set(unsolved_limitations))
            candidate_methods = list(set(candidate_methods))

            logger.info(f"  è¡¥å……å Limitations: {len(unsolved_limitations)} æ¡")
            logger.info(f"  è¡¥å……å Methods: {len(candidate_methods)} æ¡")

        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        logger.info(f"\\nâœ… ç¢ç‰‡æ± åŒ–å®Œæˆ:")
        logger.info(f"  ğŸ“Š Limitations (Pool A + Pool D): {len(unsolved_limitations)} æ¡")
        logger.info(f"  ğŸ”§ Methods (Pool B + Pool C): {len(candidate_methods)} æ¡")

        return unsolved_limitations, candidate_methods

    @staticmethod
    def _fallback_extract(
        graph: nx.Graph,
        min_text_length: int = 50
    ) -> tuple[List[str], List[str]]:
        """
        é™çº§æå–ç­–ç•¥ï¼šå½“ç¢ç‰‡æ± åŒ–ç»“æœä¸è¶³æ—¶ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¡¥å……

        ç®€å•åœ°ä»æ‰€æœ‰èŠ‚ç‚¹æå– limitation å’Œ contributionï¼Œä¸è€ƒè™‘å¼•ç”¨å…³ç³»

        Args:
            graph: NetworkX å›¾å¯¹è±¡
            min_text_length: æœ€å°æ–‡æœ¬é•¿åº¦

        Returns:
            tuple[List[str], List[str]]: (limitations, methods)
        """
        fallback_limitations = []
        fallback_methods = []

        for _, node_data in graph.nodes(data=True):
            # æå– limitation
            limitation_text = node_data.get('rag_limitation', '')
            if isinstance(limitation_text, str) and len(limitation_text.strip()) > min_text_length:
                fallback_limitations.append(limitation_text.strip())

            # æå– contribution
            contribution_text = node_data.get('rag_method', '')
            if isinstance(contribution_text, str) and len(contribution_text.strip()) > min_text_length:
                fallback_methods.append(contribution_text.strip())

        return list(set(fallback_limitations)), list(set(fallback_methods))


class ResearchIdeaGenerator:
    """
    ç ”ç©¶åˆ›æ„ç”Ÿæˆå™¨ - ä¸¤æ­¥æµç¨‹ï¼šè·å– â†’ ç”Ÿæˆ

    ğŸ“‹ æ ¸å¿ƒæµç¨‹ï¼ˆä¸¤æ­¥ï¼‰ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Step 1: è·å– Limitation å’Œ Method                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
    â”‚  - KnowledgeGraphExtractor.extract_from_graph()           â”‚
    â”‚  - ç¢ç‰‡æ± åŒ–ï¼šåŸºäºå¼•ç”¨å…³ç³»ç±»å‹ï¼ˆSocket Matchingï¼‰           â”‚
    â”‚  - Pool A: æœªè¢« Overcomes çš„ Limitation                   â”‚
    â”‚  - Pool B: è¢« Extends â‰¥2 æ¬¡çš„ Method                      â”‚
    â”‚  - Pool C: æ¥è‡ª Adapts_to çš„ Method                       â”‚
    â”‚  - Pool D: æœªè¢« Realizes çš„ Future Work                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Step 2: åˆ›æ„ç”Ÿæˆï¼ˆå«è‡ªåŠ¨è¿‡æ»¤ï¼‰                            â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
    â”‚  - HypothesisGenerator.batch_generate()                   â”‚
    â”‚  - Limitation Ã— Method ç¬›å¡å°”ç§¯                            â”‚
    â”‚  - Chain of Thought æ¨ç†ï¼š                                 â”‚
    â”‚    1. Compatibility Analysisï¼ˆå…¼å®¹æ€§åˆ†æï¼‰                 â”‚
    â”‚    2. Gap Identificationï¼ˆå·®è·è¯†åˆ«ï¼‰                       â”‚
    â”‚    3. Idea Draftingï¼ˆåˆ›æ„è‰æ‹Ÿï¼‰                            â”‚
    â”‚  - è‡ªåŠ¨è¿‡æ»¤ï¼šåªä¿ç•™ status="SUCCESS" çš„åˆ›æ„                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    æ¶æ„è®¾è®¡:
        ResearchIdeaGenerator (é«˜å±‚æ¥å£ - åè°ƒä¸¤æ­¥æµç¨‹)
            â”œâ”€â”€ KnowledgeGraphExtractor (Step 1: è·å–)
            â”‚   â””â”€â”€ extract_from_graph() - ç¢ç‰‡æ± åŒ–
            â””â”€â”€ HypothesisGenerator (Step 2: ç”Ÿæˆ)
                â””â”€â”€ batch_generate() - CoT æ¨ç† + è‡ªåŠ¨è¿‡æ»¤

    ä½¿ç”¨åœºæ™¯:
        - æ–‡çŒ®ç»¼è¿°åçš„åˆ›æ„ç”Ÿæˆ
        - ä»çŸ¥è¯†å›¾è°±å‘ç°ç ”ç©¶æœºä¼š
        - æ‰¹é‡ç”Ÿæˆå’Œç­›é€‰ç ”ç©¶å‡è®¾

    Example:
        >>> # åˆå§‹åŒ–ç”Ÿæˆå™¨
        >>> config = {'model_name': 'gpt-4o', 'max_ideas': 10}
        >>> generator = ResearchIdeaGenerator(config=config)
        >>>
        >>> # ä»çŸ¥è¯†å›¾è°±ç”Ÿæˆåˆ›æ„ï¼ˆä¸¤æ­¥æµç¨‹è‡ªåŠ¨æ‰§è¡Œï¼‰
        >>> result = generator.generate_from_knowledge_graph(
        ...     graph=citation_graph,
        ...     topic="Transformer Optimization"
        ... )
        >>>
        >>> # æŸ¥çœ‹ç»“æœ
        >>> print(f"Step 1: {result['pools']['unsolved_limitations']} limitations")
        >>> print(f"Step 1: {result['pools']['candidate_methods']} methods")
        >>> print(f"Step 2: {result['successful_ideas']} successful ideas")
    """

    def __init__(
        self,
        config: Dict = None,
        llm_client = None,
        critic_agent = None
    ):
        """
        åˆå§‹åŒ–ç ”ç©¶åˆ›æ„ç”Ÿæˆå™¨

        æ³¨æ„:
            - ä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§,ä¿ç•™äº† llm_client å’Œ critic_agent å‚æ•°
            - è¿™äº›å‚æ•°åœ¨å½“å‰å®ç°ä¸­è¢«å¿½ç•¥,å› ä¸ºä½¿ç”¨äº†æ–°çš„ HypothesisGenerator
            - å¦‚æœä½ æ˜¯æ–°ç”¨æˆ·,åªéœ€è¦ä¼ å…¥ config å‚æ•°å³å¯

        Args:
            config: é…ç½®å­—å…¸,æ”¯æŒä»¥ä¸‹é”®å€¼:
                - model_name (str): OpenAI æ¨¡å‹åç§°,é»˜è®¤ 'gpt-4o'
                  æ”¯æŒ: gpt-4o, gpt-4, gpt-3.5-turbo ç­‰
                - temperature (float): é‡‡æ ·æ¸©åº¦,é»˜è®¤ 0.3
                  èŒƒå›´: 0.0-1.0 (è¶Šä½è¶Šç¡®å®š,è¶Šé«˜è¶Šæœ‰åˆ›é€ æ€§)
                - openai_api_key (str): OpenAI API å¯†é’¥
                  å¦‚æœä¸æä¾›,å°†ä½¿ç”¨ç¯å¢ƒå˜é‡ OPENAI_API_KEY
                - openai_base_url (str): API åŸºç¡€ URL (å¯é€‰)
                  ç”¨äºä»£ç†æˆ–è‡ªå®šä¹‰ç«¯ç‚¹
                - max_ideas (int): æœ€å¤§ç”Ÿæˆåˆ›æ„æ•°é‡,é»˜è®¤ 10
            llm_client: (å·²åºŸå¼ƒ) æ—§ç‰ˆ LLM å®¢æˆ·ç«¯,ä¿ç•™ç”¨äºå‘åå…¼å®¹
            critic_agent: (å·²åºŸå¼ƒ) æ—§ç‰ˆè¯„åˆ¤ä»£ç†,ä¿ç•™ç”¨äºå‘åå…¼å®¹

        Example:
            >>> # åŸºç¡€ç”¨æ³•
            >>> config = {
            ...     'model_name': 'gpt-4o',
            ...     'temperature': 0.3,
            ...     'max_ideas': 5
            ... }
            >>> generator = ResearchIdeaGenerator(config=config)
            >>>
            >>> # ä½¿ç”¨è‡ªå®šä¹‰ API é…ç½®
            >>> config = {
            ...     'openai_api_key': 'your-api-key',
            ...     'openai_base_url': 'https://your-proxy.com/v1',
            ...     'max_ideas': 10
            ... }
            >>> generator = ResearchIdeaGenerator(config=config)
        """
        # åŠ è½½é…ç½®,å¦‚æœæœªæä¾›åˆ™ä½¿ç”¨ç©ºå­—å…¸
        self.config = config or {}

        # ===== æå– OpenAI ç›¸å…³é…ç½® =====
        # ä» config ä¸­æå–å„é¡¹é…ç½®,å¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
        # ä¼˜å…ˆä» llm èŠ‚ç‚¹è¯»å–é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»é¡¶å±‚è¯»å–
        llm_config = self.config.get('llm', {})
        model_name = llm_config.get('model', self.config.get('model_name', 'gpt-4o'))  # é»˜è®¤ä½¿ç”¨ gpt-4o
        temperature = llm_config.get('temperature', self.config.get('temperature', 0.3))  # é»˜è®¤æ¸©åº¦ 0.3
        api_key = llm_config.get('api_key') or self.config.get('openai_api_key')  # API å¯†é’¥ (å¯é€‰)
        base_url = llm_config.get('base_url') or self.config.get('openai_base_url')  # åŸºç¡€ URL (å¯é€‰)

        # å¦‚æœä»ç„¶æ²¡æœ‰ API keyï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
        if not api_key:
            import os
            api_key = os.getenv('OPENAI_API_KEY')

        # è·å– use_step_structure é…ç½®ï¼ˆç”¨äºæ¶ˆèå®éªŒï¼‰
        use_step_structure = self.config.get('use_step_structure', True)

        # ===== åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ =====
        # åˆ›å»º HypothesisGenerator å®ä¾‹,è¿™æ˜¯å®é™…æ‰§è¡Œåˆ›æ„ç”Ÿæˆçš„æ ¸å¿ƒç»„ä»¶
        self.hypothesis_generator = HypothesisGenerator(
            model_name=model_name,
            temperature=temperature,
            api_key=api_key,
            base_url=base_url,
            use_step_structure=use_step_structure
        )

        # è®¾ç½®æœ€å¤§åˆ›æ„æ•°é‡é™åˆ¶,é¿å…ç”Ÿæˆè¿‡å¤šåˆ›æ„
        # ä¼˜å…ˆä» research_idea.max_ideas è¯»å–ï¼Œå…¶æ¬¡ä»é¡¶å±‚ max_ideasï¼Œæœ€åé»˜è®¤10
        research_idea_config = self.config.get('research_idea', {})
        self.max_ideas = research_idea_config.get('max_ideas', self.config.get('max_ideas', 10))

        # åˆ›å»ºçŸ¥è¯†å›¾è°±æå–å™¨å®ä¾‹,ç”¨äºä»å›¾è°±ä¸­æå–æ•°æ®
        self.kg_extractor = KnowledgeGraphExtractor()

        logger.info(f"ResearchIdeaGenerator initialized with HypothesisGenerator (max_ideas={self.max_ideas})")

    def generate_from_knowledge_graph(
        self,
        graph: nx.Graph,
        topic: str = "",
        min_text_length: int = 50,
        verbose: bool = True
    ) -> Dict:
        """
        ä»çŸ¥è¯†å›¾è°±ç›´æ¥ç”Ÿæˆç ”ç©¶åˆ›æ„ï¼ˆä¸¤æ­¥æµç¨‹ï¼‰

        ğŸ“‹ æ•´ä½“æµç¨‹ï¼š
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Step 1: è·å– Limitation å’Œ Method                         â”‚
        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
        â”‚  è¾“å…¥ï¼šçŸ¥è¯†å›¾è°±ï¼ˆå«å¼•ç”¨å…³ç³»ç±»å‹ï¼‰                           â”‚
        â”‚  å¤„ç†ï¼šKnowledgeGraphExtractor.extract_from_graph()       â”‚
        â”‚  - Pool A: æœªè¢« Overcomes çš„ Limitation                   â”‚
        â”‚  - Pool B: è¢« Extends â‰¥2 æ¬¡çš„ Method                      â”‚
        â”‚  - Pool C: æ¥è‡ª Adapts_to çš„ Method                       â”‚
        â”‚  - Pool D: æœªè¢« Realizes çš„ Future Work                   â”‚
        â”‚  è¾“å‡ºï¼š(unsolved_limitations, candidate_methods)           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Step 2: åˆ›æ„ç”Ÿæˆï¼ˆå«è‡ªåŠ¨è¿‡æ»¤ï¼‰                            â”‚
        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
        â”‚  è¾“å…¥ï¼šLimitation Ã— Method ç¬›å¡å°”ç§¯                        â”‚
        â”‚  å¤„ç†ï¼šHypothesisGenerator.batch_generate()               â”‚
        â”‚  - å…¼å®¹æ€§åˆ†æ (Compatibility Analysis)                     â”‚
        â”‚  - å·®è·è¯†åˆ« (Gap Identification)                           â”‚
        â”‚  - åˆ›æ„è‰æ‹Ÿ (Idea Drafting)                                â”‚
        â”‚  - è‡ªåŠ¨è¿‡æ»¤ï¼šåªä¿ç•™ status="SUCCESS" çš„åˆ›æ„                â”‚
        â”‚  è¾“å‡ºï¼šé«˜è´¨é‡å¯è¡Œåˆ›æ„åˆ—è¡¨                                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        Args:
            graph: NetworkX å›¾å¯¹è±¡ï¼ŒèŠ‚ç‚¹åº”åŒ…å«è®ºæ–‡ä¿¡æ¯
                   å¿…éœ€çš„èŠ‚ç‚¹å±æ€§ï¼ˆç¢ç‰‡æ± åŒ–ï¼‰:
                   - rag_limitation (str): RAG æå–çš„å±€é™æ€§
                   - rag_future_work (str): RAG æå–çš„æœªæ¥å·¥ä½œ
                   - rag_method (str): RAG æå–çš„è´¡çŒ®/æ–¹æ³•
                   å¿…éœ€çš„è¾¹å±æ€§ï¼ˆç¢ç‰‡æ± åŒ–ï¼‰:
                   - edge_type (str): å¼•ç”¨å…³ç³»ç±»å‹ (Overcomes, Realizes, Extends, Adapts_to)
            topic: ç ”ç©¶ä¸»é¢˜ï¼Œç”¨äºè®°å½•å’Œè¾“å‡ºï¼ˆå¯é€‰ï¼‰
            min_text_length: æ–‡æœ¬æœ€å°é•¿åº¦é˜ˆå€¼ï¼Œé»˜è®¤ 50
                            ç”¨äºè¿‡æ»¤è¿‡çŸ­çš„æ–‡æœ¬ç‰‡æ®µ
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼Œé»˜è®¤ True

        Returns:
            Dict: åŒ…å«ç”Ÿæˆç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
                {
                    "topic": str,                    # ç ”ç©¶ä¸»é¢˜
                    "total_ideas": int,              # Step 2 ç”Ÿæˆçš„æ€»åˆ›æ„æ•°
                    "successful_ideas": int,         # Step 2 è¿‡æ»¤åçš„å¯è¡Œåˆ›æ„æ•°
                    "ideas": List[Dict],             # å¯è¡Œåˆ›æ„åˆ—è¡¨ï¼ˆåªå« SUCCESSï¼‰
                    "pools": {
                        "unsolved_limitations": int, # Step 1 æå–çš„å±€é™æ€§æ•°é‡
                        "candidate_methods": int     # Step 1 æå–çš„æ–¹æ³•æ•°é‡
                    }
                }

        Error Handling:
            - ç©ºå›¾è°±ï¼šè¿”å›ç©ºç»“æœå­—å…¸
            - Step 1 æ•°æ®ä¸è¶³ï¼ˆlimitations æˆ– methods ä¸ºç©ºï¼‰ï¼šè¿”å›ç©ºç»“æœå­—å…¸å¹¶è­¦å‘Š

        Example:
            >>> # åˆå§‹åŒ–ç”Ÿæˆå™¨
            >>> generator = ResearchIdeaGenerator(config={'max_ideas': 10})
            >>>
            >>> # ä»çŸ¥è¯†å›¾è°±ç”Ÿæˆåˆ›æ„ï¼ˆä¸¤æ­¥æµç¨‹è‡ªåŠ¨æ‰§è¡Œï¼‰
            >>> result = generator.generate_from_knowledge_graph(
            ...     graph=citation_graph,
            ...     topic="Transformer Optimization"
            ... )
            >>>
            >>> # æŸ¥çœ‹ç»“æœ
            >>> print(f"Step 1: æå–äº† {result['pools']['unsolved_limitations']} ä¸ªé™åˆ¶")
            >>> print(f"Step 1: æå–äº† {result['pools']['candidate_methods']} ä¸ªæ–¹æ³•")
            >>> print(f"Step 2: ç”Ÿæˆäº† {result['total_ideas']} ä¸ªå€™é€‰åˆ›æ„")
            >>> print(f"Step 2: è¿‡æ»¤åå‰©ä½™ {result['successful_ideas']} ä¸ªå¯è¡Œåˆ›æ„")
        """
        # ===== Step 1: è·å– Limitation å’Œ Methodï¼ˆç¢ç‰‡æ± åŒ–ï¼‰=====
        # ä»çŸ¥è¯†å›¾è°±ä¸­æå–é«˜è´¨é‡çš„ç ”ç©¶ç¢ç‰‡
        logger.info("ğŸ“‹ Step 1: ä»çŸ¥è¯†å›¾è°±æå– Limitation å’Œ Method")

        # æ£€æŸ¥å›¾è°±æ˜¯å¦ä¸ºç©º
        if len(graph.nodes()) == 0:
            logger.warning("Knowledge graph is empty, cannot generate ideas")
            # è¿”å›ç©ºç»“æœç»“æ„
            return {
                "topic": topic,
                "total_ideas": 0,
                "successful_ideas": 0,
                "ideas": [],
                "pools": {
                    "unsolved_limitations": 0,
                    "candidate_methods": 0
                }
            }

        # ä½¿ç”¨ KnowledgeGraphExtractor ä»å›¾è°±ä¸­æå– limitations å’Œ methods
        unsolved_limitations, candidate_methods = self.kg_extractor.extract_from_graph(
            graph, min_text_length
        )

        # éªŒè¯æ•°æ®å……åˆ†æ€§
        # éœ€è¦è‡³å°‘ 1 ä¸ª limitation å’Œ 1 ä¸ª method æ‰èƒ½è¿›è¡Œåˆ›æ„ç”Ÿæˆ
        if len(unsolved_limitations) == 0 or len(candidate_methods) == 0:
            logger.warning(
                f"Step 1 æ•°æ®ä¸è¶³: "
                f"{len(unsolved_limitations)} limitations, "
                f"{len(candidate_methods)} methods (need at least 1 of each)"
            )
            # è¿”å›ç©ºç»“æœï¼Œä½†åŒ…å«æå–çš„æ•°é‡ä¿¡æ¯
            return {
                "topic": topic,
                "total_ideas": 0,
                "successful_ideas": 0,
                "ideas": [],
                "pools": {
                    "unsolved_limitations": len(unsolved_limitations),
                    "candidate_methods": len(candidate_methods)
                }
            }

        logger.info(f"âœ… Step 1 å®Œæˆ: {len(unsolved_limitations)} limitations, {len(candidate_methods)} methods")

        # ===== Step 2: åˆ›æ„ç”Ÿæˆï¼ˆå«è‡ªåŠ¨è¿‡æ»¤ï¼‰=====
        # è°ƒç”¨åº•å±‚çš„ generate_from_pools() æ–¹æ³•
        # è¯¥æ–¹æ³•ä¼šè¿›è¡Œ limitation Ã— method çš„ç¬›å¡å°”ç§¯ç»„åˆ
        # å¹¶ä½¿ç”¨ Chain of Thought æ¨ç†ç­›é€‰å¯è¡Œçš„åˆ›æ„
        logger.info("ğŸ“‹ Step 2: åˆ›æ„ç”Ÿæˆï¼ˆLimitation Ã— Method + CoT æ¨ç† + è‡ªåŠ¨è¿‡æ»¤ï¼‰")

        return self.generate_from_pools(
            unsolved_limitations=unsolved_limitations,
            candidate_methods=candidate_methods,
            topic=topic,
            verbose=verbose
        )

    def generate_from_pools(
        self,
        unsolved_limitations: List[str],
        candidate_methods: List[str],
        topic: str = "",
        verbose: bool = True
    ) -> Dict:
        """
        ä» Limitation å’Œ Method æ± ç”Ÿæˆç ”ç©¶åˆ›æ„ï¼ˆStep 2 çš„å®ç°ï¼‰

        è¯¥æ–¹æ³•æ‰§è¡Œ Step 2 çš„å®Œæ•´æµç¨‹ï¼š
        1. Limitation Ã— Method ç¬›å¡å°”ç§¯ç»„åˆ
        2. Chain of Thought æ¨ç†ï¼ˆå…¼å®¹æ€§åˆ†æ â†’ å·®è·è¯†åˆ« â†’ åˆ›æ„è‰æ‹Ÿï¼‰
        3. è‡ªåŠ¨è¿‡æ»¤ï¼ˆåªä¿ç•™ status="SUCCESS" çš„åˆ›æ„ï¼‰

        Args:
            unsolved_limitations: Limitation åˆ—è¡¨ï¼ˆæ¥è‡ª Step 1 ç¢ç‰‡æ± åŒ–ï¼‰
            candidate_methods: Method åˆ—è¡¨ï¼ˆæ¥è‡ª Step 1 ç¢ç‰‡æ± åŒ–ï¼‰
            topic: ç ”ç©¶ä¸»é¢˜ï¼ˆå¯é€‰ï¼‰
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†è¿›åº¦æ—¥å¿—

        Returns:
            Dict: åŒ…å«ç”Ÿæˆç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
                {
                    "topic": str,
                    "total_ideas": int,              # ç”Ÿæˆçš„å¯è¡Œåˆ›æ„æ€»æ•°
                    "successful_ideas": int,         # åŒ total_ideasï¼ˆå·²è¿‡æ»¤ï¼‰
                    "ideas": List[Dict],             # åªå« SUCCESS çŠ¶æ€çš„åˆ›æ„
                    "pools": {
                        "unsolved_limitations": int,
                        "candidate_methods": int
                    }
                }
        """
        if verbose:
            logger.info(f"Generating research ideas for topic: {topic}")
            logger.info(f"Limitations pool: {len(unsolved_limitations)}")
            logger.info(f"Methods pool: {len(candidate_methods)}")

        # è°ƒç”¨ HypothesisGenerator è¿›è¡Œæ‰¹é‡ç”Ÿæˆ
        # batch_generate å†…éƒ¨ä¼šï¼š
        # 1. è¿›è¡Œ limitation Ã— method ç¬›å¡å°”ç§¯éå†
        # 2. å¯¹æ¯ä¸ªç»„åˆè°ƒç”¨ Chain of Thought æ¨ç†
        # 3. è‡ªåŠ¨è¿‡æ»¤ï¼Œåªè¿”å› status="SUCCESS" çš„åˆ›æ„
        ideas = self.hypothesis_generator.batch_generate(
            unsolved_limitations=unsolved_limitations,
            candidate_methods=candidate_methods,
            max_ideas=self.max_ideas,
            verbose=verbose
        )

        return {
            "topic": topic,
            "total_ideas": len(ideas),
            "successful_ideas": len([i for i in ideas if i["status"] == "SUCCESS"]),
            "ideas": ideas,
            "pools": {
                "unsolved_limitations": len(unsolved_limitations),
                "candidate_methods": len(candidate_methods)
            }
        }


# Convenience function for direct use
def generate_innovation_idea(
    limitation: str,
    method: str,
    model_name: str = "gpt-4o",
    temperature: float = 0.3,
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    verbose: bool = False
) -> Dict:
    """
    Convenience function to generate a single innovation idea

    Args:
        limitation: Research limitation/bottleneck description
        method: Candidate method description
        model_name: OpenAI model to use
        temperature: Sampling temperature
        api_key: OpenAI API key
        base_url: Optional API base URL
        verbose: Print detailed output

    Returns:
        Dictionary with status, title, abstract, modification, and reasoning

    Example:
        >>> idea = generate_innovation_idea(
        ...     limitation="Standard attention mechanisms have O(nÂ²) complexity",
        ...     method="FlashAttention uses tiling to reduce memory IO operations"
        ... )
        >>> print(idea["status"])  # "SUCCESS" or "INCOMPATIBLE"
        >>> print(idea["title"])
        >>> print(idea["abstract"])
    """
    generator = HypothesisGenerator(
        model_name=model_name,
        temperature=temperature,
        api_key=api_key,
        base_url=base_url
    )

    return generator.generate_innovation_idea(limitation, method, verbose=verbose)


if __name__ == "__main__":
    # Example usage
    logging.basicConfig(level=logging.INFO)

    # Example 1: Single idea generation
    print("\n" + "="*80)
    print("EXAMPLE 1: Single Idea Generation")
    print("="*80)

    limitation = "Standard attention mechanisms in transformers have quadratic computational complexity O(nÂ²) with respect to sequence length, limiting their application to long sequences."

    method = "FlashAttention uses tiling and recomputation strategies to reduce memory IO operations, achieving significant speedups while maintaining exact attention computation."

    idea = generate_innovation_idea(limitation, method, verbose=True)

    print("\nResult:")
    print(json.dumps(idea, indent=2, ensure_ascii=False))

    # Example 2: Batch generation
    print("\n\n" + "="*80)
    print("EXAMPLE 2: Batch Idea Generation")
    print("="*80)

    limitations = [
        "Current vision transformers require large amounts of training data and struggle with small datasets.",
        "Graph neural networks suffer from over-smoothing when stacking many layers.",
        "Reinforcement learning algorithms have high sample complexity in sparse reward environments."
    ]

    methods = [
        "Self-supervised learning with contrastive objectives enables learning useful representations without labels.",
        "Attention mechanisms can selectively focus on relevant parts of the input.",
        "Meta-learning algorithms can adapt quickly to new tasks with few examples."
    ]

    generator = HypothesisGenerator(model_name="gpt-4o", temperature=0.3)
    ideas = generator.batch_generate(
        unsolved_limitations=limitations,
        candidate_methods=methods,
        max_ideas=5,
        verbose=True
    )

    print(f"\n\nGenerated {len(ideas)} successful ideas:")
    for i, idea in enumerate(ideas, 1):
        print(f"\n{'='*80}")
        print(f"IDEA {i}")
        print(f"{'='*80}")
        print(f"Title: {idea['title']}")
        print(f"Abstract: {idea['abstract'][:200]}...")
        print(f"Key Modification: {idea['modification']}")
