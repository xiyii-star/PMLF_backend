"""
GROBID PDFè§£æå™¨
ä½¿ç”¨GROBIDæœåŠ¡è¿›è¡Œé«˜ç²¾åº¦çš„å­¦æœ¯è®ºæ–‡PDFè§£æ

ä¸»è¦åŠŸèƒ½ï¼š
- è¯†åˆ«æ–‡æ¡£ç»“æ„ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€ç« èŠ‚ï¼‰
- æå–å‚è€ƒæ–‡çŒ®
- å¤„ç†å¤æ‚å¸ƒå±€ï¼ˆå¤šåˆ—ã€å›¾è¡¨ã€å…¬å¼ï¼‰
- è¾“å‡ºç»“æ„åŒ–ç« èŠ‚ä¿¡æ¯
"""

import logging
import requests
import xml.etree.ElementTree as ET
from typing import List, Optional, Dict
from pathlib import Path
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class PaperSection:
    """è®ºæ–‡ç« èŠ‚æ•°æ®ç»“æ„"""
    title: str
    content: str
    page_num: int
    section_type: str


class GrobidPDFParser:
    """
    GROBID PDFè§£æå™¨

    ä½¿ç”¨GROBIDæœåŠ¡å°†PDFè½¬æ¢ä¸ºç»“æ„åŒ–çš„TEI XMLï¼Œ
    ç„¶åæå–ç« èŠ‚ä¿¡æ¯
    """

    def __init__(self, grobid_url: str = "http://localhost:8070"):
        """
        åˆå§‹åŒ–GROBIDè§£æå™¨

        Args:
            grobid_url: GROBIDæœåŠ¡åœ°å€
        """
        self.grobid_url = grobid_url.rstrip('/')
        self.tei_ns = {'tei': 'http://www.tei-c.org/ns/1.0'}
        self.timeout = 60  # APIè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        self._check_service()

    def _check_service(self) -> bool:
        """æ£€æŸ¥GROBIDæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(
                f"{self.grobid_url}/api/isalive",
                timeout=5
            )

            if response.status_code == 200 and response.text.strip().lower() == 'true':
                logger.info(f"âœ… GROBIDæœåŠ¡å¯ç”¨: {self.grobid_url}")
                return True
            else:
                logger.warning(f"âš ï¸ GROBIDæœåŠ¡å“åº”å¼‚å¸¸: {response.text}")
                return False

        except requests.exceptions.RequestException as e:
            logger.warning(f"âš ï¸ æ— æ³•è¿æ¥åˆ°GROBIDæœåŠ¡: {e}")
            logger.info(f"   è¯·ç¡®ä¿GROBIDæœåŠ¡è¿è¡Œåœ¨: {self.grobid_url}")
            logger.info(f"   å¯åŠ¨æ–¹æ³•: docker run -d -p 8070:8070 lfoppiano/grobid:0.8.0")
            return False

    def extract_sections_from_pdf(self, pdf_path: str) -> List[PaperSection]:
        """
        ä»PDFä¸­æå–ç« èŠ‚ï¼ˆä½¿ç”¨GROBIDï¼‰

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            ç« èŠ‚åˆ—è¡¨
        """
        if not Path(pdf_path).exists():
            logger.error(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            return []

        try:
            logger.info(f"  ğŸ“„ ä½¿ç”¨GROBIDè§£æPDF: {Path(pdf_path).name}")

            # 1. è°ƒç”¨GROBID API
            tei_xml = self._call_grobid_api(pdf_path)

            if not tei_xml:
                return []

            # 2. è§£æTEI XML
            sections = self._parse_tei_xml(tei_xml)

            logger.info(f"  âœ… GROBIDæˆåŠŸæå– {len(sections)} ä¸ªç« èŠ‚")
            return sections

        except Exception as e:
            logger.error(f"  âŒ GROBIDè§£æå¤±è´¥: {e}")
            return []

    def _call_grobid_api(self, pdf_path: str) -> Optional[str]:
        """
        è°ƒç”¨GROBID APIå¤„ç†PDF

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            TEI XMLå­—ç¬¦ä¸²
        """
        try:
            with open(pdf_path, 'rb') as f:
                files = {'input': f}

                # è°ƒç”¨processFulltextDocument API
                response = requests.post(
                    f"{self.grobid_url}/api/processFulltextDocument",
                    files=files,
                    timeout=self.timeout
                )

            if response.status_code != 200:
                logger.error(f"  GROBID APIè¿”å›é”™è¯¯: {response.status_code}")
                return None

            return response.text

        except requests.exceptions.Timeout:
            logger.error(f"  GROBID APIè¶…æ—¶ï¼ˆ{self.timeout}ç§’ï¼‰")
            return None
        except Exception as e:
            logger.error(f"  GROBID APIè°ƒç”¨å¤±è´¥: {e}")
            return None

    def _parse_tei_xml(self, tei_xml: str) -> List[PaperSection]:
        """
        è§£æGROBIDè¿”å›çš„TEI XML

        Args:
            tei_xml: TEI XMLå­—ç¬¦ä¸²

        Returns:
            ç« èŠ‚åˆ—è¡¨
        """
        try:
            root = ET.fromstring(tei_xml)
            sections = []

            # 1. æå–æ ‡é¢˜
            title_elem = root.find('.//tei:titleStmt/tei:title[@type="main"]', self.tei_ns)
            if title_elem is not None and title_elem.text:
                sections.append(PaperSection(
                    title='Title',
                    content=self._extract_text(title_elem),
                    page_num=0,
                    section_type='title'
                ))

            # 2. æå–æ‘˜è¦
            abstract = root.find('.//tei:abstract', self.tei_ns)
            if abstract is not None:
                content = self._extract_text(abstract)
                if content.strip():
                    sections.append(PaperSection(
                        title='Abstract',
                        content=content,
                        page_num=0,
                        section_type='abstract'
                    ))

            # 3. æå–æ­£æ–‡ç« èŠ‚
            body = root.find('.//tei:body', self.tei_ns)
            if body is not None:
                sections.extend(self._parse_body_sections(body))

            # 4. æå–ç»“è®ºï¼ˆå¦‚æœåœ¨bodyå¤–ï¼‰
            back = root.find('.//tei:back', self.tei_ns)
            if back is not None:
                # æœ‰äº›è®ºæ–‡çš„conclusionåœ¨backéƒ¨åˆ†
                for div in back.findall('.//tei:div', self.tei_ns):
                    head = div.find('tei:head', self.tei_ns)
                    if head is not None and head.text:
                        section_title = self._extract_text(head)

                        # æå–æ®µè½å†…å®¹
                        paragraphs = div.findall('.//tei:p', self.tei_ns)
                        content = '\n\n'.join([
                            self._extract_text(p) for p in paragraphs if self._extract_text(p).strip()
                        ])

                        if content.strip():
                            section_type = self._infer_section_type(section_title)
                            sections.append(PaperSection(
                                title=section_title,
                                content=content,
                                page_num=0,
                                section_type=section_type
                            ))

            return sections

        except ET.ParseError as e:
            logger.error(f"  TEI XMLè§£æå¤±è´¥: {e}")
            return []
        except Exception as e:
            logger.error(f"  TEIå¤„ç†å¤±è´¥: {e}")
            return []

    def _parse_body_sections(self, body: ET.Element) -> List[PaperSection]:
        """
        è§£æbodyéƒ¨åˆ†çš„ç« èŠ‚

        Args:
            body: TEI bodyå…ƒç´ 

        Returns:
            ç« èŠ‚åˆ—è¡¨
        """
        sections = []

        # éå†æ‰€æœ‰divå…ƒç´ ï¼ˆç« èŠ‚ï¼‰
        for div in body.findall('.//tei:div', self.tei_ns):
            # è·å–ç« èŠ‚æ ‡é¢˜
            head = div.find('tei:head', self.tei_ns)
            section_title = self._extract_text(head) if head is not None else 'Unknown Section'

            # åªæå–ç›´æ¥å­divçš„æ®µè½ï¼Œé¿å…åµŒå¥—é‡å¤
            # ä½¿ç”¨XPathçš„é™åˆ¶ï¼šåªæ‰¾å½“å‰divä¸‹çš„pï¼Œä¸é€’å½’
            paragraphs = []
            for child in div:
                if child.tag == f'{{{self.tei_ns["tei"]}}}p':
                    text = self._extract_text(child)
                    if text.strip():
                        paragraphs.append(text)

            # å¦‚æœæ²¡æœ‰ç›´æ¥æ®µè½ï¼Œå¯èƒ½æ˜¯æœ‰å­ç« èŠ‚ï¼Œé€’å½’æå–
            if not paragraphs:
                sub_divs = div.findall('tei:div', self.tei_ns)
                if sub_divs:
                    # æœ‰å­ç« èŠ‚ï¼Œé€’å½’å¤„ç†
                    for sub_div in sub_divs:
                        sub_head = sub_div.find('tei:head', self.tei_ns)
                        sub_title = self._extract_text(sub_head) if sub_head is not None else section_title

                        sub_paragraphs = sub_div.findall('.//tei:p', self.tei_ns)
                        sub_content = '\n\n'.join([
                            self._extract_text(p) for p in sub_paragraphs if self._extract_text(p).strip()
                        ])

                        if sub_content.strip():
                            sub_type = self._infer_section_type(sub_title)
                            sections.append(PaperSection(
                                title=sub_title,
                                content=sub_content,
                                page_num=0,
                                section_type=sub_type
                            ))
                    continue

            # æ„å»ºå†…å®¹
            content = '\n\n'.join(paragraphs)

            if content.strip():
                section_type = self._infer_section_type(section_title)
                sections.append(PaperSection(
                    title=section_title,
                    content=content,
                    page_num=0,
                    section_type=section_type
                ))

        return sections

    def _extract_text(self, element: Optional[ET.Element]) -> str:
        """
        é€’å½’æå–å…ƒç´ ä¸­çš„æ‰€æœ‰æ–‡æœ¬

        Args:
            element: XMLå…ƒç´ 

        Returns:
            æå–çš„æ–‡æœ¬
        """
        if element is None:
            return ""

        # ä½¿ç”¨itertext()è·å–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
        text_parts = []
        for text in element.itertext():
            text_parts.append(text.strip())

        return ' '.join(text_parts).strip()

    def _infer_section_type(self, title: str) -> str:
        """
        æ¨æ–­ç« èŠ‚ç±»å‹

        Args:
            title: ç« èŠ‚æ ‡é¢˜

        Returns:
            ç« èŠ‚ç±»å‹
        """
        title_lower = title.lower().strip()

        # ç§»é™¤ç¼–å·ï¼ˆå¦‚ "1.", "1.1", "I.", "A.", etc.ï¼‰
        # ä¿®å¤: åªåŒ¹é…å¼€å¤´çš„ç¼–å·éƒ¨åˆ†,ä¸è¦åŒ¹é…å•è¯ä¸­çš„å­—æ¯
        import re
        # åŒ¹é…: æ•°å­—ç¼–å·ã€ç½—é©¬æ•°å­—(åé¢å¿…é¡»è·Ÿ.)ã€å­—æ¯ç¼–å·(åé¢å¿…é¡»è·Ÿ.)
        title_clean = re.sub(r'^(?:\d+\.)*\d+\s+|^[IVXLCDM]+\.\s+|^[A-Z]\.\s+', '', title_lower, flags=re.IGNORECASE).strip()

        # åŒ¹é…ç« èŠ‚ç±»å‹
        if 'abstract' in title_clean:
            return 'abstract'
        elif 'introduction' in title_clean:
            return 'introduction'
        elif any(kw in title_clean for kw in ['related work', 'background', 'literature review', 'prior work']):
            return 'related_work'
        elif any(kw in title_clean for kw in ['method', 'approach', 'model', 'architecture', 'algorithm']):
            return 'method'
        elif any(kw in title_clean for kw in ['experiment', 'evaluation', 'result', 'performance']):
            return 'experiment'
        elif 'discussion' in title_clean:
            return 'discussion'
        elif any(kw in title_clean for kw in ['conclusion', 'summary']):
            return 'conclusion'
        elif any(kw in title_clean for kw in ['limitation', 'weakness']):
            return 'limitation'
        elif any(kw in title_clean for kw in ['future work', 'future direction', 'future research']):
            return 'future_work'
        else:
            return 'other'

    def extract_metadata(self, pdf_path: str) -> Dict:
        """
        æå–è®ºæ–‡å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ç­‰ï¼‰

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            å…ƒæ•°æ®å­—å…¸
        """
        try:
            tei_xml = self._call_grobid_api(pdf_path)
            if not tei_xml:
                return {}

            root = ET.fromstring(tei_xml)
            metadata = {}

            # æ ‡é¢˜
            title_elem = root.find('.//tei:titleStmt/tei:title[@type="main"]', self.tei_ns)
            if title_elem is not None:
                metadata['title'] = self._extract_text(title_elem)

            # ä½œè€…
            authors = []
            for author in root.findall('.//tei:sourceDesc//tei:author', self.tei_ns):
                persName = author.find('.//tei:persName', self.tei_ns)
                if persName is not None:
                    forename = persName.find('tei:forename', self.tei_ns)
                    surname = persName.find('tei:surname', self.tei_ns)

                    name_parts = []
                    if forename is not None and forename.text:
                        name_parts.append(forename.text)
                    if surname is not None and surname.text:
                        name_parts.append(surname.text)

                    if name_parts:
                        authors.append(' '.join(name_parts))

            if authors:
                metadata['authors'] = authors

            # æ‘˜è¦
            abstract = root.find('.//tei:abstract', self.tei_ns)
            if abstract is not None:
                metadata['abstract'] = self._extract_text(abstract)

            return metadata

        except Exception as e:
            logger.error(f"å…ƒæ•°æ®æå–å¤±è´¥: {e}")
            return {}


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    parser = GrobidPDFParser()

    # æµ‹è¯•PDFè§£æ
    test_pdf = "./data/papers/sample.pdf"
    if Path(test_pdf).exists():
        sections = parser.extract_sections_from_pdf(test_pdf)

        print(f"\næå–åˆ° {len(sections)} ä¸ªç« èŠ‚:\n")
        for i, section in enumerate(sections, 1):
            print(f"{i}. [{section.section_type}] {section.title}")
            print(f"   å†…å®¹é•¿åº¦: {len(section.content)} å­—ç¬¦")
            print(f"   å†…å®¹é¢„è§ˆ: {section.content[:100]}...")
            print()
    else:
        print(f"æµ‹è¯•PDFä¸å­˜åœ¨: {test_pdf}")
