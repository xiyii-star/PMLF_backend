"""
æ·±åº¦ç»¼è¿°ç”Ÿæˆæ¨¡å—
Deep Survey Analyzer

æ ¸å¿ƒæ–¹æ³•è®ºï¼šåŸºäºå…³ç³»çš„å›¾è°±å‰ªæ + å…³é”®æ¼”åŒ–è·¯å¾„è¯†åˆ«

è´Ÿè´£åŸºäºçŸ¥è¯†å›¾è°±ç”Ÿæˆæ·±åº¦å­¦æœ¯ç»¼è¿°ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºäºå…³ç³»çš„å›¾è°±å‰ªæ (Relation-Based Graph Pruning) - è§£å†³"æ•°æ®å™ªéŸ³"
   - ä¿ç•™ Seed Papers
   - åªä¿ç•™é€šè¿‡å¼ºé€»è¾‘å…³ç³»ï¼ˆOvercomes, Realizes, Extendsï¼‰ä¸ Seed è¿é€šçš„è®ºæ–‡
   - å‰”é™¤ä»…ç”± Baselines è¿æ¥æˆ–å­¤ç«‹çš„è®ºæ–‡
2. å…³é”®æ¼”åŒ–è·¯å¾„è¯†åˆ« (Critical Evolutionary Paths) - è§£å†³"ç¢ç‰‡åŒ–"
   - è¯†åˆ«çº¿æ€§é“¾æ¡ (The Chain): A -> Overcomes -> B -> Extends -> C
   - è¯†åˆ«åˆ†åŒ–æ¨¡å¼ (The Divergence): Seed -> [Multiple Routes]
   - è¯†åˆ«æ±‡èšæ¨¡å¼ (The Convergence): [Multiple Sources] -> Integration Point
   - ä¸ºæ¯ä¸ªæ¼”åŒ–è·¯å¾„ç”Ÿæˆå™äº‹å•å…ƒ
3. ç»“æ„åŒ– Deep Survey æŠ¥å‘Š (Structured Survey Report)
   - Thread å½¢å¼å±•ç¤ºå„ä¸ªæ¼”åŒ–æ•…äº‹
   - é…åˆå¯è§†åŒ–å›¾å’Œæ–‡å­—è§£è¯»
"""

import logging
from typing import Dict, List, Optional, Tuple, Set
from datetime import datetime

try:
    import networkx as nx
except ImportError:
    raise ImportError("éœ€è¦å®‰è£…networkx: pip install networkx")

logger = logging.getLogger(__name__)


class DeepSurveyAnalyzer:
    """
    æ·±åº¦ç»¼è¿°åˆ†æå™¨

    åŸºäºçŸ¥è¯†å›¾è°±ç”Ÿæˆæ·±åº¦å­¦æœ¯ç»¼è¿°
    é‡‡ç”¨å…³ç³»å‰ªæ + æ¼”åŒ–è·¯å¾„è¯†åˆ«æ–¹æ³•è®º
    """

    def __init__(self, config: Dict = None):
        """
        åˆå§‹åŒ–æ·±åº¦ç»¼è¿°åˆ†æå™¨

        Args:
            config: é…ç½®å‚æ•°å­—å…¸ï¼Œæ”¯æŒ:
                - llm_config_path: LLMé…ç½®æ–‡ä»¶è·¯å¾„
                - strong_relations: å¼ºé€»è¾‘å…³ç³»ç±»å‹åˆ—è¡¨
                - weak_relations: å¼±å…³ç³»ç±»å‹åˆ—è¡¨
                - min_chain_length: æœ€å°é“¾æ¡é•¿åº¦
                - max_threads: æœ€å¤§æ¼”åŒ–æ•…äº‹æ•°é‡
                - pruning_mode: å‰ªææ¨¡å¼ ('seed_centric' æˆ– 'comprehensive')
                - min_component_size: æœ€å°è¿é€šåˆ†é‡å¤§å°ï¼ˆç”¨äºè¿‡æ»¤å™ªéŸ³ç°‡ï¼‰
        """
        self.config = config or {}

        # å®šä¹‰å¼ºé€»è¾‘å…³ç³»ï¼ˆç”¨äºå‰ªæä¿ç•™ï¼‰- åŸºäºSocket Matchingçš„6ç§å…³ç³»ç±»å‹
        self.strong_relations = self.config.get('strong_relations', [
            'Overcomes',   # æ”»å…‹/ä¼˜åŒ–ï¼ˆçºµå‘æ·±åŒ–ï¼‰- Match 1: Limitationâ†’Problem
            'Realizes',    # å®ç°æ„¿æ™¯ï¼ˆç§‘ç ”ä¼ æ‰¿ï¼‰- Match 2: Future_Workâ†’Problem
            'Extends',     # æ–¹æ³•æ‰©å±•ï¼ˆå¾®åˆ›æ–°ï¼‰- Match 3: Method Extension
            'Alternative', # å¦è¾Ÿè¹Šå¾„ï¼ˆé¢ è¦†åˆ›æ–°ï¼‰- Match 3: Alternative Route
            'Adapts_to'    # æŠ€æœ¯è¿ç§»ï¼ˆæ¨ªå‘æ‰©æ•£ï¼‰- Match 4: Problemâ†’Problemè·¨åŸŸ
        ])

        # å®šä¹‰å¼±å…³ç³»ï¼ˆç”¨äºå‰ªæåˆ é™¤ï¼‰
        self.weak_relations = self.config.get('weak_relations', [
            'Baselines'    # åŸºçº¿å¯¹æ¯”ï¼ˆèƒŒæ™¯å™ªéŸ³ï¼‰- æ— åŒ¹é…
        ])

        # å‰ªææ¨¡å¼é…ç½®
        self.pruning_mode = self.config.get('pruning_mode', 'comprehensive')
        # 'seed_centric': ä»…ä¿ç•™ä¸Seedè¿é€šçš„å¼ºå…³ç³»å­å›¾ï¼ˆåŸå®ç°ï¼‰
        # 'comprehensive': ä¿ç•™æ‰€æœ‰å¼ºå…³ç³»è¿é€šåˆ†é‡ï¼ˆæ–°å®ç°ï¼‰

        # æœ€å°è¿é€šåˆ†é‡å¤§å°ï¼ˆç”¨äºè¿‡æ»¤å™ªéŸ³ç°‡ï¼‰
        self.min_component_size = self.config.get('min_component_size', 3)

        # æ¼”åŒ–è·¯å¾„æ¢ç´¢æ·±åº¦ï¼ˆç”¨äºåˆ†åŒ–å’Œæ±‡èšæ¨¡å¼ï¼‰
        self.exploration_depth = self.config.get('exploration_depth', 5)

        # å­˜å‚¨è¿é€šåˆ†é‡å…ƒæ•°æ®ï¼ˆç”¨äºåç»­å»é‡ï¼‰
        self.strong_components = []

        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ï¼ˆç”¨äºç”Ÿæˆå™äº‹æ–‡æœ¬ï¼‰
        self.llm_client = None
        llm_config_path = self.config.get('llm_config_path')
        if llm_config_path:
            try:
                from llm_config import LLMClient, LLMConfig
                llm_config = LLMConfig.from_file(llm_config_path)
                self.llm_client = LLMClient(llm_config)
                logger.info("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæ–‡æœ¬")

        logger.info("DeepSurveyAnalyzer åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  å‰ªææ¨¡å¼: {self.pruning_mode}")
        logger.info(f"  å¼ºé€»è¾‘å…³ç³»: {self.strong_relations}")
        logger.info(f"  å¼±å…³ç³»: {self.weak_relations}")
        logger.info(f"  æœ€å°è¿é€šåˆ†é‡å¤§å°: {self.min_component_size}")
        logger.info(f"  æ¼”åŒ–è·¯å¾„æ¢ç´¢æ·±åº¦: {self.exploration_depth}")

    def analyze(self, graph: nx.DiGraph, topic: str) -> Dict:
        """
        æ‰§è¡Œæ·±åº¦ç»¼è¿°åˆ†æ

        Args:
            graph: çŸ¥è¯†å›¾è°±ï¼ˆNetworkXæœ‰å‘å›¾ï¼‰
            topic: ç ”ç©¶ä¸»é¢˜

        Returns:
            æ·±åº¦ç»¼è¿°åˆ†æç»“æœ
        """
        logger.info(f"å¼€å§‹æ·±åº¦ç»¼è¿°åˆ†æ: {topic}")
        logger.info(f"  åŸå§‹å›¾è°±: {len(graph.nodes())} ä¸ªèŠ‚ç‚¹, {len(graph.edges())} æ¡è¾¹")

        if len(graph.nodes()) == 0:
            logger.warning("çŸ¥è¯†å›¾è°±ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆæ·±åº¦ç»¼è¿°")
            return self._empty_result(topic)

        # ========== ç¬¬ä¸€æ­¥ï¼šåŸºäºå…³ç³»çš„å›¾è°±å‰ªæ (Relation-Based Pruning) ==========
        logger.info("\n" + "="*60)
        logger.info("æ­¥éª¤1: åŸºäºå…³ç³»çš„å›¾è°±å‰ªæ (Relation-Based Pruning)")
        logger.info("="*60)
        pruned_graph, pruning_stats = self._prune_graph_by_relations(graph)
        logger.info(f"  å‰ªæå: {len(pruned_graph.nodes())} ä¸ªèŠ‚ç‚¹ (ä¿ç•™ç‡: {len(pruned_graph.nodes())/len(graph.nodes())*100:.1f}%)")
        logger.info(f"  å‰ªæå: {len(pruned_graph.edges())} æ¡è¾¹ (ä¿ç•™ç‡: {len(pruned_graph.edges())/len(graph.edges())*100:.1f}%)")

        if len(pruned_graph.nodes()) == 0:
            logger.warning("å‰ªæåå›¾è°±ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆç»¼è¿°")
            return self._empty_result(topic)

        # ========== ç¬¬äºŒæ­¥ï¼šå…³é”®æ¼”åŒ–è·¯å¾„è¯†åˆ« (Critical Evolutionary Paths) ==========
        logger.info("\n" + "="*60)
        logger.info("æ­¥éª¤2: å…³é”®æ¼”åŒ–è·¯å¾„è¯†åˆ« (Critical Evolutionary Paths)")
        logger.info("="*60)
        evolutionary_paths = self._identify_evolutionary_paths(pruned_graph)
        logger.info(f"  è¯†åˆ«å‡º {len(evolutionary_paths)} æ¡æ¼”åŒ–è·¯å¾„")
        for i, path in enumerate(evolutionary_paths, 1):
            logger.info(f"    Thread {i}: {path['pattern_type']} - {len(path['papers'])} ç¯‡è®ºæ–‡")

        # ========== ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç»“æ„åŒ– Deep Survey æŠ¥å‘Š ==========
        logger.info("\n" + "="*60)
        logger.info("æ­¥éª¤3: ç”Ÿæˆç»“æ„åŒ– Deep Survey æŠ¥å‘Š")
        logger.info("="*60)
        survey_report = self._generate_survey_report(
            topic=topic,
            pruned_graph=pruned_graph,
            evolutionary_paths=evolutionary_paths,
            pruning_stats=pruning_stats
        )

        result = {
            'topic': topic,
            'timestamp': datetime.now().isoformat(),
            'pruning_stats': pruning_stats,
            'evolutionary_paths': evolutionary_paths,
            'survey_report': survey_report,
            'summary': {
                'original_papers': len(graph.nodes()),
                'pruned_papers': len(pruned_graph.nodes()),
                'total_threads': len(evolutionary_paths),
            }
        }

        logger.info("\næ·±åº¦ç»¼è¿°åˆ†æå®Œæˆ âœ…")
        return result

    def _empty_result(self, topic: str) -> Dict:
        """è¿”å›ç©ºç»“æœ"""
        return {
            'topic': topic,
            'timestamp': datetime.now().isoformat(),
            'pruning_stats': {},
            'evolutionary_paths': [],
            'survey_report': {},
            'summary': {
                'original_papers': 0,
                'pruned_papers': 0,
                'total_threads': 0,
            }
        }

    def _prune_graph_by_relations(self, graph: nx.DiGraph) -> Tuple[nx.DiGraph, Dict]:
        """
        ç¬¬ä¸€æ­¥ï¼šåŸºäºå…³ç³»çš„å›¾è°±å‰ªæ

        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        - comprehensive: ä¿ç•™æ‰€æœ‰å¼ºå…³ç³»è¿é€šåˆ†é‡
        - seed_centric: åªä¿ç•™ä¸Seedè¿é€šçš„è®ºæ–‡ï¼ˆåŸå®ç°ï¼‰

        Args:
            graph: åŸå§‹çŸ¥è¯†å›¾è°±

        Returns:
            (å‰ªæåçš„å›¾è°±, ç»Ÿè®¡ä¿¡æ¯)
        """
        logger.info("  æ­£åœ¨æ‰§è¡Œå›¾è°±å‰ªæ...")
        logger.info(f"    å‰ªææ¨¡å¼: {self.pruning_mode}")

        # åˆ›å»ºæ–°å›¾ï¼ˆä¿ç•™åŸå›¾ç»“æ„ï¼‰
        pruned_graph = nx.DiGraph()

        # Step 1: è¯†åˆ«æ‰€æœ‰ Seed Papersï¼ˆä¸¤ç§æ¨¡å¼éƒ½éœ€è¦ï¼‰
        seed_papers = set()
        for node_id in graph.nodes():
            node_data = graph.nodes[node_id]
            if node_data.get('is_seed', False):
                seed_papers.add(node_id)

        logger.info(f"    è¯†åˆ«åˆ° {len(seed_papers)} ä¸ª Seed Papers")

        # Step 2: æ ¹æ®å‰ªææ¨¡å¼ç¡®å®šè¦ä¿ç•™çš„è®ºæ–‡
        if self.pruning_mode == 'comprehensive':
            # æ–°æ¨¡å¼ï¼šä¿ç•™æ‰€æœ‰å¼ºå…³ç³»è¿é€šåˆ†é‡
            papers_to_keep = self._find_all_strong_components(graph)
        else:
            # åŸæ¨¡å¼ï¼šåªä¿ç•™ä¸Seedè¿é€šçš„è®ºæ–‡
            if len(seed_papers) == 0:
                logger.warning("    æœªæ‰¾åˆ° Seed Papersï¼Œå°†ä½¿ç”¨å‰5ä¸ªè®ºæ–‡èŠ‚ç‚¹ä½œä¸ºç§å­")
                # é™çº§ç­–ç•¥ï¼šé€‰æ‹©å‰5ä¸ªè®ºæ–‡èŠ‚ç‚¹ä½œä¸ºç§å­
                all_nodes = list(graph.nodes())
                seed_papers = set(all_nodes[:5])
                logger.info(f"    é™çº§ç­–ç•¥ï¼šé€‰æ‹©äº† {len(seed_papers)} ä¸ªè®ºæ–‡èŠ‚ç‚¹ä½œä¸ºç§å­")

            papers_to_keep = self._find_seed_connected_papers(graph, seed_papers)

        # Step 3: æ„å»ºå‰ªæåçš„å­å›¾
        for node_id in papers_to_keep:
            node_data = graph.nodes[node_id]
            pruned_graph.add_node(node_id, **node_data)

        # Step 4: æ·»åŠ è¾¹ï¼ˆåªä¿ç•™å¼ºå…³ç³»è¾¹ï¼‰å¹¶ç»Ÿè®¡å…³ç³»ç±»å‹åˆ†å¸ƒ
        strong_edges = 0
        weak_edges_removed = 0
        relation_type_count = {}  # ç»Ÿè®¡å„ç§å…³ç³»ç±»å‹çš„æ•°é‡

        for u, v, edge_data in graph.edges(data=True):
            if u in papers_to_keep and v in papers_to_keep:
                edge_type = edge_data.get('type') or edge_data.get('edge_type', '')

                # ç»Ÿè®¡å…³ç³»ç±»å‹
                if edge_type:
                    relation_type_count[edge_type] = relation_type_count.get(edge_type, 0) + 1

                if edge_type in self.strong_relations or edge_type == '':
                    # ä¿ç•™å¼ºå…³ç³»è¾¹ï¼ˆç©ºç±»å‹é»˜è®¤ä¿ç•™ï¼‰
                    pruned_graph.add_edge(u, v, **edge_data)
                    strong_edges += 1
                else:
                    weak_edges_removed += 1

        # ç»Ÿè®¡ä¿¡æ¯
        pruning_stats = {
            'original_papers': len(graph.nodes()),
            'pruned_papers': len(pruned_graph.nodes()),
            'removed_papers': len(graph.nodes()) - len(pruned_graph.nodes()),
            'pruning_mode': self.pruning_mode,

            # æ–°å¢ï¼šè¿é€šåˆ†é‡ç»Ÿè®¡
            'strong_components_count': len(self.strong_components) if hasattr(self, 'strong_components') else 0,
            'components_with_seed': sum(1 for c in getattr(self, 'strong_components', []) if c['has_seed']),
            'largest_component_size': max((c['size'] for c in getattr(self, 'strong_components', [])), default=0),

            # åŸæœ‰ç»Ÿè®¡
            'seed_papers': len(seed_papers),
            'original_edges': len(graph.edges()),
            'strong_edges': strong_edges,
            'weak_edges_removed': weak_edges_removed,
            'retention_rate': len(pruned_graph.nodes()) / len(graph.nodes()) if len(graph.nodes()) > 0 else 0,
            'relation_type_distribution': relation_type_count
        }

        logger.info(f"    âœ… å‰ªæå®Œæˆ:")
        logger.info(f"       - ä¿ç•™è®ºæ–‡: {pruning_stats['pruned_papers']} / {pruning_stats['original_papers']}")
        logger.info(f"       - å‰”é™¤è®ºæ–‡: {pruning_stats['removed_papers']}")
        logger.info(f"       - ä¿ç•™å¼ºå…³ç³»è¾¹: {strong_edges}")
        logger.info(f"       - å‰”é™¤å¼±å…³ç³»è¾¹: {weak_edges_removed}")

        # è¾“å‡ºè¿é€šåˆ†é‡ç»Ÿè®¡ï¼ˆä»…comprehensiveæ¨¡å¼ï¼‰
        if self.pruning_mode == 'comprehensive' and hasattr(self, 'strong_components'):
            logger.info(f"    ğŸ“Š å¼ºå…³ç³»è¿é€šåˆ†é‡ç»Ÿè®¡:")
            logger.info(f"       - è¿é€šåˆ†é‡æ€»æ•°: {len(self.strong_components)}")
            logger.info(f"       - åŒ…å«ç§å­çš„åˆ†é‡: {sum(1 for c in self.strong_components if c['has_seed'])}")
            logger.info(f"       - æœ€å¤§åˆ†é‡å¤§å°: {max((c['size'] for c in self.strong_components), default=0)}")

            # åˆ—å‡ºå‰5ä¸ªæœ€å¤§çš„è¿é€šåˆ†é‡
            sorted_components = sorted(self.strong_components, key=lambda x: x['size'], reverse=True)
            for i, comp in enumerate(sorted_components[:5], 1):
                logger.info(f"       - åˆ†é‡{i}: {comp['size']}ç¯‡è®ºæ–‡, æ€»å¼•ç”¨{comp['total_citations']}")

        # è¾“å‡ºå…³ç³»ç±»å‹åˆ†å¸ƒ
        if relation_type_count:
            logger.info(f"    ğŸ“Š å…³ç³»ç±»å‹åˆ†å¸ƒ:")
            for rel_type, count in sorted(relation_type_count.items(), key=lambda x: x[1], reverse=True):
                percentage = count / sum(relation_type_count.values()) * 100
                logger.info(f"       - {rel_type}: {count} ({percentage:.1f}%)")

        return pruned_graph, pruning_stats

    def _bfs_strong_relations(
        self,
        graph: nx.DiGraph,
        start_node: str,
        direction: str = 'forward'
    ) -> Set[str]:
        """
        ä½¿ç”¨ BFS ä»èµ·å§‹èŠ‚ç‚¹å‡ºå‘ï¼Œæ²¿å¼ºé€»è¾‘å…³ç³»éå†å›¾

        Args:
            graph: å›¾è°±
            start_node: èµ·å§‹èŠ‚ç‚¹
            direction: 'forward' (åç»§) æˆ– 'backward' (å‰é©±)

        Returns:
            å¯è¾¾èŠ‚ç‚¹é›†åˆ
        """
        visited = set()
        queue = [start_node]

        while queue:
            current = queue.pop(0)
            if current in visited:
                continue
            visited.add(current)

            # è·å–é‚»å±…èŠ‚ç‚¹
            if direction == 'forward':
                neighbors = graph.successors(current)
            else:
                neighbors = graph.predecessors(current)

            for neighbor in neighbors:
                if neighbor in visited:
                    continue

                # æ£€æŸ¥è¾¹çš„ç±»å‹
                if direction == 'forward':
                    edge_data = graph.edges[current, neighbor]
                else:
                    edge_data = graph.edges[neighbor, current]

                edge_type = edge_data.get('type') or edge_data.get('edge_type', '')

                # åªæ²¿å¼ºå…³ç³»è¾¹éå†
                if edge_type in self.strong_relations or edge_type == '':
                    queue.append(neighbor)

        return visited

    def _bfs_strong_relations_bidirectional(
        self,
        graph: nx.DiGraph,
        start_node: str
    ) -> Set[str]:
        """
        ä»èµ·å§‹èŠ‚ç‚¹åŒå‘BFSï¼Œæ‰¾åˆ°æ‰€æœ‰å¼ºå…³ç³»è¿é€šçš„èŠ‚ç‚¹

        Args:
            graph: å›¾è°±
            start_node: èµ·å§‹èŠ‚ç‚¹

        Returns:
            å¯è¾¾èŠ‚ç‚¹é›†åˆï¼ˆæ­£å‘+åå‘çš„å¹¶é›†ï¼‰
        """
        # æ­£å‘éå†ï¼šæ‰¾åˆ°åç»§èŠ‚ç‚¹
        forward = self._bfs_strong_relations(graph, start_node, 'forward')
        # åå‘éå†ï¼šæ‰¾åˆ°å‰é©±èŠ‚ç‚¹
        backward = self._bfs_strong_relations(graph, start_node, 'backward')

        # è¿”å›ä¸¤è€…çš„å¹¶é›†
        return forward | backward

    def _find_all_strong_components(self, graph: nx.DiGraph) -> Set[str]:
        """
        æ‰¾åˆ°æ‰€æœ‰å¼ºå…³ç³»è¿é€šåˆ†é‡

        ç­–ç•¥ï¼š
        1. éå†æ‰€æœ‰èŠ‚ç‚¹ï¼Œä½¿ç”¨åŒå‘BFSæ‰¾åˆ°å¼ºå…³ç³»è¿é€šåˆ†é‡
        2. åªä¿ç•™å¤§å° >= min_component_size çš„è¿é€šåˆ†é‡
        3. è®°å½•æ¯ä¸ªè¿é€šåˆ†é‡çš„å…ƒæ•°æ®ï¼ˆç”¨äºåç»­å»é‡ï¼‰

        Args:
            graph: åŸå§‹å›¾è°±

        Returns:
            æ‰€æœ‰åº”ä¿ç•™çš„è®ºæ–‡èŠ‚ç‚¹é›†åˆ
        """
        visited = set()
        papers_to_keep = set()
        self.strong_components = []  # æ¸…ç©ºå¹¶é‡æ–°è®°å½•

        for node in graph.nodes():
            if node in visited:
                continue

            # åŒå‘BFSæ‰¾åˆ°è¯¥èŠ‚ç‚¹çš„å¼ºå…³ç³»è¿é€šåˆ†é‡
            component = self._bfs_strong_relations_bidirectional(graph, node)
            visited.update(component)

            # è¿‡æ»¤å°ç°‡ï¼ˆå¯èƒ½æ˜¯å™ªéŸ³ï¼‰
            if len(component) >= self.min_component_size:
                papers_to_keep.update(component)

                # è®°å½•è¿é€šåˆ†é‡å…ƒæ•°æ®
                total_citations = sum(
                    graph.nodes[p].get('cited_by_count', 0)
                    for p in component
                )

                has_seed = any(
                    graph.nodes[p].get('is_seed', False)
                    for p in component
                )

                self.strong_components.append({
                    'papers': component,
                    'size': len(component),
                    'total_citations': total_citations,
                    'has_seed': has_seed
                })

        logger.info(f"    è¯†åˆ«åˆ° {len(self.strong_components)} ä¸ªå¼ºå…³ç³»è¿é€šåˆ†é‡")
        logger.info(f"    ä¿ç•™ {len(papers_to_keep)} ç¯‡è®ºæ–‡ï¼ˆè¿é€šåˆ†é‡å¤§å° >= {self.min_component_size}ï¼‰")

        return papers_to_keep

    def _find_seed_connected_papers(
        self,
        graph: nx.DiGraph,
        seed_papers: Set[str]
    ) -> Set[str]:
        """
        åŸæœ‰é€»è¾‘ï¼šåªä¿ç•™ä¸ç§å­è¿é€šçš„è®ºæ–‡ï¼ˆç”¨äº seed_centric æ¨¡å¼ï¼‰

        Args:
            graph: å›¾è°±
            seed_papers: ç§å­è®ºæ–‡é›†åˆ

        Returns:
            ä¸ç§å­è¿é€šçš„è®ºæ–‡é›†åˆ
        """
        papers_to_keep = set(seed_papers)

        # æ­£å‘éå†ï¼šSeed -> åç»­è®ºæ–‡ï¼ˆé€šè¿‡å¼ºå…³ç³»ï¼‰
        for seed in seed_papers:
            reachable_forward = self._bfs_strong_relations(
                graph, seed, direction='forward'
            )
            papers_to_keep.update(reachable_forward)

        # åå‘éå†ï¼šSeed <- å‰é©±è®ºæ–‡ï¼ˆé€šè¿‡å¼ºå…³ç³»ï¼‰
        for seed in seed_papers:
            reachable_backward = self._bfs_strong_relations(
                graph, seed, direction='backward'
            )
            papers_to_keep.update(reachable_backward)

        logger.info(f"    é€šè¿‡å¼ºå…³ç³»è¿é€šæ€§åˆ†æï¼Œä¿ç•™ {len(papers_to_keep)} ç¯‡è®ºæ–‡")

        return papers_to_keep

    def _lightweight_explore_chains(
        self,
        graph: nx.DiGraph,
        start_node: str,
        scope: Set[str],
        max_depth: int = 5
    ) -> Set[str]:
        """
        è½»é‡çº§é“¾æ¡æ¢ç´¢ï¼ˆç”¨äºé¢„è¯„ä¼°ï¼‰

        æ²¿å¼ºå…³ç³»è¾¹è¿›è¡ŒBFSï¼Œé™åˆ¶æ·±åº¦

        Args:
            graph: å›¾è°±
            start_node: èµ·å§‹èŠ‚ç‚¹
            scope: å…è®¸çš„èŠ‚ç‚¹èŒƒå›´
            max_depth: æœ€å¤§æ¢ç´¢æ·±åº¦

        Returns:
            å¯è¾¾èŠ‚ç‚¹é›†åˆ
        """
        visited = set([start_node])
        current_layer = {start_node}

        for _ in range(max_depth):
            next_layer = set()
            for node in current_layer:
                # æ¢ç´¢åç»§èŠ‚ç‚¹
                for successor in graph.successors(node):
                    if successor not in scope or successor in visited:
                        continue
                    edge_data = graph.edges[node, successor]
                    edge_type = edge_data.get('type') or edge_data.get('edge_type', '')
                    if edge_type in self.strong_relations or edge_type == '':
                        next_layer.add(successor)
                        visited.add(successor)

            if not next_layer:
                break
            current_layer = next_layer

        return visited

    def _lightweight_explore_divergence(
        self,
        graph: nx.DiGraph,
        center_node: str,
        scope: Set[str],
        max_depth: int = 5
    ) -> Set[str]:
        """
        è½»é‡çº§åˆ†åŒ–æ¢ç´¢ï¼ˆç”¨äºé¢„è¯„ä¼°ï¼‰

        ä»ä¸­å¿ƒèŠ‚ç‚¹å‘predecessorsæ–¹å‘æ¢ç´¢

        Args:
            graph: å›¾è°±
            center_node: ä¸­å¿ƒèŠ‚ç‚¹
            scope: å…è®¸çš„èŠ‚ç‚¹èŒƒå›´
            max_depth: æœ€å¤§æ¢ç´¢æ·±åº¦

        Returns:
            å¯è¾¾èŠ‚ç‚¹é›†åˆ
        """
        visited = set([center_node])

        # è·å–æ‰€æœ‰é€šè¿‡å¼ºå…³ç³»å¼•ç”¨ä¸­å¿ƒèŠ‚ç‚¹çš„å‰é©±
        predecessors = [
            p for p in graph.predecessors(center_node)
            if p in scope
        ]

        for predecessor in predecessors:
            edge_data = graph.edges[predecessor, center_node]
            edge_type = edge_data.get('type') or edge_data.get('edge_type', '')

            if edge_type not in self.strong_relations:
                continue

            visited.add(predecessor)

            # ç»§ç»­å‘åæ¢ç´¢ï¼ˆé™åˆ¶æ·±åº¦ï¼‰
            current = predecessor
            for _ in range(max_depth - 1):
                next_predecessors = [
                    np for np in graph.predecessors(current)
                    if np in scope and np not in visited
                ]

                if not next_predecessors:
                    break

                # é€‰æ‹©å¼ºå…³ç³»å‰é©±
                valid_predecessors = [
                    np for np in next_predecessors
                    if (graph.edges[np, current].get('type') or
                        graph.edges[np, current].get('edge_type', '')) in self.strong_relations
                ]

                if not valid_predecessors:
                    break

                # é€‰æ‹©å¼•ç”¨æ•°æœ€é«˜çš„
                next_node = max(
                    valid_predecessors,
                    key=lambda x: graph.nodes[x].get('cited_by_count', 0)
                )
                visited.add(next_node)
                current = next_node

        return visited

    def _lightweight_explore_convergence(
        self,
        graph: nx.DiGraph,
        center_node: str,
        scope: Set[str],
        max_depth: int = 5
    ) -> Set[str]:
        """
        è½»é‡çº§æ±‡èšæ¢ç´¢ï¼ˆç”¨äºé¢„è¯„ä¼°ï¼‰

        ä»ä¸­å¿ƒèŠ‚ç‚¹å‘successorsæ–¹å‘æ¢ç´¢

        Args:
            graph: å›¾è°±
            center_node: ä¸­å¿ƒèŠ‚ç‚¹
            scope: å…è®¸çš„èŠ‚ç‚¹èŒƒå›´
            max_depth: æœ€å¤§æ¢ç´¢æ·±åº¦

        Returns:
            å¯è¾¾èŠ‚ç‚¹é›†åˆ
        """
        visited = set([center_node])

        # è·å–æ‰€æœ‰è¢«ä¸­å¿ƒèŠ‚ç‚¹é€šè¿‡å¼ºå…³ç³»å¼•ç”¨çš„åç»§
        successors = [
            s for s in graph.successors(center_node)
            if s in scope
        ]

        for successor in successors:
            edge_data = graph.edges[center_node, successor]
            edge_type = edge_data.get('type') or edge_data.get('edge_type', '')

            if edge_type not in self.strong_relations:
                continue

            visited.add(successor)

            # ç»§ç»­å‘åæ¢ç´¢ï¼ˆé™åˆ¶æ·±åº¦ï¼‰
            current = successor
            for _ in range(max_depth - 1):
                next_successors = [
                    ns for ns in graph.successors(current)
                    if ns in scope and ns not in visited
                ]

                if not next_successors:
                    break

                # é€‰æ‹©å¼ºå…³ç³»åç»§
                valid_successors = [
                    ns for ns in next_successors
                    if (graph.edges[current, ns].get('type') or
                        graph.edges[current, ns].get('edge_type', '')) in self.strong_relations
                ]

                if not valid_successors:
                    break

                # é€‰æ‹©å¼•ç”¨æ•°æœ€é«˜çš„
                next_node = max(
                    valid_successors,
                    key=lambda x: graph.nodes[x].get('cited_by_count', 0)
                )
                visited.add(next_node)
                current = next_node

        return visited

    def _pre_evaluate_node_coverage(
        self,
        graph: nx.DiGraph,
        node_id: str,
        scope: Set[str]
    ) -> int:
        """
        é¢„è¯„ä¼°èŠ‚ç‚¹èƒ½è¦†ç›–çš„è®ºæ–‡æ•°

        ç»¼åˆè€ƒè™‘é“¾æ¡ã€åˆ†åŒ–ã€æ±‡èšä¸‰ä¸ªæ–¹å‘

        Args:
            graph: å›¾è°±
            node_id: å€™é€‰èŠ‚ç‚¹ID
            scope: å…è®¸çš„èŠ‚ç‚¹èŒƒå›´ï¼ˆè¿é€šåˆ†é‡ï¼‰

        Returns:
            é¢„è®¡èƒ½è¦†ç›–çš„è®ºæ–‡æ€»æ•°
        """
        all_papers = set()

        # 1. é“¾æ¡æ–¹å‘è¦†ç›–
        chain_papers = self._lightweight_explore_chains(
            graph, node_id, scope, self.exploration_depth
        )
        all_papers.update(chain_papers)

        # 2. åˆ†åŒ–æ–¹å‘è¦†ç›–
        divergence_papers = self._lightweight_explore_divergence(
            graph, node_id, scope, self.exploration_depth
        )
        all_papers.update(divergence_papers)

        # 3. æ±‡èšæ–¹å‘è¦†ç›–
        convergence_papers = self._lightweight_explore_convergence(
            graph, node_id, scope, self.exploration_depth
        )
        all_papers.update(convergence_papers)

        return len(all_papers)

    def _find_key_nodes_in_component(
        self,
        graph: nx.DiGraph,
        component_papers: Set[str]
    ) -> List[str]:
        """
        æ‰¾åˆ°è¿é€šåˆ†é‡çš„å…³é”®èŠ‚ç‚¹

        ä¼˜å…ˆçº§ï¼š
        1. ç§å­èŠ‚ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰
        2. é¢„è¯„ä¼°é€‰æ‹©è¦†ç›–ç‡æœ€é«˜çš„å‰3ä¸ªèŠ‚ç‚¹

        Args:
            graph: å›¾è°±
            component_papers: è¿é€šåˆ†é‡çš„è®ºæ–‡é›†åˆ

        Returns:
            å…³é”®èŠ‚ç‚¹IDåˆ—è¡¨
        """
        key_nodes = []

        # ä¼˜å…ˆé€‰æ‹©ç§å­èŠ‚ç‚¹
        seed_nodes = [
            node_id for node_id in component_papers
            if graph.nodes[node_id].get('is_seed', False)
        ]
        key_nodes.extend(seed_nodes)

        # å¦‚æœæ²¡æœ‰ç§å­ï¼Œä½¿ç”¨é¢„è¯„ä¼°æœºåˆ¶
        if len(key_nodes) == 0:
            component_size = len(component_papers)

            # æ€§èƒ½ä¼˜åŒ–ï¼šå¤§è¿é€šåˆ†é‡ï¼ˆ>50èŠ‚ç‚¹ï¼‰ä½¿ç”¨é™çº§ç­–ç•¥
            if component_size > 50:
                logger.info(f"    è¿é€šåˆ†é‡è¿‡å¤§({component_size}èŠ‚ç‚¹)ï¼Œä½¿ç”¨åº¦ä¸­å¿ƒæ€§é™çº§ç­–ç•¥")
                subgraph = graph.subgraph(component_papers)
                centrality = nx.degree_centrality(subgraph)
                top_nodes = sorted(
                    centrality.items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:3]
                key_nodes = [node_id for node_id, _ in top_nodes]
            else:
                # é¢„è¯„ä¼°ç­–ç•¥ï¼šè®¡ç®—åº¦ä¸­å¿ƒæ€§å‰10ä¸ªå€™é€‰èŠ‚ç‚¹çš„è¦†ç›–ç‡
                subgraph = graph.subgraph(component_papers)
                centrality = nx.degree_centrality(subgraph)
                top_candidates = sorted(
                    centrality.items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:10]  # å€™é€‰æ± ï¼šå‰10ä¸ª

                # é¢„è¯„ä¼°æ¯ä¸ªå€™é€‰èŠ‚ç‚¹çš„è®ºæ–‡è¦†ç›–æ•°
                candidate_scores = []
                for node_id, _ in top_candidates:
                    coverage = self._pre_evaluate_node_coverage(
                        graph, node_id, component_papers
                    )
                    candidate_scores.append((node_id, coverage))
                    logger.info(f"      é¢„è¯„ä¼°èŠ‚ç‚¹ {node_id[:20]}...: é¢„è®¡è¦†ç›– {coverage} ç¯‡è®ºæ–‡")

                # æŒ‰è¦†ç›–æ•°æ’åºï¼Œé€‰æ‹©å‰3ä¸ª
                candidate_scores.sort(key=lambda x: x[1], reverse=True)
                key_nodes = [node_id for node_id, _ in candidate_scores[:3]]

                logger.info(f"    é¢„è¯„ä¼°é€‰æ‹©å®Œæˆï¼Œé€‰å‡º3ä¸ªæœ€ä¼˜èŠ‚ç‚¹")

        return key_nodes

    def _find_linear_chains_in_scope(
        self,
        graph: nx.DiGraph,
        start_node: str,
        scope: Set[str]
    ) -> List[List[str]]:
        """
        åœ¨æŒ‡å®šèŒƒå›´å†…è¯†åˆ«çº¿æ€§é“¾æ¡ï¼ˆé¿å…è·¨è¿é€šåˆ†é‡è¯†åˆ«ï¼‰

        Args:
            graph: å›¾è°±
            start_node: èµ·å§‹èŠ‚ç‚¹
            scope: å…è®¸çš„èŠ‚ç‚¹èŒƒå›´ï¼ˆè¿é€šåˆ†é‡å†…çš„èŠ‚ç‚¹ï¼‰

        Returns:
            é“¾æ¡åˆ—è¡¨ï¼Œæ¯ä¸ªé“¾æ¡æ˜¯èŠ‚ç‚¹IDåˆ—è¡¨
        """
        chains = []
        min_chain_length = self.config.get('min_chain_length', 3)

        def dfs_chain(current_path: List[str]):
            """DFS æœç´¢é“¾æ¡ï¼ˆé™åˆ¶åœ¨scopeå†…ï¼‰"""
            current = current_path[-1]
            successors = list(graph.successors(current))

            if len(successors) == 0:
                # åˆ°è¾¾ç»ˆç‚¹
                if len(current_path) >= min_chain_length:
                    chains.append(current_path.copy())
                return

            # ä¼˜å…ˆæ²¿ç€å¼ºå…³ç³»ç»§ç»­
            for successor in successors:
                # å…³é”®ä¿®æ”¹ï¼šåªè€ƒè™‘scopeå†…çš„èŠ‚ç‚¹
                if successor not in scope:
                    continue
                if successor in current_path:  # é¿å…ç¯
                    continue

                edge_data = graph.edges[current, successor]
                edge_type = edge_data.get('type') or edge_data.get('edge_type', '')

                if edge_type in self.strong_relations or edge_type == '':
                    current_path.append(successor)
                    dfs_chain(current_path)
                    current_path.pop()

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¼ºå…³ç³»ï¼Œè®°å½•å½“å‰é“¾æ¡
            if len(current_path) >= min_chain_length:
                chains.append(current_path.copy())

        dfs_chain([start_node])

        return chains

    def _find_divergence_pattern_in_scope(
        self,
        graph: nx.DiGraph,
        center_node: str,
        scope: Set[str]
    ) -> Optional[Dict]:
        """
        åœ¨æŒ‡å®šèŒƒå›´å†…è¯†åˆ«åˆ†åŒ–ç»“æ„ï¼ˆé¿å…è·¨è¿é€šåˆ†é‡è¯†åˆ«ï¼‰

        åˆ†åŒ–å®šä¹‰ï¼šä¸€ä¸ªä¸­å¿ƒèŠ‚ç‚¹è¢«å¤šç¯‡åç»­è®ºæ–‡å¼•ç”¨/æ‰©å±•
        æ–¹å‘ï¼šä¸­å¿ƒèŠ‚ç‚¹ -> å¤šæ¡åˆ†æ”¯ï¼ˆå‘predecessorsæ¢ç´¢ï¼‰

        Args:
            graph: å›¾è°±
            center_node: ä¸­å¿ƒèŠ‚ç‚¹
            scope: å…è®¸çš„èŠ‚ç‚¹èŒƒå›´ï¼ˆè¿é€šåˆ†é‡å†…çš„èŠ‚ç‚¹ï¼‰

        Returns:
            åˆ†åŒ–ç»“æ„å­—å…¸ï¼ŒåŒ…å«ä¸­å¿ƒèŠ‚ç‚¹å’Œå„æ¡è·¯çº¿
        """
        # ä½¿ç”¨ predecessors æ‰¾å¼•ç”¨ä¸­å¿ƒèŠ‚ç‚¹çš„è®ºæ–‡ï¼ˆBug Fix #6ï¼‰
        predecessors = list(graph.predecessors(center_node))

        # å…³é”®ä¿®æ”¹ï¼šåªè€ƒè™‘scopeå†…çš„å‰é©±èŠ‚ç‚¹
        predecessors = [p for p in predecessors if p in scope]

        if len(predecessors) < 2:
            return None

        # å¯¹æ¯ä¸ªå‰é©±èŠ‚ç‚¹ï¼Œè¯†åˆ«å…¶æ¼”åŒ–è·¯çº¿
        routes = []
        for predecessor in predecessors:
            edge_data = graph.edges[predecessor, center_node]
            edge_type = edge_data.get('type') or edge_data.get('edge_type', '')

            # åªä¿ç•™å¼ºå…³ç³»è·¯çº¿
            if edge_type not in self.strong_relations:
                continue

            # åˆå§‹åŒ–è·¯çº¿
            route = {
                'relation_type': edge_type,
                'papers': [predecessor]
            }

            # ç»§ç»­å‘åæ¢ç´¢ï¼ˆæœ€å¤šexploration_depthå±‚ï¼‰- æ‰¾æ›´æ–°çš„è®ºæ–‡
            current = predecessor
            for _ in range(self.exploration_depth):
                next_predecessors = list(graph.predecessors(current))
                # å…³é”®ä¿®æ”¹ï¼šåªè€ƒè™‘scopeå†…çš„èŠ‚ç‚¹
                next_predecessors = [np for np in next_predecessors if np in scope]

                if len(next_predecessors) == 0:
                    break

                # é€‰æ‹©å¼•ç”¨æ•°æœ€é«˜çš„å‰é©±ï¼ˆä¸”æ˜¯å¼ºå…³ç³»ï¼‰
                valid_predecessors = []
                for np in next_predecessors:
                    edge_data = graph.edges[np, current]
                    if (edge_data.get('type') or edge_data.get('edge_type', '')) in self.strong_relations:
                        valid_predecessors.append(np)

                if not valid_predecessors:
                    break

                next_node = max(
                    valid_predecessors,
                    key=lambda x: graph.nodes[x].get('cited_by_count', 0)
                )
                if next_node in route['papers']:
                    break
                route['papers'].append(next_node)
                current = next_node

            routes.append(route)

        # è‡³å°‘æœ‰2æ¡æœ‰æ•ˆè·¯çº¿æ‰ç®—åˆ†åŒ–
        if len(routes) < 2:
            return None

        return {
            'center': center_node,
            'routes': routes
        }

    def _identify_paths_in_component(
        self,
        graph: nx.DiGraph,
        component: Dict
    ) -> List[Dict]:
        """
        ä¸ºä¸€ä¸ªè¿é€šåˆ†é‡è¯†åˆ«çº¿æ€§é“¾æ¡å’Œåˆ†åŒ–ç»“æ„

        ç­–ç•¥ï¼š
        1. æ‰¾åˆ°è¿é€šåˆ†é‡çš„"å…³é”®èŠ‚ç‚¹"ï¼ˆé«˜ä¸­å¿ƒæ€§æˆ–ç§å­èŠ‚ç‚¹ï¼‰
        2. ä»å…³é”®èŠ‚ç‚¹è¯†åˆ«é“¾æ¡å’Œåˆ†åŒ–
        3. æ ‡è®°è¿é€šåˆ†é‡IDï¼Œç”¨äºå»é‡

        Args:
            graph: å›¾è°±
            component: è¿é€šåˆ†é‡å­—å…¸ï¼ˆåŒ…å«papers, size, total_citationsç­‰ï¼‰

        Returns:
            æ¼”åŒ–è·¯å¾„åˆ—è¡¨
        """
        paths = []
        component_papers = component['papers']

        # æ‰¾åˆ°è¿é€šåˆ†é‡çš„å…³é”®èŠ‚ç‚¹
        key_nodes = self._find_key_nodes_in_component(graph, component_papers)

        for node_id in key_nodes:
            node_data = graph.nodes[node_id]

            # è¯†åˆ«çº¿æ€§é“¾æ¡ï¼ˆé™åˆ¶åœ¨å½“å‰è¿é€šåˆ†é‡å†…ï¼‰
            chains = self._find_linear_chains_in_scope(
                graph,
                node_id,
                component_papers
            )
            for chain in chains:
                path = self._create_chain_narrative(graph, chain, node_data)
                path['component_id'] = id(component)  # æ ‡è®°æ‰€å±è¿é€šåˆ†é‡
                paths.append(path)

            # è¯†åˆ«åˆ†åŒ–ç»“æ„ï¼ˆé™åˆ¶åœ¨å½“å‰è¿é€šåˆ†é‡å†…ï¼‰
            divergence = self._find_divergence_pattern_in_scope(
                graph,
                node_id,
                component_papers
            )
            if divergence and len(divergence['routes']) > 1:
                path = self._create_divergence_narrative(graph, divergence, node_data)
                path['component_id'] = id(component)
                paths.append(path)

            # è¯†åˆ«æ±‡èšç»“æ„ï¼ˆé™åˆ¶åœ¨å½“å‰è¿é€šåˆ†é‡å†…ï¼‰
            convergence = self._find_convergence_pattern_in_scope(
                graph,
                node_id,
                component_papers
            )
            if convergence and len(convergence['routes']) > 1:
                path = self._create_convergence_narrative(graph, convergence, node_data)
                path['component_id'] = id(component)
                paths.append(path)
                logger.info(f"      è¯†åˆ«åˆ°æ±‡èšç»“æ„: {len(convergence['routes'])} æ¡è·¯çº¿æ±‡èš")

        return paths

    def _calculate_path_priority(self, path: Dict) -> float:
        """
        è®¡ç®—è·¯å¾„ä¼˜å…ˆçº§

        è¯„åˆ†ç»´åº¦ï¼š
        1. è®ºæ–‡æ•°é‡ï¼ˆæƒé‡ï¼š30%ï¼‰
        2. æ€»å¼•ç”¨æ•°ï¼ˆæƒé‡ï¼š30%ï¼‰
        3. è·¯å¾„ç±»å‹å¤šæ ·æ€§ï¼ˆæƒé‡ï¼š20%ï¼‰
        4. å…³é”®å…³ç³»ç±»å‹ï¼ˆæƒé‡ï¼š20%ï¼‰

        Args:
            path: æ¼”åŒ–è·¯å¾„

        Returns:
            ä¼˜å…ˆçº§åˆ†æ•°
        """
        # 1. è®ºæ–‡æ•°é‡ï¼ˆæƒé‡ï¼š30%ï¼‰
        paper_count_score = len(path['papers']) * 10

        # 2. æ€»å¼•ç”¨æ•°ï¼ˆæƒé‡ï¼š30%ï¼‰
        citation_score = path.get('total_citations', 0) / 100  # å½’ä¸€åŒ–

        # 3. è·¯å¾„ç±»å‹å¤šæ ·æ€§ï¼ˆæƒé‡ï¼š20%ï¼‰
        diversity_score = 0
        if path['thread_type'] in ['divergence', 'convergence']:
            # åˆ†åŒ–/æ±‡èšç»“æ„ï¼šå…³ç³»ç±»å‹è¶Šå¤šæ ·ï¼Œåˆ†æ•°è¶Šé«˜
            route_types = set(r['relation_type'] for r in path.get('routes', []))
            diversity_score = len(route_types) * 20
            # ç‰¹åˆ«åŠ åˆ†ï¼šåŒæ—¶åŒ…å« Alternative å’Œ Extends
            if 'Alternative' in route_types and 'Extends' in route_types:
                diversity_score += 30
        else:
            # é“¾æ¡ç»“æ„ï¼šé•¿åº¦æœ¬èº«å°±ä»£è¡¨æ¼”åŒ–æ·±åº¦
            diversity_score = len(path['papers']) * 5

        # 4. æ˜¯å¦åŒ…å«å…³é”®å…³ç³»ï¼ˆæƒé‡ï¼š20%ï¼‰
        key_relation_score = 0
        if path['thread_type'] in ['divergence', 'convergence']:
            route_types = set(r['relation_type'] for r in path.get('routes', []))
            # Overcomes å’Œ Alternative æ˜¯æœ€æœ‰ä»·å€¼çš„å…³ç³»
            if 'Overcomes' in route_types:
                key_relation_score += 25
            if 'Alternative' in route_types:
                key_relation_score += 25
            if 'Realizes' in route_types:
                key_relation_score += 15

        return paper_count_score + citation_score + diversity_score + key_relation_score

    def _identify_evolutionary_paths(self, graph: nx.DiGraph) -> List[Dict]:
        """
        ç¬¬äºŒæ­¥ï¼šå…³é”®æ¼”åŒ–è·¯å¾„è¯†åˆ«

        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        - comprehensive: ä¸ºæ¯ä¸ªè¿é€šåˆ†é‡è¯†åˆ«è·¯å¾„
        - seed_centric: åŸºäºç§å­èŠ‚ç‚¹è¯†åˆ«è·¯å¾„ï¼ˆåŸå®ç°ï¼‰

        è¯†åˆ«ä¸¤ç§æ ¸å¿ƒæ¼”åŒ–æ¨¡å¼ï¼š
        1. çº¿æ€§é“¾æ¡ (The Chain): A -> Overcomes -> B -> Extends -> C
        2. åˆ†åŒ–æ¨¡å¼ (The Divergence): Seed -> [Multiple Routes]

        Args:
            graph: å‰ªæåçš„å›¾è°±

        Returns:
            æ¼”åŒ–è·¯å¾„åˆ—è¡¨
        """
        logger.info("  æ­£åœ¨è¯†åˆ«æ¼”åŒ–è·¯å¾„...")

        evolutionary_paths = []

        if self.pruning_mode == 'comprehensive':
            # æ–°ç­–ç•¥ï¼šä¸ºæ¯ä¸ªè¿é€šåˆ†é‡è¯†åˆ«è·¯å¾„
            logger.info(f"    comprehensiveæ¨¡å¼ï¼šä¸º {len(self.strong_components)} ä¸ªè¿é€šåˆ†é‡è¯†åˆ«è·¯å¾„")

            for component in self.strong_components:
                component_paths = self._identify_paths_in_component(
                    graph,
                    component
                )
                evolutionary_paths.extend(component_paths)

        else:
            # åŸç­–ç•¥ï¼šåŸºäºç§å­èŠ‚ç‚¹è¯†åˆ«
            # è¯†åˆ« Seed Papers
            seed_papers = [node_id for node_id in graph.nodes()
                          if graph.nodes[node_id].get('is_seed', False)]

            if len(seed_papers) == 0:
                logger.warning("    æœªæ‰¾åˆ° Seed Papersï¼Œä½¿ç”¨é«˜ä¸­å¿ƒæ€§èŠ‚ç‚¹")
                # é™çº§ï¼šä½¿ç”¨åº¦ä¸­å¿ƒæ€§
                centrality = nx.degree_centrality(graph)
                top_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:5]
                seed_papers = [node_id for node_id, _ in top_nodes]

            logger.info(f"    seed_centricæ¨¡å¼ï¼šåŸºäº {len(seed_papers)} ä¸ªç§å­èŠ‚ç‚¹è¯†åˆ«æ¼”åŒ–è·¯å¾„")

            # ä¸ºæ¯ä¸ª Seed è¯†åˆ«æ¼”åŒ–è·¯å¾„
            for seed_id in seed_papers:
                seed_data = graph.nodes[seed_id]

                # æ¨¡å¼1: è¯†åˆ«ä»è¯¥ Seed å‡ºå‘çš„çº¿æ€§é“¾æ¡
                chains = self._find_linear_chains(graph, seed_id)
                for chain in chains:
                    path = self._create_chain_narrative(graph, chain, seed_data)
                    evolutionary_paths.append(path)

                # æ¨¡å¼2: è¯†åˆ«ä»¥è¯¥ Seed ä¸ºä¸­å¿ƒçš„åˆ†åŒ–ç»“æ„
                divergence = self._find_divergence_pattern(graph, seed_id)
                if divergence and len(divergence['routes']) > 1:  # è‡³å°‘æœ‰2æ¡åˆ†æ”¯æ‰ç®—åˆ†åŒ–
                    path = self._create_divergence_narrative(graph, divergence, seed_data)
                    evolutionary_paths.append(path)

                # æ¨¡å¼3: è¯†åˆ«ä»¥è¯¥ Seed ä¸ºä¸­å¿ƒçš„æ±‡èšç»“æ„
                convergence = self._find_convergence_pattern(graph, seed_id)
                if convergence and len(convergence['routes']) > 1:  # è‡³å°‘æœ‰2æ¡è·¯çº¿æ‰ç®—æ±‡èš
                    path = self._create_convergence_narrative(graph, convergence, seed_data)
                    evolutionary_paths.append(path)
                    logger.info(f"    Seed {seed_id[:20]}... å½¢æˆæ±‡èšç»“æ„")

        # æŒ‰é‡è¦æ€§æ’åº
        evolutionary_paths.sort(key=self._calculate_path_priority, reverse=True)

        # å¢å¼ºçš„å»é‡æœºåˆ¶ï¼ˆä½¿ç”¨æ–°å®ç°ï¼‰
        evolutionary_paths = self._deduplicate_paths_enhanced(evolutionary_paths)

        # åªä¿ç•™å‰Nä¸ªæœ€é‡è¦çš„æ•…äº‹
        max_threads = self.config.get('max_threads', 5)
        evolutionary_paths = evolutionary_paths[:max_threads]

        logger.info(f"    âœ… è¯†åˆ«å®Œæˆ: {len(evolutionary_paths)} æ¡æ¼”åŒ–è·¯å¾„")

        return evolutionary_paths

    def _calculate_dedup_threshold(self, path1: Dict, path2: Dict) -> float:
        """
        åŠ¨æ€è®¡ç®—å»é‡é˜ˆå€¼

        è§„åˆ™ï¼š
        1. åŒä¸€è¿é€šåˆ†é‡å†…çš„è·¯å¾„ï¼šé«˜é˜ˆå€¼ï¼ˆ0.8ï¼‰
        2. ä¸åŒè¿é€šåˆ†é‡çš„è·¯å¾„ï¼šä¸­ç­‰é˜ˆå€¼ï¼ˆ0.6ï¼‰
        3. ä¸åŒç±»å‹çš„è·¯å¾„ï¼ˆchain vs starï¼‰ï¼šä½é˜ˆå€¼ï¼ˆ0.5ï¼‰

        Args:
            path1: ç¬¬ä¸€æ¡è·¯å¾„
            path2: ç¬¬äºŒæ¡è·¯å¾„

        Returns:
            å»é‡é˜ˆå€¼
        """
        base_threshold = self.config.get('path_overlap_threshold', 0.8)

        # å¦‚æœæ¥è‡ªä¸åŒè¿é€šåˆ†é‡ï¼Œé™ä½é˜ˆå€¼
        if path1.get('component_id') != path2.get('component_id'):
            base_threshold = 0.6

        # å¦‚æœç±»å‹ä¸åŒï¼Œè¿›ä¸€æ­¥é™ä½é˜ˆå€¼
        if path1.get('thread_type') != path2.get('thread_type'):
            base_threshold = min(base_threshold, 0.5)

        return base_threshold

    def _are_semantically_different(self, path1: Dict, path2: Dict) -> bool:
        """
        åˆ¤æ–­ä¸¤æ¡è·¯å¾„æ˜¯å¦è¯­ä¹‰ä¸åŒï¼ˆå³ä½¿è®ºæ–‡é‡å ï¼‰

        è€ƒè™‘å› ç´ ï¼š
        1. å…³ç³»ç±»å‹å·®å¼‚ï¼šOvercomes vs Alternative
        2. è®ºæ–‡è§’è‰²å·®å¼‚ï¼šåŒä¸€è®ºæ–‡åœ¨ä¸åŒè·¯å¾„ä¸­çš„è§’è‰²
        3. æ¼”åŒ–æ–¹å‘å·®å¼‚ï¼šæ—¶é—´é¡ºåºä¸åŒ

        Args:
            path1: ç¬¬ä¸€æ¡è·¯å¾„
            path2: ç¬¬äºŒæ¡è·¯å¾„

        Returns:
            Trueè¡¨ç¤ºè¯­ä¹‰ä¸åŒï¼Œåº”ä¿ç•™ä¸¤è€…
        """
        # 1. æ£€æŸ¥ä¸»è¦å…³ç³»ç±»å‹æ˜¯å¦ä¸åŒï¼ˆåˆ†åŒ–/æ±‡èšç»“æ„ï¼‰
        if path1.get('thread_type') in ['divergence', 'convergence'] and \
           path2.get('thread_type') in ['divergence', 'convergence']:
            routes1 = set(r['relation_type'] for r in path1.get('routes', []))
            routes2 = set(r['relation_type'] for r in path2.get('routes', []))

            # å¦‚æœåˆ†åŒ–/æ±‡èšç»“æ„çš„å…³ç³»ç±»å‹å®Œå…¨ä¸åŒï¼Œè®¤ä¸ºè¯­ä¹‰ä¸åŒ
            if len(routes1 & routes2) == 0:
                return True

        # 2. æ£€æŸ¥ä¸­å¿ƒèŠ‚ç‚¹æ˜¯å¦ä¸åŒï¼ˆåˆ†åŒ–/æ±‡èšç»“æ„ï¼‰
        if path1.get('thread_type') in ['divergence', 'convergence'] and \
           path2.get('thread_type') in ['divergence', 'convergence']:
            papers1 = path1['papers']
            papers2 = path2['papers']

            center1 = next((p for p in papers1 if p.get('role') == 'center'), None)
            center2 = next((p for p in papers2 if p.get('role') == 'center'), None)

            if center1 and center2 and center1['paper_id'] != center2['paper_id']:
                return True

        # 3. æ£€æŸ¥å…³ç³»é“¾çš„å·®å¼‚ï¼ˆé“¾æ¡ç»“æ„ï¼‰
        if path1.get('thread_type') == 'chain' and path2.get('thread_type') == 'chain':
            chain1 = path1.get('relation_chain', [])
            chain2 = path2.get('relation_chain', [])

            if len(chain1) > 0 and len(chain2) > 0:
                # å¦‚æœå…³ç³»é“¾çš„ä¸»è¦å…³ç³»ç±»å‹ä¸åŒï¼Œè®¤ä¸ºè¯­ä¹‰ä¸åŒ
                types1 = set(r['relation_type'] for r in chain1)
                types2 = set(r['relation_type'] for r in chain2)

                max_len = max(len(types1), len(types2))
                if max_len > 0 and len(types1 & types2) / max_len < 0.5:
                    return True

        return False

    def _deduplicate_paths_enhanced(self, paths: List[Dict]) -> List[Dict]:
        """
        å¢å¼ºçš„è·¯å¾„å»é‡æœºåˆ¶

        å¤šå±‚å»é‡ç­–ç•¥ï¼š
        1. åŸºäºè®ºæ–‡é›†åˆçš„Jaccardç›¸ä¼¼åº¦ï¼ˆç°æœ‰æœºåˆ¶ï¼‰
        2. è€ƒè™‘è·¯å¾„ç±»å‹å’Œè§’è‰²å·®å¼‚ï¼ˆæ–°å¢ï¼‰
        3. åŠ¨æ€é˜ˆå€¼è°ƒæ•´ï¼ˆæ–°å¢ï¼‰

        Args:
            paths: æ¼”åŒ–è·¯å¾„åˆ—è¡¨ï¼ˆå·²æŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

        Returns:
            å»é‡åçš„è·¯å¾„åˆ—è¡¨
        """
        if len(paths) <= 1:
            return paths

        deduplicated = []
        removed_count = 0

        for i, path in enumerate(paths):
            path_papers = set(p['paper_id'] for p in path['papers'])

            is_duplicate = False
            for kept_path in deduplicated:
                kept_papers = set(p['paper_id'] for p in kept_path['papers'])

                # è®¡ç®—Jaccardç›¸ä¼¼åº¦
                intersection = len(path_papers & kept_papers)
                union = len(path_papers | kept_papers)
                similarity = intersection / union if union > 0 else 0

                # åŠ¨æ€é˜ˆå€¼ï¼šæ ¹æ®è·¯å¾„ç‰¹å¾è°ƒæ•´
                threshold = self._calculate_dedup_threshold(path, kept_path)

                if similarity >= threshold:
                    # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šæ˜¯å¦è¯­ä¹‰ä¸åŒä½†è®ºæ–‡é‡å 
                    if self._are_semantically_different(path, kept_path):
                        # è¯­ä¹‰ä¸åŒï¼Œä¿ç•™ä¸¤è€…
                        continue

                    is_duplicate = True
                    removed_count += 1
                    logger.info(
                        f"    å»é‡: Thread #{i+1} ä¸ Thread #{deduplicated.index(kept_path)+1} "
                        f"é‡å åº¦ {similarity:.2%}ï¼ˆé˜ˆå€¼{threshold:.2%}ï¼‰ï¼Œå·²ç§»é™¤"
                    )
                    break

            if not is_duplicate:
                deduplicated.append(path)

        if removed_count > 0:
            logger.info(f"    å»é‡å®Œæˆ: ç§»é™¤äº† {removed_count} æ¡é‡å¤è·¯å¾„")

        return deduplicated

    def _deduplicate_paths(self, paths: List[Dict]) -> List[Dict]:
        """
        Bug Fix #2: å»é‡/åˆå¹¶é‡å åº¦é«˜çš„æ¼”åŒ–è·¯å¾„

        å¦‚æœä¸¤æ¡è·¯å¾„çš„è®ºæ–‡é‡åˆåº¦è¶…è¿‡é˜ˆå€¼ï¼Œä¿ç•™æ›´é•¿/æ›´é‡è¦çš„é‚£æ¡

        Args:
            paths: æ¼”åŒ–è·¯å¾„åˆ—è¡¨ï¼ˆå·²æŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

        Returns:
            å»é‡åçš„è·¯å¾„åˆ—è¡¨
        """
        if len(paths) <= 1:
            return paths

        overlap_threshold = self.config.get('path_overlap_threshold', 0.8)
        deduplicated = []
        removed_count = 0

        for i, path in enumerate(paths):
            # æå–å½“å‰è·¯å¾„çš„è®ºæ–‡IDé›†åˆ
            path_papers = set(p['paper_id'] for p in path['papers'])

            # æ£€æŸ¥æ˜¯å¦ä¸å·²ä¿ç•™çš„è·¯å¾„é‡å 
            is_duplicate = False
            for kept_path in deduplicated:
                kept_papers = set(p['paper_id'] for p in kept_path['papers'])

                # è®¡ç®— Jaccard ç›¸ä¼¼åº¦
                intersection = len(path_papers & kept_papers)
                union = len(path_papers | kept_papers)

                if union > 0:
                    similarity = intersection / union
                else:
                    similarity = 0

                # å¦‚æœé‡å åº¦è¶…è¿‡é˜ˆå€¼ï¼Œæ ‡è®°ä¸ºé‡å¤
                if similarity >= overlap_threshold:
                    is_duplicate = True
                    removed_count += 1
                    logger.info(f"    å»é‡: Thread #{i+1} ä¸ Thread #{deduplicated.index(kept_path)+1} é‡å åº¦ {similarity:.2%}ï¼Œå·²ç§»é™¤")
                    break

            if not is_duplicate:
                deduplicated.append(path)

        if removed_count > 0:
            logger.info(f"    å»é‡å®Œæˆ: ç§»é™¤äº† {removed_count} æ¡é‡å¤è·¯å¾„")

        return deduplicated

    def _find_linear_chains(self, graph: nx.DiGraph, start_node: str) -> List[List[str]]:
        """
        è¯†åˆ«ä»èµ·å§‹èŠ‚ç‚¹å‡ºå‘çš„çº¿æ€§é“¾æ¡

        é“¾æ¡å®šä¹‰ï¼šA -> B -> Cï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ€å¤šåªæœ‰ä¸€ä¸ªä¸»è¦åç»§

        Args:
            graph: å›¾è°±
            start_node: èµ·å§‹èŠ‚ç‚¹

        Returns:
            é“¾æ¡åˆ—è¡¨ï¼Œæ¯ä¸ªé“¾æ¡æ˜¯èŠ‚ç‚¹IDåˆ—è¡¨
        """
        chains = []
        min_chain_length = self.config.get('min_chain_length', 3)

        def dfs_chain(current_path: List[str]):
            """DFS æœç´¢é“¾æ¡"""
            current = current_path[-1]
            successors = list(graph.successors(current))

            if len(successors) == 0:
                # åˆ°è¾¾ç»ˆç‚¹
                if len(current_path) >= min_chain_length:
                    chains.append(current_path.copy())
                return

            # ä¼˜å…ˆæ²¿ç€å¼ºå…³ç³»ç»§ç»­
            for successor in successors:
                if successor in current_path:  # é¿å…ç¯
                    continue

                edge_data = graph.edges[current, successor]
                edge_type = edge_data.get('type') or edge_data.get('edge_type', '')

                if edge_type in self.strong_relations or edge_type == '':
                    current_path.append(successor)
                    dfs_chain(current_path)
                    current_path.pop()

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¼ºå…³ç³»ï¼Œè®°å½•å½“å‰é“¾æ¡
            if len(current_path) >= min_chain_length:
                chains.append(current_path.copy())

        dfs_chain([start_node])

        return chains

    def _find_divergence_pattern(self, graph: nx.DiGraph, center_node: str) -> Optional[Dict]:
        """
        è¯†åˆ«ä»¥ä¸­å¿ƒèŠ‚ç‚¹ä¸ºæ ¸å¿ƒçš„åˆ†åŒ–ç»“æ„

        åˆ†åŒ–å®šä¹‰ï¼šä¸€ä¸ªåŸºç¡€æ€§ä¸­å¿ƒèŠ‚ç‚¹è¢«å¤šç¯‡åç»­è®ºæ–‡å¼•ç”¨/æ‰©å±•
        æ­£ç¡®æ–¹å‘ï¼šå¯»æ‰¾ predecessorsï¼ˆå‰é©±ï¼‰ï¼Œå³å¼•ç”¨ä¸­å¿ƒèŠ‚ç‚¹çš„è®ºæ–‡

        ç‰¹åˆ«å…³æ³¨ï¼šOvercomesã€Alternativeã€Extends ç­‰åˆ†æ”¯

        Args:
            graph: å›¾è°±
            center_node: ä¸­å¿ƒèŠ‚ç‚¹

        Returns:
            åˆ†åŒ–ç»“æ„å­—å…¸ï¼ŒåŒ…å«ä¸­å¿ƒèŠ‚ç‚¹å’Œå„æ¡è·¯çº¿
        """
        # Bug Fix #6: ä½¿ç”¨ predecessors è€Œä¸æ˜¯ successors
        # predecessors = å¼•ç”¨äº†ä¸­å¿ƒèŠ‚ç‚¹çš„è®ºæ–‡ï¼ˆå³åç»­ç ”ç©¶ï¼‰
        predecessors = list(graph.predecessors(center_node))

        if len(predecessors) < 2:
            return None

        # å¯¹æ¯ä¸ªå‰é©±èŠ‚ç‚¹ï¼ˆå¼•ç”¨ä¸­å¿ƒè®ºæ–‡çš„è®ºæ–‡ï¼‰ï¼Œè¯†åˆ«å…¶æ¼”åŒ–è·¯çº¿
        routes = []
        for predecessor in predecessors:
            edge_data = graph.edges[predecessor, center_node]
            edge_type = edge_data.get('type') or edge_data.get('edge_type', '')

            # åªä¿ç•™å¼ºå…³ç³»è·¯çº¿ï¼ˆè¿‡æ»¤æ‰ Baselinesï¼‰
            if edge_type not in self.strong_relations:
                continue

            # æ²¿ç€è¿™ä¸ªå‰é©±ç»§ç»­å‘åæœç´¢ï¼Œå½¢æˆä¸€æ¡è·¯çº¿
            route = {
                'relation_type': edge_type,
                'papers': [predecessor]
            }

            # ç»§ç»­å‘åæ¢ç´¢ï¼ˆæœ€å¤šexploration_depthå±‚ï¼‰- æ‰¾æ›´æ–°çš„è®ºæ–‡
            current = predecessor
            for _ in range(self.exploration_depth):
                # Bug Fix #6: ä½¿ç”¨ predecessors æ‰¾æ›´æ–°çš„è®ºæ–‡
                next_predecessors = list(graph.predecessors(current))
                if len(next_predecessors) == 0:
                    break
                # é€‰æ‹©å¼•ç”¨æ•°æœ€é«˜çš„å‰é©±ï¼ˆä¸”æ˜¯å¼ºå…³ç³»ï¼‰
                valid_predecessors = []
                for np in next_predecessors:
                    edge_data = graph.edges[np, current]
                    if (edge_data.get('type') or edge_data.get('edge_type', '')) in self.strong_relations:
                        valid_predecessors.append(np)

                if not valid_predecessors:
                    break

                next_node = max(
                    valid_predecessors,
                    key=lambda x: graph.nodes[x].get('cited_by_count', 0)
                )
                if next_node in route['papers']:
                    break
                route['papers'].append(next_node)
                current = next_node

            routes.append(route)

        # è‡³å°‘æœ‰2æ¡æœ‰æ•ˆè·¯çº¿æ‰ç®—åˆ†åŒ–
        if len(routes) < 2:
            return None

        return {
            'center': center_node,
            'routes': routes
        }

    def _find_convergence_pattern_in_scope(
        self,
        graph: nx.DiGraph,
        center_node: str,
        scope: Set[str]
    ) -> Optional[Dict]:
        """
        åœ¨æŒ‡å®šèŒƒå›´å†…è¯†åˆ«æ±‡èšç»“æ„ï¼ˆé¿å…è·¨è¿é€šåˆ†é‡è¯†åˆ«ï¼‰

        æ±‡èšå®šä¹‰ï¼šä¸€ä¸ªç»¼åˆæ€§ä¸­å¿ƒèŠ‚ç‚¹å¼•ç”¨/æ•´åˆäº†å¤šç¯‡å‰åºè®ºæ–‡
        æ–¹å‘ï¼šä¸­å¿ƒèŠ‚ç‚¹ <- å¤šæ¡åŸºç¡€è·¯çº¿ï¼ˆå‘successorsæ¢ç´¢ï¼‰

        Args:
            graph: å›¾è°±
            center_node: ä¸­å¿ƒèŠ‚ç‚¹
            scope: å…è®¸çš„èŠ‚ç‚¹èŒƒå›´ï¼ˆè¿é€šåˆ†é‡å†…çš„èŠ‚ç‚¹ï¼‰

        Returns:
            æ±‡èšç»“æ„å­—å…¸ï¼ŒåŒ…å«ä¸­å¿ƒèŠ‚ç‚¹å’Œå„æ¡è·¯çº¿
        """
        # ä½¿ç”¨ successors æ‰¾è¢«ä¸­å¿ƒèŠ‚ç‚¹å¼•ç”¨çš„è®ºæ–‡ï¼ˆå³åŸºç¡€å·¥ä½œï¼‰
        successors = list(graph.successors(center_node))

        # å…³é”®ä¿®æ”¹ï¼šåªè€ƒè™‘scopeå†…çš„åç»§èŠ‚ç‚¹
        successors = [s for s in successors if s in scope]

        if len(successors) < 2:
            return None

        # å¯¹æ¯ä¸ªåç»§èŠ‚ç‚¹ï¼Œè¯†åˆ«å…¶åŸºç¡€è·¯çº¿
        routes = []
        for successor in successors:
            edge_data = graph.edges[center_node, successor]
            edge_type = edge_data.get('type') or edge_data.get('edge_type', '')

            # åªä¿ç•™å¼ºå…³ç³»è·¯çº¿
            if edge_type not in self.strong_relations:
                continue

            # åˆå§‹åŒ–è·¯çº¿
            route = {
                'relation_type': edge_type,
                'papers': [successor]
            }

            # ç»§ç»­å‘åæ¢ç´¢ï¼ˆæœ€å¤šexploration_depthå±‚ï¼‰- æ‰¾æ›´æ—©çš„åŸºç¡€è®ºæ–‡
            current = successor
            for _ in range(self.exploration_depth):
                next_successors = list(graph.successors(current))
                # å…³é”®ä¿®æ”¹ï¼šåªè€ƒè™‘scopeå†…çš„èŠ‚ç‚¹
                next_successors = [ns for ns in next_successors if ns in scope]

                if len(next_successors) == 0:
                    break

                # é€‰æ‹©å¼•ç”¨æ•°æœ€é«˜çš„åç»§ï¼ˆä¸”æ˜¯å¼ºå…³ç³»ï¼‰
                valid_successors = []
                for ns in next_successors:
                    edge_data = graph.edges[current, ns]
                    if (edge_data.get('type') or edge_data.get('edge_type', '')) in self.strong_relations:
                        valid_successors.append(ns)

                if not valid_successors:
                    break

                next_node = max(
                    valid_successors,
                    key=lambda x: graph.nodes[x].get('cited_by_count', 0)
                )
                if next_node in route['papers']:
                    break
                route['papers'].append(next_node)
                current = next_node

            routes.append(route)

        # è‡³å°‘æœ‰2æ¡æœ‰æ•ˆè·¯çº¿æ‰ç®—æ±‡èš
        if len(routes) < 2:
            return None

        return {
            'center': center_node,
            'routes': routes
        }

    def _find_convergence_pattern(self, graph: nx.DiGraph, center_node: str) -> Optional[Dict]:
        """
        è¯†åˆ«ä»¥ä¸­å¿ƒèŠ‚ç‚¹ä¸ºæ ¸å¿ƒçš„æ±‡èšç»“æ„

        æ±‡èšå®šä¹‰ï¼šä¸€ä¸ªç»¼åˆæ€§ä¸­å¿ƒèŠ‚ç‚¹æ•´åˆäº†å¤šç¯‡å‰åºåŸºç¡€å·¥ä½œ
        æ­£ç¡®æ–¹å‘ï¼šå¯»æ‰¾ successorsï¼ˆåç»§ï¼‰ï¼Œå³è¢«ä¸­å¿ƒèŠ‚ç‚¹å¼•ç”¨çš„è®ºæ–‡

        Args:
            graph: å›¾è°±
            center_node: ä¸­å¿ƒèŠ‚ç‚¹

        Returns:
            æ±‡èšç»“æ„å­—å…¸ï¼ŒåŒ…å«ä¸­å¿ƒèŠ‚ç‚¹å’Œå„æ¡è·¯çº¿
        """
        # ä½¿ç”¨ successors è€Œé predecessors
        # successors = è¢«ä¸­å¿ƒèŠ‚ç‚¹å¼•ç”¨çš„è®ºæ–‡ï¼ˆå³åŸºç¡€å·¥ä½œï¼‰
        successors = list(graph.successors(center_node))

        if len(successors) < 2:
            return None

        # å¯¹æ¯ä¸ªåç»§èŠ‚ç‚¹ï¼ˆè¢«ä¸­å¿ƒå¼•ç”¨çš„è®ºæ–‡ï¼‰ï¼Œè¯†åˆ«å…¶åŸºç¡€è·¯çº¿
        routes = []
        for successor in successors:
            edge_data = graph.edges[center_node, successor]
            edge_type = edge_data.get('type') or edge_data.get('edge_type', '')

            # åªä¿ç•™å¼ºå…³ç³»è·¯çº¿ï¼ˆè¿‡æ»¤æ‰ Baselinesï¼‰
            if edge_type not in self.strong_relations:
                continue

            # æ²¿ç€è¿™ä¸ªåç»§ç»§ç»­å‘åæœç´¢ï¼Œå½¢æˆä¸€æ¡è·¯çº¿
            route = {
                'relation_type': edge_type,
                'papers': [successor]
            }

            # ç»§ç»­å‘åæ¢ç´¢ï¼ˆæœ€å¤šexploration_depthå±‚ï¼‰- æ‰¾æ›´æ—©çš„åŸºç¡€è®ºæ–‡
            current = successor
            for _ in range(self.exploration_depth):
                # ä½¿ç”¨ successors æ‰¾æ›´æ—©çš„åŸºç¡€è®ºæ–‡
                next_successors = list(graph.successors(current))
                if len(next_successors) == 0:
                    break
                # é€‰æ‹©å¼•ç”¨æ•°æœ€é«˜çš„åç»§ï¼ˆä¸”æ˜¯å¼ºå…³ç³»ï¼‰
                valid_successors = []
                for ns in next_successors:
                    edge_data = graph.edges[current, ns]
                    if (edge_data.get('type') or edge_data.get('edge_type', '')) in self.strong_relations:
                        valid_successors.append(ns)

                if not valid_successors:
                    break

                next_node = max(
                    valid_successors,
                    key=lambda x: graph.nodes[x].get('cited_by_count', 0)
                )
                if next_node in route['papers']:
                    break
                route['papers'].append(next_node)
                current = next_node

            routes.append(route)

        # è‡³å°‘æœ‰2æ¡æœ‰æ•ˆè·¯çº¿æ‰ç®—æ±‡èš
        if len(routes) < 2:
            return None

        return {
            'center': center_node,
            'routes': routes
        }

    def _create_chain_narrative(
        self,
        graph: nx.DiGraph,
        chain: List[str],
        seed_data: Dict
    ) -> Dict:
        """
        ä¸ºçº¿æ€§é“¾æ¡åˆ›å»ºå™äº‹å•å…ƒ

        ç”Ÿæˆæ¨¡æ¿ï¼š
        - èµ·å› ï¼šè®ºæ–‡ A æå‡ºäº† [Method A] æ¥è§£å†³ [Problem]ï¼Œä½†åœ¨ [Limitation] æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
        - è½¬æŠ˜ï¼šè®ºæ–‡ B é’ˆå¯¹è¿™ä¸€ä¸è¶³ï¼Œé€šè¿‡ [Method B] æˆåŠŸå…‹æœäº†è¯¥é—®é¢˜ã€‚
        - å‘å±•ï¼šéšåï¼Œè®ºæ–‡ C åœ¨ B çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†å…¶åº”ç”¨åœºæ™¯ã€‚

        Args:
            graph: å›¾è°±
            chain: é“¾æ¡èŠ‚ç‚¹åˆ—è¡¨
            seed_data: ç§å­èŠ‚ç‚¹æ•°æ®

        Returns:
            å™äº‹å•å…ƒå­—å…¸
        """
        # Bug Fix #1: æŒ‰æ—¶é—´æ’åºé“¾æ¡ï¼ˆæ—¶é—´å€’æµé—®é¢˜ï¼‰
        # è·å–æ¯ç¯‡è®ºæ–‡çš„å¹´ä»½ï¼ŒæŒ‰æ—¶é—´æ­£åºæ’åº
        chain_with_years = []
        for paper_id in chain:
            node_data = graph.nodes[paper_id]
            year = node_data.get('year', 0)
            chain_with_years.append((paper_id, year))

        # æŒ‰å¹´ä»½å‡åºæ’åº
        chain_with_years.sort(key=lambda x: x[1])
        sorted_chain = [paper_id for paper_id, _ in chain_with_years]

        logger.info(f"    æŒ‰æ—¶é—´æ’åºé“¾æ¡: {[f'{pid}({y})' for pid, y in chain_with_years]}")

        papers_info = []
        total_citations = 0
        relation_chain = []  # æ–°å¢ï¼šè¯¦ç»†çš„å…³ç³»é“¾

        for i, paper_id in enumerate(sorted_chain):
            node_data = graph.nodes[paper_id]
            papers_info.append({
                'paper_id': paper_id,
                'title': node_data.get('title', ''),
                'year': node_data.get('year', 0),
                'cited_by_count': node_data.get('cited_by_count', 0)
            })
            total_citations += node_data.get('cited_by_count', 0)

            # æ„å»ºå…³ç³»é“¾ï¼šæå–æ¯å¯¹è®ºæ–‡ä¹‹é—´çš„å…³ç³»
            # Bug Fix #3: ä¿®æ­£å…³ç³»æ–¹å‘ - æ”¹ä¸ºå‘å±•æ–¹å‘ï¼ˆæ—©->æ™šï¼‰
            # Bug Fix #5: ä¿®æ­£å…³ç³»è¯­ä¹‰ - æ—¶é—´æ­£åºæ—¶éœ€è¦åè½¬å…³ç³»å«ä¹‰
            if i < len(sorted_chain) - 1:
                next_paper_id = sorted_chain[i + 1]
                edge_type = 'Unknown'
                edge_description = 'Unknown'

                # å°è¯•ä¸¤ä¸ªæ–¹å‘æŸ¥æ‰¾è¾¹
                if graph.has_edge(paper_id, next_paper_id):
                    # æ—©è®ºæ–‡ -> æ™šè®ºæ–‡ï¼ˆè¿™ç§æƒ…å†µå°‘è§ï¼Œå¯èƒ½æ˜¯ Inspires ç±»å‹ï¼‰
                    edge_data = graph.edges[paper_id, next_paper_id]
                    original_type = edge_data.get('type') or edge_data.get('edge_type', 'Temporal_Evolution')
                    edge_type = original_type
                    edge_description = original_type
                elif graph.has_edge(next_paper_id, paper_id):
                    # æ™šè®ºæ–‡ -> æ—©è®ºæ–‡ï¼ˆå¸¸è§æƒ…å†µï¼šæ–°è®ºæ–‡ Overcomes æ—§è®ºæ–‡ï¼‰
                    edge_data = graph.edges[next_paper_id, paper_id]
                    original_type = edge_data.get('type') or edge_data.get('edge_type', 'Unknown')

                    # å…³é”®ä¿®å¤ï¼šåè½¬å…³ç³»è¯­ä¹‰ä»¥ç¬¦åˆæ—¶é—´å™äº‹
                    # åŸå§‹ï¼šæ–°è®ºæ–‡(2023) --Overcomes--> æ—§è®ºæ–‡(2021)
                    # å™äº‹ï¼šæ—§è®ºæ–‡(2021) --Was_Overcome_By--> æ–°è®ºæ–‡(2023)
                    edge_type = original_type
                    edge_description = self._reverse_relation_semantics(original_type)
                else:
                    # æ²¡æœ‰ç›´æ¥è¾¹ï¼Œæ ‡è®°ä¸ºæ—¶é—´æ¼”è¿›å…³ç³»
                    edge_type = 'Temporal_Evolution'
                    edge_description = 'Temporal_Evolution'

                next_node_data = graph.nodes[next_paper_id]
                relation_chain.append({
                    'from_paper': {
                        'id': paper_id,
                        'title': node_data.get('title', ''),
                        'year': node_data.get('year', 0)
                    },
                    'to_paper': {
                        'id': next_paper_id,
                        'title': next_node_data.get('title', ''),
                        'year': next_node_data.get('year', 0)
                    },
                    'relation_type': edge_type,  # åŸå§‹å…³ç³»ç±»å‹
                    'narrative_relation': edge_description,  # å™äº‹ç”¨çš„å…³ç³»æè¿°
                    'direction': 'chronological'  # æ˜ç¡®æ ‡è®°ä¸ºæ—¶é—´æ­£åº
                })

        # æå–å…³é”®ä¿¡æ¯ï¼ˆä½¿ç”¨æ’åºåçš„é“¾æ¡ï¼‰
        first_paper = graph.nodes[sorted_chain[0]]
        last_paper = graph.nodes[sorted_chain[-1]]

        # ç”Ÿæˆæ ‡é¢˜
        first_method = self._extract_key_method(first_paper)
        last_method = self._extract_key_method(last_paper)

        title = f"ä» {first_method} åˆ° {last_method} çš„æ¼”è¿›ä¹‹è·¯"

        # ç”Ÿæˆå™äº‹æ–‡æœ¬ï¼ˆä½¿ç”¨LLMæˆ–æ¨¡æ¿ï¼Œä½¿ç”¨æ’åºåçš„é“¾æ¡ï¼‰
        narrative = self._generate_chain_narrative_text(graph, sorted_chain)

        return {
            'thread_type': 'chain',
            'pattern_type': 'The Chain (çº¿æ€§é“¾æ¡)',
            'title': title,
            'narrative': narrative,
            'papers': papers_info,
            'total_citations': total_citations,
            'visual_structure': ' -> '.join([f"Paper_{i+1}" for i in range(len(sorted_chain))]),
            'relation_chain': relation_chain  # æ–°å¢ï¼šè¯¦ç»†çš„å…³ç³»é“¾
        }

    def _create_divergence_narrative(
        self,
        graph: nx.DiGraph,
        divergence: Dict,
        seed_data: Dict
    ) -> Dict:
        """
        ä¸ºåˆ†åŒ–ç»“æ„åˆ›å»ºå™äº‹å•å…ƒ

        Bug Fix #6: ä¿®æ­£åˆ†åŒ–ç»“æ„çš„æ—¶é—´é€»è¾‘
        æ­£ç¡®å™äº‹ï¼šä¸­å¿ƒè®ºæ–‡ï¼ˆæ—©æœŸåŸºç¡€ï¼‰-> å¤šæ¡åç»­æ¼”è¿›è·¯çº¿ï¼ˆæ™šæœŸï¼‰

        ç”Ÿæˆæ¨¡æ¿ï¼š
        - ç„¦ç‚¹ï¼šä¸­å¿ƒè®ºæ–‡æ˜¯è¯¥é¢†åŸŸçš„åŸºçŸ³ï¼Œä½†å®ƒç•™ä¸‹äº† [Limitation] çš„é—®é¢˜ã€‚
        - åˆ†æ­§ï¼šå­¦æœ¯ç•Œå¯¹æ­¤äº§ç”Ÿäº†ä¸åŒçš„æ¼”è¿›è·¯çº¿ã€‚
        - å¯¹æ¯”ï¼š(æ’å…¥å„è·¯çº¿çš„å¯¹æ¯”)

        Args:
            graph: å›¾è°±
            divergence: åˆ†åŒ–ç»“æ„
            seed_data: ç§å­èŠ‚ç‚¹æ•°æ®

        Returns:
            å™äº‹å•å…ƒå­—å…¸
        """
        center_id = divergence['center']
        center_data = graph.nodes[center_id]
        center_year = center_data.get('year', 0)

        papers_info = [{
            'paper_id': center_id,
            'title': center_data.get('title', ''),
            'year': center_year,
            'cited_by_count': center_data.get('cited_by_count', 0),
            'role': 'center'
        }]

        total_citations = center_data.get('cited_by_count', 0)

        # æ”¶é›†æ‰€æœ‰è·¯çº¿çš„è®ºæ–‡å’Œå…³ç³»é“¾
        routes_info = []
        relation_chain = []  # æ–°å¢ï¼šè¯¦ç»†çš„å…³ç³»é“¾

        for route_idx, route in enumerate(divergence['routes']):
            route_papers = []
            for paper_id in route['papers']:
                node_data = graph.nodes[paper_id]
                route_papers.append({
                    'paper_id': paper_id,
                    'title': node_data.get('title', ''),
                    'year': node_data.get('year', 0),
                    'cited_by_count': node_data.get('cited_by_count', 0)
                })
                total_citations += node_data.get('cited_by_count', 0)
                papers_info.append(route_papers[-1])

            routes_info.append({
                'relation_type': route['relation_type'],
                'papers': route_papers
            })

            # Bug Fix #6: ä¿®æ­£å…³ç³»é“¾æ–¹å‘
            # ç°åœ¨ route['papers'] ä¸­çš„è®ºæ–‡å¼•ç”¨äº†ä¸­å¿ƒè®ºæ–‡
            # æ‰€ä»¥å…³ç³»æ–¹å‘åº”è¯¥æ˜¯ï¼šè·¯çº¿è®ºæ–‡ -> ä¸­å¿ƒè®ºæ–‡ï¼ˆå›¾ä¸­ï¼‰
            # ä½†å™äº‹æ–¹å‘åº”è¯¥æ˜¯ï¼šä¸­å¿ƒè®ºæ–‡ -> è·¯çº¿è®ºæ–‡ï¼ˆæ—¶é—´ï¼‰
            if route_papers:
                first_paper = route_papers[0]
                first_paper_year = first_paper['year']

                # æ£€æŸ¥æ—¶é—´å…³ç³»ï¼Œç¡®ä¿å™äº‹æ­£ç¡®
                if center_year <= first_paper_year:
                    # ä¸­å¿ƒè®ºæ–‡æ›´æ—©ï¼ˆæ­£å¸¸æƒ…å†µï¼‰
                    relation_chain.append({
                        'from_paper': {
                            'id': center_id,
                            'title': center_data.get('title', ''),
                            'year': center_year
                        },
                        'to_paper': {
                            'id': first_paper['paper_id'],
                            'title': first_paper['title'],
                            'year': first_paper_year
                        },
                        'relation_type': route['relation_type'],
                        'narrative_relation': self._reverse_relation_semantics(route['relation_type']),
                        'route_id': route_idx + 1,
                        'direction': 'chronological'
                    })
                else:
                    # å¼‚å¸¸æƒ…å†µï¼šè·¯çº¿è®ºæ–‡æ›´æ—©ï¼ˆè®°å½•ä½†æ ‡æ³¨ï¼‰
                    logger.warning(f"    åˆ†åŒ–ç»“æ„å¼‚å¸¸: è·¯çº¿è®ºæ–‡ {first_paper['paper_id']} ({first_paper_year}) æ—©äºä¸­å¿ƒè®ºæ–‡ {center_id} ({center_year})")
                    relation_chain.append({
                        'from_paper': {
                            'id': first_paper['paper_id'],
                            'title': first_paper['title'],
                            'year': first_paper_year
                        },
                        'to_paper': {
                            'id': center_id,
                            'title': center_data.get('title', ''),
                            'year': center_year
                        },
                        'relation_type': route['relation_type'],
                        'narrative_relation': route['relation_type'],
                        'route_id': route_idx + 1,
                        'direction': 'reverse_chronological'
                    })

        # æå–æ ¸å¿ƒé—®é¢˜
        seed_problem = self._extract_key_problem(center_data)

        # ç”Ÿæˆæ ‡é¢˜
        title = f"é’ˆå¯¹ {seed_problem} çš„å¤šæŠ€æœ¯è·¯çº¿åšå¼ˆ"

        # ç”Ÿæˆå™äº‹æ–‡æœ¬
        narrative = self._generate_divergence_narrative_text(graph, divergence, center_data)

        return {
            'thread_type': 'divergence',
            'pattern_type': 'The Divergence (åˆ†åŒ–æ¨¡å¼)',
            'title': title,
            'narrative': narrative,
            'center_paper': center_data.get('title', ''),
            'routes_count': len(routes_info),
            'routes': routes_info,
            'papers': papers_info,
            'total_citations': total_citations,
            'visual_structure': f"Center -> {len(routes_info)} Routes",
            'relation_chain': relation_chain  # æ–°å¢ï¼šè¯¦ç»†çš„å…³ç³»é“¾
        }

    def _create_convergence_narrative(
        self,
        graph: nx.DiGraph,
        convergence: Dict,
        seed_data: Dict
    ) -> Dict:
        """
        ä¸ºæ±‡èšç»“æ„åˆ›å»ºå™äº‹å•å…ƒ

        æ±‡èšæ¨¡å¼ï¼šå¤šæ¡åŸºç¡€è·¯çº¿æ±‡èšåˆ°ä¸€ä¸ªç»¼åˆæ€§è®ºæ–‡

        ç”Ÿæˆæ¨¡æ¿ï¼š
        - èƒŒæ™¯ï¼šå¤šä¸ªç‹¬ç«‹çš„ç ”ç©¶æ–¹å‘åˆ†åˆ«æ¢ç´¢äº†ä¸åŒçš„æŠ€æœ¯è·¯å¾„
        - æ±‡èšï¼šä¸­å¿ƒè®ºæ–‡æ•´åˆäº†è¿™äº›æ–¹å‘ï¼Œå½¢æˆç»¼åˆæ¡†æ¶
        - æ„ä¹‰ï¼šæ ‡å¿—ç€é¢†åŸŸç†è®ºçš„ç³»ç»ŸåŒ–æ•´åˆ

        Args:
            graph: å›¾è°±
            convergence: æ±‡èšç»“æ„
            seed_data: ç§å­èŠ‚ç‚¹æ•°æ®

        Returns:
            å™äº‹å•å…ƒå­—å…¸
        """
        center_id = convergence['center']
        center_data = graph.nodes[center_id]
        center_year = center_data.get('year', 0)

        papers_info = [{
            'paper_id': center_id,
            'title': center_data.get('title', ''),
            'year': center_year,
            'cited_by_count': center_data.get('cited_by_count', 0),
            'role': 'center'
        }]

        total_citations = center_data.get('cited_by_count', 0)

        # æ”¶é›†æ‰€æœ‰è·¯çº¿çš„è®ºæ–‡å’Œå…³ç³»é“¾
        routes_info = []
        relation_chain = []

        for route_idx, route in enumerate(convergence['routes']):
            route_papers = []
            for paper_id in route['papers']:
                node_data = graph.nodes[paper_id]
                route_papers.append({
                    'paper_id': paper_id,
                    'title': node_data.get('title', ''),
                    'year': node_data.get('year', 0),
                    'cited_by_count': node_data.get('cited_by_count', 0)
                })
                total_citations += node_data.get('cited_by_count', 0)
                papers_info.append(route_papers[-1])

            routes_info.append({
                'relation_type': route['relation_type'],
                'papers': route_papers
            })

            # æ„å»ºå…³ç³»é“¾ï¼šä¸­å¿ƒè®ºæ–‡å¼•ç”¨åŸºç¡€è·¯çº¿
            if route_papers:
                first_paper = route_papers[0]
                first_paper_year = first_paper['year']

                # æ±‡èšç»“æ„ï¼šä¸­å¿ƒè®ºæ–‡å¼•ç”¨åŸºç¡€è®ºæ–‡ï¼ˆæ—¶é—´ä¸Šä¸­å¿ƒè®ºæ–‡åº”è¯¥æ›´æ™šï¼‰
                if center_year >= first_paper_year:
                    # ä¸­å¿ƒè®ºæ–‡æ›´æ™šï¼ˆæ­£å¸¸æƒ…å†µï¼‰
                    relation_chain.append({
                        'from_paper': {
                            'id': first_paper['paper_id'],
                            'title': first_paper['title'],
                            'year': first_paper_year
                        },
                        'to_paper': {
                            'id': center_id,
                            'title': center_data.get('title', ''),
                            'year': center_year
                        },
                        'relation_type': route['relation_type'],
                        'narrative_relation': f"è¢«{center_data.get('title', '')}æ•´åˆ",
                        'route_id': route_idx + 1,
                        'direction': 'chronological'
                    })
                else:
                    # å¼‚å¸¸æƒ…å†µï¼šåŸºç¡€è®ºæ–‡æ›´æ™š
                    logger.warning(f"    æ±‡èšç»“æ„å¼‚å¸¸: åŸºç¡€è®ºæ–‡ {first_paper['paper_id']} ({first_paper_year}) æ™šäºä¸­å¿ƒè®ºæ–‡ {center_id} ({center_year})")
                    relation_chain.append({
                        'from_paper': {
                            'id': center_id,
                            'title': center_data.get('title', ''),
                            'year': center_year
                        },
                        'to_paper': {
                            'id': first_paper['paper_id'],
                            'title': first_paper['title'],
                            'year': first_paper_year
                        },
                        'relation_type': route['relation_type'],
                        'narrative_relation': route['relation_type'],
                        'route_id': route_idx + 1,
                        'direction': 'reverse_chronological'
                    })

        # æå–æ ¸å¿ƒæ–¹æ³•
        center_method = self._extract_key_method(center_data)

        # ç”Ÿæˆæ ‡é¢˜
        title = f"å¤šæŠ€æœ¯è·¯çº¿æ±‡èšåˆ° {center_method}"

        # ç”Ÿæˆå™äº‹æ–‡æœ¬
        narrative = self._generate_convergence_narrative_text(graph, convergence, center_data)

        return {
            'thread_type': 'convergence',
            'pattern_type': 'The Convergence (æ±‡èšæ¨¡å¼)',
            'title': title,
            'narrative': narrative,
            'center_paper': center_data.get('title', ''),
            'routes_count': len(routes_info),
            'routes': routes_info,
            'papers': papers_info,
            'total_citations': total_citations,
            'visual_structure': f"{len(routes_info)} Routes -> Center",
            'relation_chain': relation_chain
        }

    def _generate_chain_narrative_text(self, graph: nx.DiGraph, chain: List[str]) -> str:
        """
        ç”Ÿæˆçº¿æ€§é“¾æ¡çš„å™äº‹æ–‡æœ¬

        Args:
            graph: å›¾è°±
            chain: é“¾æ¡èŠ‚ç‚¹åˆ—è¡¨

        Returns:
            å™äº‹æ–‡æœ¬
        """
        if self.llm_client:
            return self._generate_chain_narrative_with_llm(graph, chain)
        else:
            return self._generate_chain_narrative_template(graph, chain)

    def _generate_chain_narrative_template(self, graph: nx.DiGraph, chain: List[str]) -> str:
        """
        ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆé“¾æ¡å™äº‹ï¼ˆé™çº§æ–¹æ¡ˆï¼‰

        æ”¹è¿›ç‰ˆæœ¬ï¼šå¼•ç”¨ç±»å‹æ„ŸçŸ¥å™äº‹ç”Ÿæˆ
        Bug Fix #4: æŒ‰æ—¶é—´æ­£åºå™è¿°ï¼ˆæ—©->æ™šï¼‰
        Bug Fix #5: ä½¿ç”¨æ­£ç¡®çš„å…³ç³»è¯­ä¹‰
        """
        # æŒ‰å¹´ä»½æ’åºï¼ˆæ—¶é—´æ­£åºï¼‰
        chain_with_years = [(pid, graph.nodes[pid].get('year', 0)) for pid in chain]
        chain_with_years.sort(key=lambda x: x[1])
        sorted_chain = [pid for pid, _ in chain_with_years]

        narrative_parts = []

        for i, paper_id in enumerate(sorted_chain):
            node_data = graph.nodes[paper_id]
            title = node_data.get('title', 'Unknown')
            year = node_data.get('year', 'N/A')

            if i == 0:
                # èµ·æºï¼šæœ€æ—©çš„è®ºæ–‡ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                method = self._extract_key_method(node_data)
                problem = self._extract_key_problem(node_data)
                limitation = self._extract_key_limitation(node_data)
                narrative_parts.append(
                    f"**èµ·æº** ({year}å¹´): è®ºæ–‡ã€Š{title}ã€‹é¦–æ¬¡æå‡ºäº† {method} æ¥è§£å†³ {problem}ï¼Œ"
                    f"å¼€åˆ›äº†è¿™ä¸€ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œè¯¥å·¥ä½œåœ¨ {limitation} æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚"
                )
            else:
                # æ¼”è¿›å’Œæœ€æ–°è¿›å±•ï¼šä½¿ç”¨å¼•ç”¨ç±»å‹æ„ŸçŸ¥å™äº‹
                prev_paper_id = sorted_chain[i-1]

                # è·å–è¾¹çš„å…³ç³»ç±»å‹
                relation_type = self._get_relation_type(graph, prev_paper_id, paper_id)

                # æå–å¼•ç”¨ç±»å‹ç›¸å…³çš„ä¿¡æ¯
                info = self._extract_papers_info_for_relation(
                    graph, prev_paper_id, paper_id, relation_type
                )

                # ç”Ÿæˆé’ˆå¯¹æ€§å™äº‹ç‰‡æ®µ
                narrative_fragment = self._generate_relation_narrative_fragment(info)
                narrative_parts.append(narrative_fragment)

        return "\n\n".join(narrative_parts)

    def _generate_chain_narrative_with_llm(self, graph: nx.DiGraph, chain: List[str]) -> str:
        """
        ä½¿ç”¨LLMç”Ÿæˆé“¾æ¡å™äº‹

        æ”¹è¿›ç‰ˆæœ¬ï¼šå¼•ç”¨ç±»å‹æ„ŸçŸ¥çš„Promptå¢å¼º
        Bug Fix #4: æŒ‰æ—¶é—´æ­£åºå™è¿°
        """
        # æŒ‰å¹´ä»½æ’åºï¼ˆæ—¶é—´æ­£åºï¼‰
        chain_with_years = [(pid, graph.nodes[pid].get('year', 0)) for pid in chain]
        chain_with_years.sort(key=lambda x: x[1])
        sorted_chain = [pid for pid, _ in chain_with_years]

        # å‡†å¤‡ä¸Šä¸‹æ–‡
        papers_context = []
        for i, paper_id in enumerate(sorted_chain, 1):
            node_data = graph.nodes[paper_id]
            title = node_data.get('title', '')
            year = node_data.get('year', '')

            # æå–åŸºç¡€ä¿¡æ¯
            method = self._extract_key_method(node_data)
            problem = self._extract_key_problem(node_data)
            limitation = self._extract_key_limitation(node_data)
            future_work = self._extract_key_future_work(node_data)

            paper_info = f"è®ºæ–‡{i}: {title} ({year}å¹´)\n" \
                         f"- ç ”ç©¶é—®é¢˜: {problem}\n" \
                         f"- ç ”ç©¶æ–¹æ³•: {method}\n" \
                         f"- å±€é™æ€§: {limitation}\n" \
                         f"- æœªæ¥å·¥ä½œ: {future_work}"

            # å¦‚æœä¸æ˜¯ç¬¬ä¸€ç¯‡è®ºæ–‡ï¼Œæ·»åŠ ä¸å‰ä¸€ç¯‡çš„å…³ç³»ä¿¡æ¯
            if i > 1:
                prev_paper_id = sorted_chain[i-2]  # iä»1å¼€å§‹ï¼Œæ‰€ä»¥i-2æ˜¯å‰ä¸€ç¯‡
                relation_type = self._get_relation_type(graph, prev_paper_id, paper_id)
                relation_focus = self._get_relation_focus_hint(relation_type)
                paper_info += f"\n- ä¸å‰ä¸€ç¯‡çš„å…³ç³»: {relation_type} ({relation_focus})"

            papers_context.append(paper_info)

        context = "\n\n".join(papers_context)

        prompt = f"""ä½ æ˜¯ä¸€ä½å­¦æœ¯ç»¼è¿°ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æŠ€æœ¯æ¼”è¿›é“¾æ¡ï¼Œç”Ÿæˆä¸€æ®µé€šé¡ºçš„å™äº‹æ–‡æœ¬ã€‚

**æ¼”è¿›é“¾æ¡**ï¼ˆå…±{len(sorted_chain)}ç¯‡è®ºæ–‡ï¼ŒæŒ‰æ—¶é—´æ­£åºä»æ—©åˆ°æ™šæ’åˆ—ï¼‰:
{context}

**ä»»åŠ¡è¦æ±‚**:
è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆå™äº‹æ–‡æœ¬ï¼ˆ3-5æ®µï¼Œæ¯æ®µ2-3å¥è¯ï¼‰ï¼š
1. **èµ·æº**: æè¿°ç¬¬ä¸€ç¯‡ï¼ˆæœ€æ—©ï¼‰è®ºæ–‡å¦‚ä½•å¼€åˆ›äº†è¿™ä¸€æ–¹å‘ï¼Œè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Œä½†ç•™ä¸‹äº†ä»€ä¹ˆä¸è¶³
2. **æ¼”è¿›**: æè¿°ä¸­é—´è®ºæ–‡å¦‚ä½•é’ˆå¯¹å‰äººçš„ä¸è¶³é€æ­¥æ”¹è¿›ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
3. **æœ€æ–°è¿›å±•**: æè¿°æœ€åä¸€ç¯‡ï¼ˆæœ€æ–°ï¼‰è®ºæ–‡å¦‚ä½•åœ¨å‰äººåŸºç¡€ä¸Šå®ç°äº†çªç ´

**è¾“å‡ºè¦æ±‚**:
- ä½¿ç”¨è¿è´¯ã€å­¦æœ¯åŒ–çš„ä¸­æ–‡è¡¨è¾¾
- çªå‡ºè®ºæ–‡ä¹‹é—´çš„å› æœå…³ç³»å’ŒæŠ€æœ¯æ¼”è¿›é€»è¾‘
- æŒ‰æ—¶é—´æ­£åºå™è¿°ï¼ˆä»æ—©åˆ°æ™šï¼‰ï¼Œæ˜ç¡®æ ‡æ³¨å¹´ä»½
- æ¯æ®µä»¥ **èµ·æº**ã€**æ¼”è¿›**ã€**æœ€æ–°è¿›å±•** ç­‰æ ‡é¢˜å¼€å¤´
"""

        try:
            narrative = self.llm_client.generate(
                prompt=prompt,
                temperature=0.3,
                max_tokens=600
            )
            return narrative.strip()
        except Exception as e:
            logger.warning(f"LLMç”Ÿæˆå™äº‹å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ¿æ–¹æ³•")
            return self._generate_chain_narrative_template(graph, sorted_chain)

    def _generate_divergence_narrative_text(
        self,
        graph: nx.DiGraph,
        divergence: Dict,
        center_data: Dict
    ) -> str:
        """
        ç”Ÿæˆåˆ†åŒ–ç»“æ„çš„å™äº‹æ–‡æœ¬

        Args:
            graph: å›¾è°±
            divergence: åˆ†åŒ–ç»“æ„
            center_data: ä¸­å¿ƒèŠ‚ç‚¹æ•°æ®

        Returns:
            å™äº‹æ–‡æœ¬
        """
        if self.llm_client:
            return self._generate_divergence_narrative_with_llm(graph, divergence, center_data)
        else:
            return self._generate_divergence_narrative_template(graph, divergence, center_data)

    def _generate_divergence_narrative_template(
        self,
        graph: nx.DiGraph,
        divergence: Dict,
        center_data: Dict
    ) -> str:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆåˆ†åŒ–å™äº‹ï¼ˆé™çº§æ–¹æ¡ˆï¼‰

        æ”¹è¿›ç‰ˆæœ¬ï¼šå¼•ç”¨ç±»å‹æ„ŸçŸ¥å™äº‹ç”Ÿæˆ
        """
        center_title = center_data.get('title', 'Unknown')
        center_year = center_data.get('year', 'N/A')
        problem = self._extract_key_problem(center_data)
        limitation = self._extract_key_limitation(center_data)

        narrative_parts = []

        # ç„¦ç‚¹
        narrative_parts.append(
            f"**ç„¦ç‚¹**: è®ºæ–‡ã€Š{center_title}ã€‹({center_year}) æ˜¯è¯¥é¢†åŸŸçš„åŸºçŸ³å·¥ä½œï¼Œ"
            f"å®ƒèšç„¦äº {problem}ï¼Œä½†ç•™ä¸‹äº† {limitation} çš„é—®é¢˜ã€‚"
        )

        # åˆ†æ­§ - æ ¹æ®å®é™…å…³ç³»ç±»å‹ç”Ÿæˆæè¿°ï¼ˆå¢å¼ºç‰ˆï¼šå¼•ç”¨ç±»å‹æ„ŸçŸ¥ï¼‰
        center_paper_id = divergence['center']
        routes_desc = []
        for i, route in enumerate(divergence['routes'], 1):
            route_papers = route['papers']
            relation = route['relation_type']

            # è·å–è·¯çº¿ä¸­æœ€æ–°è®ºæ–‡ï¼ˆé€šå¸¸æ˜¯è·¯çº¿çš„ç»ˆç‚¹ï¼‰
            latest_paper_id = route_papers[-1] if route_papers else None
            if latest_paper_id:
                # ä½¿ç”¨å¼•ç”¨ç±»å‹æ„ŸçŸ¥çš„ä¿¡æ¯æå–
                info = self._extract_papers_info_for_relation(
                    graph, center_paper_id, latest_paper_id, relation
                )

                # ç”Ÿæˆé’ˆå¯¹æ€§çš„è·¯çº¿æè¿°ï¼ˆç®€åŒ–ç‰ˆï¼Œé€‚é…åˆ†åŒ–åœºæ™¯ï¼‰
                if relation == 'Overcomes':
                    method = info.get('curr_method', 'æ–°æ–¹æ³•')
                    limitation = info.get('prev_limitation', 'æŸäº›å±€é™æ€§')
                    desc = f"**è·¯çº¿{i}** (çºµå‘æ·±åŒ–): é’ˆå¯¹ã€Œ{limitation}ã€ï¼Œé€šè¿‡ {method} å®ç°çªç ´"
                elif relation == 'Realizes':
                    method = info.get('curr_method', 'æ–°æ–¹æ³•')
                    future_work = info.get('prev_future_work', 'å‰äººè®¾æƒ³')
                    desc = f"**è·¯çº¿{i}** (ç§‘ç ”ä¼ æ‰¿): å®ç°ã€Œ{future_work}ã€çš„æ„¿æ™¯ï¼Œé‡‡ç”¨ {method}"
                elif relation == 'Extends':
                    method = info.get('curr_method', 'æ”¹è¿›æ–¹æ³•')
                    desc = f"**è·¯çº¿{i}** (å¾®åˆ›æ–°): ä¿ç•™æ ¸å¿ƒæ¶æ„å¹¶æ‰©å±•ï¼Œé‡‡ç”¨ {method}"
                elif relation == 'Alternative':
                    method = info.get('curr_method', 'æ›¿ä»£æ–¹æ³•')
                    desc = f"**è·¯çº¿{i}** (é¢ è¦†åˆ›æ–°): æå‡ºæˆªç„¶ä¸åŒçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½¿ç”¨ {method}"
                elif relation == 'Adapts_to':
                    method = info.get('curr_method', 'è¿ç§»æ–¹æ³•')
                    curr_domain = info.get('curr_domain', 'æ–°é¢†åŸŸ')
                    desc = f"**è·¯çº¿{i}** (æ¨ªå‘æ‰©æ•£): å°†æŠ€æœ¯è¿ç§»åˆ°ã€Œ{curr_domain}ã€ï¼Œé‡‡ç”¨ {method}"
                else:  # Baselines
                    method = info.get('curr_method', 'æ–°æ–¹æ³•')
                    desc = f"**è·¯çº¿{i}**: åŸºäºä¸­å¿ƒè®ºæ–‡çš„å·¥ä½œï¼Œé‡‡ç”¨ {method}"

                routes_desc.append(desc)

        narrative_parts.append(
            f"**åˆ†æ­§**: å­¦æœ¯ç•Œå¯¹æ­¤äº§ç”Ÿäº†ä¸åŒçš„æ¼”è¿›è·¯çº¿ã€‚" +
            "ï¼›".join(routes_desc) + "ã€‚"
        )

        # å¯¹æ¯” - çªå‡ºä¸åŒè·¯çº¿çš„ç‰¹ç‚¹
        route_types = [r['relation_type'] for r in divergence['routes']]
        if 'Alternative' in route_types and 'Extends' in route_types:
            comparison = "è¿™äº›è·¯çº¿ä½“ç°äº†å­¦æœ¯ç ”ç©¶çš„å¤šæ ·æ€§ï¼šä¸€äº›é€‰æ‹©é¢ è¦†å¼åˆ›æ–°ï¼Œå¦ä¸€äº›é€‰æ‹©æ¸è¿›å¼æ”¹è¿›"
        elif 'Overcomes' in route_types:
            comparison = "è¿™äº›è·¯çº¿å…±åŒæ¨åŠ¨äº†è¯¥é¢†åŸŸç—›ç‚¹é—®é¢˜çš„è§£å†³ï¼Œå½¢æˆäº†å¤šè§’åº¦æ”»å…‹çš„å±€é¢"
        else:
            comparison = "è¿™äº›è·¯çº¿å„æœ‰ä¼˜åŠ¿ï¼Œå…±åŒæ¨åŠ¨äº†é¢†åŸŸçš„å¤šå…ƒåŒ–å‘å±•"

        narrative_parts.append(f"**å¯¹æ¯”**: {comparison}ã€‚ï¼ˆè¯¦è§å„è·¯çº¿è®ºæ–‡çš„æ€§èƒ½å¯¹æ¯”ï¼‰")

        return "\n\n".join(narrative_parts)

    def _generate_divergence_narrative_with_llm(
        self,
        graph: nx.DiGraph,
        divergence: Dict,
        center_data: Dict
    ) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆåˆ†åŒ–å™äº‹"""
        center_title = center_data.get('title', '')
        center_year = center_data.get('year', '')
        problem = self._extract_key_problem(center_data)
        limitation = self._extract_key_limitation(center_data)

        # å‡†å¤‡å„è·¯çº¿çš„ä¸Šä¸‹æ–‡
        routes_context = []
        for i, route in enumerate(divergence['routes'], 1):
            relation = route['relation_type']
            route_papers = []
            for paper_id in route['papers'][:2]:  # æ¯æ¡è·¯çº¿æœ€å¤š2ç¯‡
                node_data = graph.nodes[paper_id]
                route_papers.append(
                    f"  - {node_data.get('title', '')} ({node_data.get('year', '')}): "
                    f"{self._extract_key_method(node_data)}"
                )

            routes_context.append(
                f"è·¯çº¿{i} ({relation}):\n" + "\n".join(route_papers)
            )

        routes_text = "\n\n".join(routes_context)

        prompt = f"""ä½ æ˜¯ä¸€ä½å­¦æœ¯ç»¼è¿°ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹åˆ†åŒ–æ¼”è¿›ç»“æ„ï¼Œç”Ÿæˆä¸€æ®µé€šé¡ºçš„å™äº‹æ–‡æœ¬ã€‚

**ä¸­å¿ƒè®ºæ–‡**:
{center_title} ({center_year})
- ç ”ç©¶é—®é¢˜: {problem}
- å±€é™æ€§: {limitation}

**æ¼”è¿›è·¯çº¿** (å…±{len(divergence['routes'])}æ¡è·¯çº¿):
{routes_text}

**ä»»åŠ¡è¦æ±‚**:
è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆå™äº‹æ–‡æœ¬ï¼ˆ3æ®µï¼Œæ¯æ®µ2-3å¥è¯ï¼‰ï¼š
1. **ç„¦ç‚¹**: æè¿°ä¸­å¿ƒè®ºæ–‡çš„åœ°ä½å’Œé—ç•™é—®é¢˜
2. **åˆ†æ­§**: æè¿°å„æ¡æ¼”è¿›è·¯çº¿çš„ä¸åŒæŠ€æœ¯æ–¹å‘
3. **å¯¹æ¯”**: æ€»ç»“è¿™äº›è·¯çº¿çš„å¼‚åŒå’Œå„è‡ªä¼˜åŠ¿

**è¾“å‡ºè¦æ±‚**:
- ä½¿ç”¨è¿è´¯ã€å­¦æœ¯åŒ–çš„ä¸­æ–‡è¡¨è¾¾
- çªå‡ºä¸åŒè·¯çº¿çš„æŠ€æœ¯å·®å¼‚å’Œåˆ›æ–°ç‚¹
- æ¯æ®µä»¥ **ç„¦ç‚¹**ã€**åˆ†æ­§**ã€**å¯¹æ¯”** ç­‰æ ‡é¢˜å¼€å¤´
"""

        try:
            narrative = self.llm_client.generate(
                prompt=prompt,
                temperature=0.3,
                max_tokens=600
            )
            return narrative.strip()
        except Exception as e:
            logger.warning(f"LLMç”Ÿæˆå™äº‹å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ¿æ–¹æ³•")
            return self._generate_divergence_narrative_template(graph, divergence, center_data)

    def _generate_convergence_narrative_text(
        self,
        graph: nx.DiGraph,
        convergence: Dict,
        center_data: Dict
    ) -> str:
        """
        ç”Ÿæˆæ±‡èšç»“æ„çš„å™äº‹æ–‡æœ¬

        Args:
            graph: å›¾è°±
            convergence: æ±‡èšç»“æ„
            center_data: ä¸­å¿ƒèŠ‚ç‚¹æ•°æ®

        Returns:
            å™äº‹æ–‡æœ¬
        """
        if self.llm_client:
            return self._generate_convergence_narrative_with_llm(graph, convergence, center_data)
        else:
            return self._generate_convergence_narrative_template(graph, convergence, center_data)

    def _generate_convergence_narrative_template(
        self,
        graph: nx.DiGraph,
        convergence: Dict,
        center_data: Dict
    ) -> str:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæ±‡èšå™äº‹ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        center_title = center_data.get('title', 'Unknown')
        center_year = center_data.get('year', 'N/A')
        center_method = self._extract_key_method(center_data)
        problem = self._extract_key_problem(center_data)

        narrative_parts = []

        # èƒŒæ™¯ï¼šæè¿°å¤šä¸ªç‹¬ç«‹çš„åŸºç¡€è·¯çº¿
        center_paper_id = convergence['center']
        routes_desc = []
        for i, route in enumerate(convergence['routes'], 1):
            route_papers = route['papers']
            relation = route['relation_type']

            # è·å–è·¯çº¿ä¸­æœ€æ–°è®ºæ–‡ï¼ˆæœ€æ¥è¿‘æ±‡èšä¸­å¿ƒçš„è®ºæ–‡ï¼‰
            latest_paper_id = route_papers[-1] if route_papers else None
            if latest_paper_id:
                # ä½¿ç”¨å¼•ç”¨ç±»å‹æ„ŸçŸ¥çš„ä¿¡æ¯æå–ï¼ˆè·¯çº¿â†’ä¸­å¿ƒæ–¹å‘ï¼‰
                info = self._extract_papers_info_for_relation(
                    graph, latest_paper_id, center_paper_id, relation
                )

                # æ ¹æ®å¼•ç”¨ç±»å‹ç”Ÿæˆé’ˆå¯¹æ€§çš„è·¯çº¿æè¿°
                if relation == 'Overcomes':
                    limitation = info.get('prev_limitation', 'æŸäº›å±€é™æ€§')
                    method = info.get('curr_method', 'æ–°æ–¹æ³•')
                    desc = f"**è·¯çº¿{i}** (å…‹æœå‹): è¯†åˆ«äº†ã€Œ{limitation}ã€ï¼Œä¸ºä¸­å¿ƒè®ºæ–‡æä¾›äº†å¾…è§£å†³çš„é—®é¢˜"
                elif relation == 'Realizes':
                    future_work = info.get('prev_future_work', 'æŸäº›æœªæ¥æ–¹å‘')
                    method = info.get('curr_method', 'å®ç°æ–¹æ³•')
                    desc = f"**è·¯çº¿{i}** (å®ç°å‹): æå‡ºäº†ã€Œ{future_work}ã€çš„ç ”ç©¶æ–¹å‘ï¼Œè¢«ä¸­å¿ƒè®ºæ–‡å®ç°"
                elif relation == 'Extends':
                    prev_method = info.get('prev_method', 'åŸºç¡€æ–¹æ³•')
                    curr_method = info.get('curr_method', 'æ‰©å±•æ–¹æ³•')
                    desc = f"**è·¯çº¿{i}** (æ‰©å±•å‹): æä¾›äº† {prev_method} çš„åŸºç¡€ï¼Œè¢«ä¸­å¿ƒè®ºæ–‡æ‰©å±•ä¸º {curr_method}"
                elif relation == 'Alternative':
                    prev_method = info.get('prev_method', 'æ›¿ä»£æ–¹æ³•')
                    curr_method = info.get('curr_method', 'ç»Ÿä¸€æ–¹æ³•')
                    desc = f"**è·¯çº¿{i}** (æ›¿ä»£å‹): æä¾›äº† {prev_method} ä½œä¸ºå¹³è¡Œæ–¹æ¡ˆï¼Œè¢«ä¸­å¿ƒè®ºæ–‡æ•´åˆ"
                elif relation == 'Adapts_to':
                    prev_domain = info.get('prev_domain', 'åŸå§‹é¢†åŸŸ')
                    curr_domain = info.get('curr_domain', 'æ–°é¢†åŸŸ')
                    desc = f"**è·¯çº¿{i}** (é€‚é…å‹): åœ¨ {prev_domain} ä¸­çš„å·¥ä½œï¼Œè¢«ä¸­å¿ƒè®ºæ–‡é€‚é…åˆ° {curr_domain}"
                elif relation == 'Baselines':
                    prev_method = info.get('prev_method', 'åŸºå‡†æ–¹æ³•')
                    desc = f"**è·¯çº¿{i}** (åŸºå‡†å‹): æä¾›äº† {prev_method} ä½œä¸ºæ¯”è¾ƒåŸºå‡†"
                else:
                    # é™çº§å¤„ç†
                    latest_paper = graph.nodes[latest_paper_id]
                    method = self._extract_key_method(latest_paper)
                    year = latest_paper.get('year', 'N/A')
                    desc = f"è·¯çº¿{i} åœ¨{year}å¹´æå‡ºäº† {method}"

                routes_desc.append(desc)

        narrative_parts.append(
            f"**èƒŒæ™¯**: åœ¨{center_year}å¹´ä¹‹å‰ï¼Œè¯¥é¢†åŸŸå­˜åœ¨å¤šä¸ªç‹¬ç«‹çš„ç ”ç©¶æ–¹å‘ã€‚" +
            "ï¼›".join(routes_desc) + "ã€‚è¿™äº›æ–¹å‘å„è‡ªä¸ºæ”¿ï¼Œç¼ºä¹ç³»ç»Ÿæ€§æ•´åˆã€‚"
        )

        # æ±‡èšï¼šæè¿°ä¸­å¿ƒè®ºæ–‡å¦‚ä½•æ•´åˆè¿™äº›æ–¹å‘
        narrative_parts.append(
            f"**æ±‡èš**: è®ºæ–‡ã€Š{center_title}ã€‹({center_year}) åœ¨æ­¤èƒŒæ™¯ä¸‹åº”è¿è€Œç”Ÿï¼Œ"
            f"å®ƒé€šè¿‡ {center_method} å°†è¿™ {len(convergence['routes'])} æ¡ç‹¬ç«‹è·¯çº¿æœ‰æœºæ•´åˆï¼Œ"
            f"å½¢æˆäº†ç»Ÿä¸€çš„æŠ€æœ¯æ¡†æ¶æ¥è§£å†³ {problem}ã€‚"
        )

        # æ„ä¹‰ï¼šæ€»ç»“æ•´åˆçš„ä»·å€¼
        integration_value = ""
        if len(convergence['routes']) >= 3:
            integration_value = "æ ‡å¿—ç€è¯¥é¢†åŸŸä»æ¢ç´¢é˜¶æ®µè¿ˆå…¥ç³»ç»ŸåŒ–é˜¶æ®µ"
        else:
            integration_value = "ä¸ºè¯¥é¢†åŸŸæä¾›äº†ç†è®ºæ•´åˆçš„èŒƒä¾‹"

        narrative_parts.append(
            f"**æ„ä¹‰**: è¿™ç§å¤šæ–¹å‘æ±‡èš{integration_value}ï¼Œ"
            f"ä½¿å¾—åŸæœ¬åˆ†æ•£çš„æŠ€æœ¯è·¯çº¿å¾—ä»¥ååŒå‘æŒ¥ä½œç”¨ï¼Œ"
            f"æ¨åŠ¨äº†é¢†åŸŸçš„ç†è®ºç»Ÿä¸€å’Œå®è·µæ·±åŒ–ã€‚"
        )

        return "\n\n".join(narrative_parts)

    def _generate_convergence_narrative_with_llm(
        self,
        graph: nx.DiGraph,
        convergence: Dict,
        center_data: Dict
    ) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆæ±‡èšå™äº‹"""
        center_title = center_data.get('title', '')
        center_year = center_data.get('year', '')
        center_method = self._extract_key_method(center_data)
        problem = self._extract_key_problem(center_data)

        # å‡†å¤‡å„è·¯çº¿çš„ä¸Šä¸‹æ–‡
        routes_context = []
        for i, route in enumerate(convergence['routes'], 1):
            relation = route['relation_type']
            route_papers = []
            for paper_id in route['papers'][:2]:  # æ¯æ¡è·¯çº¿æœ€å¤š2ç¯‡
                node_data = graph.nodes[paper_id]
                route_papers.append(
                    f"  - {node_data.get('title', '')} ({node_data.get('year', '')}): "
                    f"{self._extract_key_method(node_data)}"
                )

            routes_context.append(
                f"è·¯çº¿{i} ({relation}):\n" + "\n".join(route_papers)
            )

        routes_text = "\n\n".join(routes_context)

        prompt = f"""ä½ æ˜¯ä¸€ä½å­¦æœ¯ç»¼è¿°ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ±‡èšæ¼”è¿›ç»“æ„ï¼Œç”Ÿæˆä¸€æ®µé€šé¡ºçš„å™äº‹æ–‡æœ¬ã€‚

**ä¸­å¿ƒè®ºæ–‡**:
{center_title} ({center_year})
- ç ”ç©¶é—®é¢˜: {problem}
- æ•´åˆæ–¹æ³•: {center_method}

**åŸºç¡€è·¯çº¿** (å…±{len(convergence['routes'])}æ¡ç‹¬ç«‹è·¯çº¿è¢«æ•´åˆ):
{routes_text}

**ä»»åŠ¡è¦æ±‚**:
è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆå™äº‹æ–‡æœ¬ï¼ˆ3æ®µï¼Œæ¯æ®µ2-3å¥è¯ï¼‰ï¼š
1. **èƒŒæ™¯**: æè¿°ä¸­å¿ƒè®ºæ–‡ä¹‹å‰å¤šä¸ªç‹¬ç«‹æ–¹å‘çš„æ¢ç´¢
2. **æ±‡èš**: æè¿°ä¸­å¿ƒè®ºæ–‡å¦‚ä½•æ•´åˆè¿™äº›æ–¹å‘å½¢æˆç»Ÿä¸€æ¡†æ¶
3. **æ„ä¹‰**: æ€»ç»“è¿™ç§æ•´åˆå¯¹é¢†åŸŸå‘å±•çš„ä»·å€¼

**è¾“å‡ºè¦æ±‚**:
- ä½¿ç”¨è¿è´¯ã€å­¦æœ¯åŒ–çš„ä¸­æ–‡è¡¨è¾¾
- çªå‡ºä»åˆ†æ•£åˆ°æ•´åˆçš„æ¼”è¿›ç‰¹ç‚¹
- å¼ºè°ƒæ•´åˆåçš„ååŒæ•ˆåº”å’Œç†è®ºä»·å€¼
- æ¯æ®µä»¥ **èƒŒæ™¯**ã€**æ±‡èš**ã€**æ„ä¹‰** ç­‰æ ‡é¢˜å¼€å¤´
"""

        try:
            narrative = self.llm_client.generate(
                prompt=prompt,
                temperature=0.3,
                max_tokens=600
            )
            return narrative.strip()
        except Exception as e:
            logger.warning(f"LLMç”Ÿæˆå™äº‹å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ¿æ–¹æ³•")
            return self._generate_convergence_narrative_template(graph, convergence, center_data)

    def _generate_survey_report(
        self,
        topic: str,
        pruned_graph: nx.DiGraph,
        evolutionary_paths: List[Dict],
        pruning_stats: Dict
    ) -> Dict:
        """
        ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç»“æ„åŒ– Deep Survey æŠ¥å‘Š

        Args:
            topic: ç ”ç©¶ä¸»é¢˜
            pruned_graph: å‰ªæåçš„å›¾è°±
            evolutionary_paths: æ¼”åŒ–è·¯å¾„åˆ—è¡¨
            pruning_stats: å‰ªæç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»¼è¿°æŠ¥å‘Šå­—å…¸
        """
        logger.info("  æ­£åœ¨ç”Ÿæˆ Deep Survey æŠ¥å‘Š...")

        report = {
            'title': f"Deep Survey: {topic}",
            'abstract': self._generate_abstract(topic, evolutionary_paths, pruning_stats),
            'threads': [],
            'metadata': {
                'total_papers_analyzed': pruning_stats['original_papers'],
                'papers_after_pruning': pruning_stats['pruned_papers'],
                'total_threads': len(evolutionary_paths),
                'generation_date': datetime.now().isoformat()
            }
        }

        # ä¸ºæ¯ä¸ªæ¼”åŒ–è·¯å¾„ç”Ÿæˆ Thread
        for i, path in enumerate(evolutionary_paths, 1):
            # æå–å…³ç³»ç±»å‹ç»Ÿè®¡
            relation_stats = self._extract_relation_stats(path, pruned_graph)

            thread = {
                'thread_id': i,
                'thread_name': f"Thread {i}: {path['pattern_type']}",
                'title': path['title'],
                'pattern_type': path['pattern_type'],
                'thread_type': path.get('thread_type', 'unknown'),  # æ–°å¢ï¼šçº¿ç¨‹ç±»å‹
                'narrative': path['narrative'],
                'papers': path['papers'],
                'total_citations': path.get('total_citations', 0),
                'visual_structure': path.get('visual_structure', ''),
                'relation_stats': relation_stats,  # å…³ç³»ç»Ÿè®¡
                'relation_chain': path.get('relation_chain', []),  # æ–°å¢ï¼šè¯¦ç»†å…³ç³»é“¾
                # ä¸ºå¯è§†åŒ–å‡†å¤‡çš„æ•°æ®
                'visualization_data': self._prepare_visualization_data(path, pruned_graph)
            }
            report['threads'].append(thread)

        logger.info(f"    âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {len(report['threads'])} ä¸ª Threads")

        return report

    def _generate_abstract(
        self,
        topic: str,
        evolutionary_paths: List[Dict],
        pruning_stats: Dict
    ) -> str:
        """ç”Ÿæˆç»¼è¿°æ‘˜è¦"""
        total_papers = pruning_stats['pruned_papers']
        total_threads = len(evolutionary_paths)

        chains = sum(1 for p in evolutionary_paths if p['thread_type'] == 'chain')
        divergences = sum(1 for p in evolutionary_paths if p['thread_type'] == 'divergence')
        convergences = sum(1 for p in evolutionary_paths if p['thread_type'] == 'convergence')

        abstract = (
            f"æœ¬ç»¼è¿°åŸºäºçŸ¥è¯†å›¾è°±åˆ†æäº† {topic} é¢†åŸŸçš„æ¼”è¿›å†ç¨‹ã€‚"
            f"é€šè¿‡å…³ç³»å‰ªæï¼Œæˆ‘ä»¬ä»åŸå§‹å›¾è°±ä¸­ç­›é€‰å‡º {total_papers} ç¯‡é«˜è´¨é‡è®ºæ–‡ï¼Œ"
            f"å¹¶è¯†åˆ«å‡º {total_threads} æ¡å…³é”®æ¼”åŒ–è·¯å¾„ã€‚"
            f"å…¶ä¸­åŒ…æ‹¬ {chains} æ¡çº¿æ€§æŠ€æœ¯é“¾æ¡ã€{divergences} ä¸ªåˆ†åŒ–ç»“æ„å’Œ {convergences} ä¸ªæ±‡èšç»“æ„ï¼Œ"
            f"å®Œæ•´å‘ˆç°äº†è¯¥é¢†åŸŸçš„æŠ€æœ¯æ¼”è¿›è„‰ç»œã€åˆ†åŒ–è¶‹åŠ¿å’Œæ•´åˆæ¨¡å¼ã€‚"
        )

        return abstract

    def _prepare_visualization_data(self, path: Dict, graph: nx.DiGraph) -> Dict:
        """
        å‡†å¤‡å¯è§†åŒ–æ•°æ®

        Bug Fix #3: ç¡®ä¿ç®­å¤´æ–¹å‘è¡¨ç¤º"å‘å±•æŒ‡å‘"ï¼ˆæ—¶é—´æ­£åºï¼šæ—©å¹´ä»½->æ™šå¹´ä»½ï¼‰

        Args:
            path: æ¼”åŒ–è·¯å¾„
            graph: å›¾è°±

        Returns:
            å¯è§†åŒ–æ•°æ®å­—å…¸
        """
        papers = path['papers']

        # æå–èŠ‚ç‚¹å’Œè¾¹
        nodes = []
        edges = []

        if path['thread_type'] == 'chain':
            # çº¿æ€§é“¾æ¡ - æŒ‰æ—¶é—´æ’åºåå¯è§†åŒ–
            # æŒ‰å¹´ä»½æ’åºè®ºæ–‡
            sorted_papers = sorted(papers, key=lambda p: p.get('year', 0))

            for i, paper_info in enumerate(sorted_papers):
                paper_id = paper_info['paper_id']
                node_data = graph.nodes.get(paper_id, {})

                nodes.append({
                    'id': paper_id,
                    'label': f"Paper {i+1}",
                    'title': paper_info.get('title', ''),
                    'year': paper_info.get('year', 0),
                    'citations': paper_info.get('cited_by_count', 0)
                })

                if i > 0:
                    prev_paper_id = sorted_papers[i-1]['paper_id']
                    # Bug Fix #3: ç®­å¤´æ–¹å‘ = æ—¶é—´æµå‘ï¼ˆæ—©->æ™šï¼‰
                    edges.append({
                        'source': prev_paper_id,  # æ—©å¹´ä»½
                        'target': paper_id,        # æ™šå¹´ä»½
                        'type': 'chronological_evolution',
                        'label': f"{sorted_papers[i-1].get('year', '')} â†’ {paper_info.get('year', '')}"
                    })

        elif path['thread_type'] in ['divergence', 'convergence']:
            # åˆ†åŒ–/æ±‡èšç»“æ„
            # ä¸­å¿ƒèŠ‚ç‚¹
            center_paper = next((p for p in papers if p.get('role') == 'center'), papers[0])
            nodes.append({
                'id': center_paper['paper_id'],
                'label': 'Center',
                'title': center_paper.get('title', ''),
                'year': center_paper.get('year', 0),
                'citations': center_paper.get('cited_by_count', 0),
                'role': 'center'
            })

            # è·¯çº¿èŠ‚ç‚¹
            for route in path.get('routes', []):
                for paper_info in route['papers']:
                    paper_id = paper_info['paper_id']
                    nodes.append({
                        'id': paper_id,
                        'label': paper_info.get('title', '')[:30] + '...',
                        'title': paper_info.get('title', ''),
                        'year': paper_info.get('year', 0),
                        'citations': paper_info.get('cited_by_count', 0)
                    })

                    # å¤„ç†ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä¸ä¸­å¿ƒçš„è¿æ¥
                    if paper_info == route['papers'][0]:
                        center_year = center_paper.get('year', 0)
                        target_year = paper_info.get('year', 0)

                        # æ ¹æ®æ¨¡å¼ç±»å‹å’Œæ—¶é—´å…³ç³»ç¡®å®šç®­å¤´æ–¹å‘
                        if path['thread_type'] == 'divergence':
                            # åˆ†åŒ–ï¼šä¸­å¿ƒ -> è·¯çº¿ï¼ˆä¸­å¿ƒè®ºæ–‡æ›´æ—©ï¼‰
                            if center_year <= target_year:
                                edges.append({
                                    'source': center_paper['paper_id'],
                                    'target': paper_id,
                                    'type': route.get('relation_type', 'related'),
                                    'direction': 'forward'
                                })
                            else:
                                # å¼‚å¸¸æƒ…å†µï¼šåå‘
                                edges.append({
                                    'source': paper_id,
                                    'target': center_paper['paper_id'],
                                    'type': f"Inspired_{route.get('relation_type', 'related')}",
                                    'direction': 'backward'
                                })
                        else:
                            # æ±‡èšï¼šè·¯çº¿ -> ä¸­å¿ƒï¼ˆä¸­å¿ƒè®ºæ–‡æ›´æ™šï¼‰
                            if center_year >= target_year:
                                edges.append({
                                    'source': paper_id,
                                    'target': center_paper['paper_id'],
                                    'type': route.get('relation_type', 'related'),
                                    'direction': 'forward'
                                })
                            else:
                                # å¼‚å¸¸æƒ…å†µï¼šåå‘
                                edges.append({
                                    'source': center_paper['paper_id'],
                                    'target': paper_id,
                                    'type': f"Extends_{route.get('relation_type', 'related')}",
                                    'direction': 'backward'
                                })

        return {
            'nodes': nodes,
            'edges': edges,
            'layout': 'hierarchical' if path['thread_type'] == 'chain' else 'radial',
            'direction_note': 'ç®­å¤´æ–¹å‘è¡¨ç¤ºæ—¶é—´æ¼”è¿›æ–¹å‘ï¼ˆæ—©å¹´ä»½ â†’ æ™šå¹´ä»½ï¼‰',
            'pattern_note': 'divergence=ä¸­å¿ƒæ‰©æ•£, convergence=å¤šæºæ±‡èš' if path['thread_type'] in ['divergence', 'convergence'] else ''
        }

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _reverse_relation_semantics(self, relation_type: str) -> str:
        """
        åè½¬å…³ç³»è¯­ä¹‰ä»¥ç¬¦åˆæ—¶é—´æ­£åºå™äº‹

        åœ¨çŸ¥è¯†å›¾è°±ä¸­ï¼šæ–°è®ºæ–‡(2023) --Overcomes--> æ—§è®ºæ–‡(2021)
        åœ¨æ—¶é—´å™äº‹ä¸­ï¼šæ—§è®ºæ–‡(2021) --Was_Overcome_By--> æ–°è®ºæ–‡(2023)

        Args:
            relation_type: åŸå§‹å…³ç³»ç±»å‹ï¼ˆä»æ–°è®ºæ–‡æŒ‡å‘æ—§è®ºæ–‡ï¼‰

        Returns:
            åè½¬åçš„å…³ç³»æè¿°ï¼ˆä»æ—§è®ºæ–‡æŒ‡å‘æ–°è®ºæ–‡ï¼‰
        """
        relation_mapping = {
            'Overcomes': 'Was_Overcome_By',      # è¢«å…‹æœ
            'Realizes': 'Inspired',              # å¯å‘äº†
            'Extends': 'Was_Extended_By',        # è¢«æ‰©å±•
            'Alternative': 'Led_To_Alternative', # å¯¼è‡´æ›¿ä»£æ–¹æ¡ˆ
            'Adapts_to': 'Was_Adapted_By',       # è¢«è¿ç§»
            'Baselines': 'Served_As_Baseline',   # ä½œä¸ºåŸºçº¿
        }

        return relation_mapping.get(relation_type, f'Led_To_{relation_type}')

    def _map_reversed_to_original_type(self, reversed_type: str) -> str:
        """
        å°†åè½¬åçš„å…³ç³»ç±»å‹æ˜ å°„å›åŸå§‹ç±»å‹ï¼ˆç”¨äºå™äº‹ç”Ÿæˆï¼‰

        Args:
            reversed_type: åè½¬åçš„å…³ç³»ç±»å‹ï¼ˆå¦‚ 'Was_Overcome_By'ï¼‰

        Returns:
            åŸå§‹å…³ç³»ç±»å‹ï¼ˆå¦‚ 'Overcomes'ï¼‰
        """
        mapping = {
            'Was_Overcome_By': 'Overcomes',
            'Inspired': 'Realizes',
            'Was_Extended_By': 'Extends',
            'Led_To_Alternative': 'Alternative',
            'Was_Adapted_By': 'Adapts_to',
            'Served_As_Baseline': 'Baselines'
        }
        return mapping.get(reversed_type, 'Baselines')

    def _get_relation_type(self, graph: nx.DiGraph, prev_paper_id: str, curr_paper_id: str) -> str:
        """
        è·å–ä¸¤ç¯‡è®ºæ–‡ä¹‹é—´çš„å…³ç³»ç±»å‹ï¼ˆå¤„ç†æ­£å‘å’Œåå‘è¾¹ï¼‰

        Args:
            graph: å›¾è°±
            prev_paper_id: æ—©æœŸè®ºæ–‡ID
            curr_paper_id: æ™šæœŸè®ºæ–‡ID

        Returns:
            å…³ç³»ç±»å‹
        """
        if graph.has_edge(prev_paper_id, curr_paper_id):
            edge_data = graph.edges[prev_paper_id, curr_paper_id]
            return edge_data.get('type') or edge_data.get('edge_type', 'Baselines')
        elif graph.has_edge(curr_paper_id, prev_paper_id):
            edge_data = graph.edges[curr_paper_id, prev_paper_id]
            original_type = edge_data.get('type') or edge_data.get('edge_type', 'Baselines')
            reversed_type = self._reverse_relation_semantics(original_type)
            return self._map_reversed_to_original_type(reversed_type)
        else:
            return 'Baselines'

    def _get_relation_focus_hint(self, relation_type: str) -> str:
        """
        è·å–å¼•ç”¨ç±»å‹çš„å™äº‹å…³æ³¨ç‚¹æç¤ºï¼ˆç”¨äºLLM Promptï¼‰

        Args:
            relation_type: å¼•ç”¨å…³ç³»ç±»å‹

        Returns:
            å…³æ³¨ç‚¹æè¿°
        """
        hints = {
            'Overcomes': 'å…³æ³¨å‰äººçš„å±€é™æ€§å¦‚ä½•è¢«å…‹æœ',
            'Realizes': 'å…³æ³¨å‰äººçš„æ„¿æ™¯å¦‚ä½•è¢«å®ç°',
            'Adapts_to': 'å…³æ³¨æ–¹æ³•å¦‚ä½•è·¨é¢†åŸŸè¿ç§»',
            'Extends': 'å…³æ³¨æ–¹æ³•å¦‚ä½•è¢«å¢é‡æ”¹è¿›',
            'Alternative': 'å…³æ³¨ä¸åŒæŠ€æœ¯èŒƒå¼çš„å¯¹æ¯”',
            'Baselines': 'ä½œä¸ºèƒŒæ™¯é“ºå«'
        }
        return hints.get(relation_type, 'ç»§æ‰¿å‰äººå·¥ä½œ')

    def _get_relation_description(self, graph: nx.DiGraph, old_paper_id: str, new_paper_id: str) -> str:
        """
        è·å–ä¸¤ç¯‡è®ºæ–‡ä¹‹é—´çš„å…³ç³»æè¿°ï¼ˆç”¨äºå™äº‹ï¼‰

        Args:
            graph: å›¾è°±
            old_paper_id: æ—©æœŸè®ºæ–‡ID
            new_paper_id: æ™šæœŸè®ºæ–‡ID

        Returns:
            ç¬¦åˆæ—¶é—´å™äº‹çš„å…³ç³»æè¿°
        """
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¾¹
        if graph.has_edge(old_paper_id, new_paper_id):
            # æ—©è®ºæ–‡ -> æ™šè®ºæ–‡ï¼ˆæ­£å‘ï¼‰
            edge_data = graph.edges[old_paper_id, new_paper_id]
            relation_type = edge_data.get('type') or edge_data.get('edge_type', 'Unknown')
            return self._get_chinese_relation_desc(relation_type, is_reversed=False)

        elif graph.has_edge(new_paper_id, old_paper_id):
            # æ™šè®ºæ–‡ -> æ—©è®ºæ–‡ï¼ˆéœ€è¦åè½¬è¯­ä¹‰ï¼‰
            edge_data = graph.edges[new_paper_id, old_paper_id]
            relation_type = edge_data.get('type') or edge_data.get('edge_type', 'Unknown')
            return self._get_chinese_relation_desc(relation_type, is_reversed=True)

        else:
            return "åœ¨å‰äººå·¥ä½œåŸºç¡€ä¸Š"

    def _get_chinese_relation_desc(self, relation_type: str, is_reversed: bool) -> str:
        """
        è·å–å…³ç³»çš„ä¸­æ–‡æè¿°

        Args:
            relation_type: å…³ç³»ç±»å‹
            is_reversed: æ˜¯å¦éœ€è¦åè½¬è¯­ä¹‰

        Returns:
            ä¸­æ–‡æè¿°
        """
        if is_reversed:
            # åå‘å…³ç³»ï¼ˆæ™šè®ºæ–‡ -> æ—©è®ºæ–‡ï¼‰éœ€è¦åè½¬è¯­ä¹‰
            descriptions = {
                'Overcomes': 'é’ˆå¯¹å‰äººçš„å±€é™æ€§è¿›è¡Œäº†æ”¹è¿›',
                'Realizes': 'å®ç°äº†å‰äººæå‡ºçš„æ„¿æ™¯',
                'Extends': 'æ‰©å±•äº†å‰äººçš„æ–¹æ³•',
                'Alternative': 'æå‡ºäº†ä¸åŒäºå‰äººçš„æ›¿ä»£æ–¹æ¡ˆ',
                'Adapts_to': 'å°†å‰äººçš„æŠ€æœ¯è¿ç§»åˆ°æ–°é¢†åŸŸ',
                'Baselines': 'åœ¨å‰äººå·¥ä½œçš„åŸºç¡€ä¸Š',
            }
        else:
            # æ­£å‘å…³ç³»ï¼ˆæ—©è®ºæ–‡ -> æ™šè®ºæ–‡ï¼‰
            descriptions = {
                'Inspires': 'å¯å‘äº†åç»­ç ”ç©¶',
                'Proposes': 'æå‡ºäº†åç»­ç ”ç©¶çš„æ–¹å‘',
                'Enables': 'ä½¿åç»­ç ”ç©¶æˆä¸ºå¯èƒ½',
            }

        return descriptions.get(relation_type, 'åœ¨å‰äººå·¥ä½œåŸºç¡€ä¸Š')

    def _extract_relation_stats(self, path: Dict, graph: nx.DiGraph) -> Dict:
        """
        æå–æ¼”åŒ–è·¯å¾„ä¸­çš„å…³ç³»ç±»å‹ç»Ÿè®¡

        Args:
            path: æ¼”åŒ–è·¯å¾„
            graph: å›¾è°±

        Returns:
            å…³ç³»ç»Ÿè®¡å­—å…¸
        """
        relation_count = {}

        if path['thread_type'] == 'chain':
            # é“¾æ¡ï¼šç»Ÿè®¡ç›¸é‚»è®ºæ–‡é—´çš„å…³ç³»
            papers = path['papers']
            for i in range(len(papers) - 1):
                paper_id = papers[i]['paper_id']
                next_paper_id = papers[i + 1]['paper_id']

                if graph.has_edge(paper_id, next_paper_id):
                    edge_data = graph.edges[paper_id, next_paper_id]
                    edge_type = edge_data.get('type') or edge_data.get('edge_type', 'Unknown')
                    relation_count[edge_type] = relation_count.get(edge_type, 0) + 1

        elif path['thread_type'] in ['divergence', 'convergence']:
            # åˆ†åŒ–/æ±‡èšç»“æ„ï¼šç»Ÿè®¡å„æ¡è·¯çº¿çš„å…³ç³»ç±»å‹
            for route in path.get('routes', []):
                relation_type = route.get('relation_type', 'Unknown')
                relation_count[relation_type] = relation_count.get(relation_type, 0) + 1

        return {
            'total_relations': sum(relation_count.values()),
            'relation_distribution': relation_count,
            'dominant_relation': max(relation_count.items(), key=lambda x: x[1])[0] if relation_count else 'Unknown'
        }

    def _extract_key_method(self, node_data: Dict) -> str:
        """æå–è®ºæ–‡çš„å…³é”®æ–¹æ³•"""
        # ä¼˜å…ˆä» deep_analysis æå–
        deep_analysis = node_data.get('deep_analysis', {})
        if isinstance(deep_analysis, dict):
            method = deep_analysis.get('method', {})
            # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼šå­—å…¸ {content: ...} æˆ–ç›´æ¥å­—ç¬¦ä¸²
            if isinstance(method, dict):
                method_text = method.get('content', '')
            else:
                method_text = str(method) if method else ''

            if method_text:
                # åªå–ç¬¬ä¸€å¥è¯ï¼Œé¿å…å¤ªé•¿
                first_sentence = method_text.split('\n')[0].strip()
                return first_sentence[:80] if len(first_sentence) > 80 else first_sentence

        # ä» rag_analysis æå–
        rag_analysis = node_data.get('rag_analysis', {})
        if isinstance(rag_analysis, dict):
            method = rag_analysis.get('method', '')
            if method:
                first_sentence = str(method).split('\n')[0].strip()
                return first_sentence[:80] if len(first_sentence) > 80 else first_sentence

        return 'æœªæå–åˆ°æ–¹æ³•'

    def _extract_key_problem(self, node_data: Dict) -> str:
        """æå–è®ºæ–‡çš„å…³é”®é—®é¢˜"""
        deep_analysis = node_data.get('deep_analysis', {})
        if isinstance(deep_analysis, dict):
            problem = deep_analysis.get('problem', {})
            # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼šå­—å…¸ {content: ...} æˆ–ç›´æ¥å­—ç¬¦ä¸²
            if isinstance(problem, dict):
                problem_text = problem.get('content', '')
            else:
                problem_text = str(problem) if problem else ''

            if problem_text:
                first_sentence = problem_text.split('\n')[0].strip()
                return first_sentence[:80] if len(first_sentence) > 80 else first_sentence

        rag_analysis = node_data.get('rag_analysis', {})
        if isinstance(rag_analysis, dict):
            problem = rag_analysis.get('problem', '')
            if problem:
                first_sentence = str(problem).split('\n')[0].strip()
                return first_sentence[:80] if len(first_sentence) > 80 else first_sentence

        return 'æœªæå–åˆ°é—®é¢˜'

    def _extract_key_limitation(self, node_data: Dict) -> str:
        """æå–è®ºæ–‡çš„å…³é”®å±€é™æ€§"""
        deep_analysis = node_data.get('deep_analysis', {})
        if isinstance(deep_analysis, dict):
            limitation = deep_analysis.get('limitation', {})
            # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼šå­—å…¸ {content: ...} æˆ–ç›´æ¥å­—ç¬¦ä¸²
            if isinstance(limitation, dict):
                limitation_text = limitation.get('content', '')
            else:
                limitation_text = str(limitation) if limitation else ''

            if limitation_text:
                first_sentence = limitation_text.split('\n')[0].strip()
                return first_sentence[:80] if len(first_sentence) > 80 else first_sentence

        rag_analysis = node_data.get('rag_analysis', {})
        if isinstance(rag_analysis, dict):
            limitation = rag_analysis.get('limitation', '')
            if limitation:
                first_sentence = str(limitation).split('\n')[0].strip()
                return first_sentence[:80] if len(first_sentence) > 80 else first_sentence

        return 'æœªæå–åˆ°å±€é™æ€§'

    def _extract_key_future_work(self, node_data: Dict) -> str:
        """
        æå–è®ºæ–‡çš„æœªæ¥å·¥ä½œæ–¹å‘ï¼ˆç”¨äºRealizeså…³ç³»ï¼‰

        Args:
            node_data: è®ºæ–‡èŠ‚ç‚¹æ•°æ®

        Returns:
            æœªæ¥å·¥ä½œæè¿°ï¼ˆåªå–ç¬¬ä¸€å¥ï¼Œé™åˆ¶80å­—ç¬¦ï¼‰
        """
        # ä¼˜å…ˆä» deep_analysis æå–
        deep_analysis = node_data.get('deep_analysis', {})
        if isinstance(deep_analysis, dict):
            future_work = deep_analysis.get('future_work', {})
            # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼šå­—å…¸ {content: ...} æˆ–ç›´æ¥å­—ç¬¦ä¸²
            if isinstance(future_work, dict):
                future_work_text = future_work.get('content', '')
            else:
                future_work_text = str(future_work) if future_work else ''

            # è¿‡æ»¤æ‰ "N/A" æˆ–ç©ºå€¼
            if future_work_text and future_work_text != "N/A":
                first_sentence = future_work_text.split('ã€‚')[0].strip()
                if not first_sentence:
                    first_sentence = future_work_text.split('\n')[0].strip()
                return first_sentence[:80] if len(first_sentence) > 80 else first_sentence

        # å¤‡ç”¨ï¼šä» rag_analysis æå–
        rag_analysis = node_data.get('rag_analysis', {})
        if isinstance(rag_analysis, dict):
            future_work = rag_analysis.get('future_work', '')
            if future_work and future_work != "N/A":
                first_sentence = str(future_work).split('ã€‚')[0].strip()
                if not first_sentence:
                    first_sentence = str(future_work).split('\n')[0].strip()
                return first_sentence[:80] if len(first_sentence) > 80 else first_sentence

        return 'æœªæå–åˆ°æœªæ¥å·¥ä½œ'

    def _extract_papers_info_for_relation(
        self,
        graph: nx.DiGraph,
        prev_paper_id: str,
        curr_paper_id: str,
        relation_type: str
    ) -> Dict[str, str]:
        """
        æ ¹æ®å¼•ç”¨ç±»å‹æå–è®ºæ–‡çš„ç›¸å…³ä¿¡æ¯

        Args:
            graph: å›¾è°±
            prev_paper_id: å‰ä¸€ç¯‡ï¼ˆæ—©æœŸï¼‰è®ºæ–‡ID
            curr_paper_id: å½“å‰ï¼ˆæ™šæœŸï¼‰è®ºæ–‡ID
            relation_type: å¼•ç”¨å…³ç³»ç±»å‹

        Returns:
            åŒ…å«å™äº‹æ‰€éœ€ä¿¡æ¯çš„å­—å…¸
        """
        prev_node = graph.nodes[prev_paper_id]
        curr_node = graph.nodes[curr_paper_id]

        info = {
            'prev_title': prev_node.get('title', 'Unknown'),
            'prev_year': prev_node.get('year', 'N/A'),
            'curr_title': curr_node.get('title', 'Unknown'),
            'curr_year': curr_node.get('year', 'N/A'),
            'relation_type': relation_type
        }

        # æ ¹æ®å¼•ç”¨ç±»å‹æå–ä¸åŒçš„è¦ç´ 
        if relation_type == 'Overcomes':
            # Overcomes: A.Limitation â†’ B.Problem + Method
            info['prev_limitation'] = self._extract_key_limitation(prev_node)
            info['curr_problem'] = self._extract_key_problem(curr_node)
            info['curr_method'] = self._extract_key_method(curr_node)

        elif relation_type == 'Realizes':
            # Realizes: A.Future_Work â†’ B.Problem + Method
            info['prev_future_work'] = self._extract_key_future_work(prev_node)
            info['curr_problem'] = self._extract_key_problem(curr_node)
            info['curr_method'] = self._extract_key_method(curr_node)

        elif relation_type == 'Adapts_to':
            # Adapts_to: A.Problem+Method â†’ B.Problem+Method (è·¨åŸŸ)
            info['prev_problem'] = self._extract_key_problem(prev_node)
            info['prev_method'] = self._extract_key_method(prev_node)
            info['curr_problem'] = self._extract_key_problem(curr_node)
            info['curr_method'] = self._extract_key_method(curr_node)
            # å°è¯•ä»deep_analysisæå–é¢†åŸŸä¿¡æ¯
            prev_deep = prev_node.get('deep_analysis', {})
            curr_deep = curr_node.get('deep_analysis', {})
            info['prev_domain'] = prev_deep.get('domain', 'åŸé¢†åŸŸ')
            info['curr_domain'] = curr_deep.get('domain', 'æ–°é¢†åŸŸ')

        elif relation_type == 'Extends':
            # Extends: A.Method â†’ B.Method (æ‰©å±•)
            info['prev_method'] = self._extract_key_method(prev_node)
            info['curr_method'] = self._extract_key_method(curr_node)
            info['curr_problem'] = self._extract_key_problem(curr_node)

        elif relation_type == 'Alternative':
            # Alternative: A.Method â†’ B.Method (ä¸åŒèŒƒå¼)
            info['prev_method'] = self._extract_key_method(prev_node)
            info['curr_method'] = self._extract_key_method(curr_node)
            info['prev_problem'] = self._extract_key_problem(prev_node)
            info['curr_problem'] = self._extract_key_problem(curr_node)

        else:  # Baselines æˆ–å…¶ä»–
            # é»˜è®¤æå–é€šç”¨ä¿¡æ¯
            info['prev_method'] = self._extract_key_method(prev_node)
            info['curr_method'] = self._extract_key_method(curr_node)
            info['curr_problem'] = self._extract_key_problem(curr_node)

        return info

    def _generate_relation_narrative_fragment(self, info: Dict[str, str]) -> str:
        """
        æ ¹æ®å¼•ç”¨ç±»å‹ç”Ÿæˆé’ˆå¯¹æ€§çš„å™äº‹ç‰‡æ®µ

        Args:
            info: é€šè¿‡ _extract_papers_info_for_relation æå–çš„ä¿¡æ¯å­—å…¸

        Returns:
            å™äº‹ç‰‡æ®µæ–‡æœ¬
        """
        relation_type = info.get('relation_type', 'Baselines')
        curr_year = info.get('curr_year', 'N/A')
        curr_title = info.get('curr_title', 'Unknown')

        if relation_type == 'Overcomes':
            # å…³æ³¨ï¼šAçš„å±€é™æ€§ â†’ Bå¦‚ä½•è§£å†³
            prev_limitation = info.get('prev_limitation', 'æŸäº›å±€é™æ€§')
            curr_method = info.get('curr_method', 'æ–°æ–¹æ³•')
            return (
                f"**å…‹æœå±€é™** ({curr_year}å¹´): é’ˆå¯¹å‰äººå·¥ä½œåœ¨ã€Œ{prev_limitation}ã€æ–¹é¢çš„ä¸è¶³ï¼Œ"
                f"è®ºæ–‡ã€Š{curr_title}ã€‹é€šè¿‡ {curr_method} å®ç°äº†çªç ´æ€§æ”¹è¿›ã€‚"
            )

        elif relation_type == 'Realizes':
            # å…³æ³¨ï¼šAçš„æ„¿æ™¯ â†’ Bå¦‚ä½•å®ç°
            prev_future_work = info.get('prev_future_work', 'å‰äººçš„è®¾æƒ³')
            curr_method = info.get('curr_method', 'æ–°æ–¹æ³•')
            return (
                f"**å®ç°æ„¿æ™¯** ({curr_year}å¹´): å‘¼åº”å‰äººã€Œ{prev_future_work}ã€çš„å±•æœ›ï¼Œ"
                f"è®ºæ–‡ã€Š{curr_title}ã€‹é€šè¿‡ {curr_method} å°†è¿™ä¸€è®¾æƒ³ä»˜è¯¸å®è·µã€‚"
            )

        elif relation_type == 'Adapts_to':
            # å…³æ³¨ï¼šAçš„é¢†åŸŸ â†’ Bçš„é¢†åŸŸ
            prev_domain = info.get('prev_domain', 'åŸé¢†åŸŸ')
            curr_domain = info.get('curr_domain', 'æ–°é¢†åŸŸ')
            curr_method = info.get('curr_method', 'æ”¹è¿›æ–¹æ³•')
            return (
                f"**è·¨åŸŸè¿ç§»** ({curr_year}å¹´): è®ºæ–‡ã€Š{curr_title}ã€‹å°†å‰äººåœ¨ã€Œ{prev_domain}ã€çš„æŠ€æœ¯"
                f"æˆåŠŸè¿ç§»åˆ°ã€Œ{curr_domain}ã€ï¼Œé€šè¿‡ {curr_method} éªŒè¯äº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚"
            )

        elif relation_type == 'Extends':
            # å…³æ³¨ï¼šAçš„æ–¹æ³• â†’ Bå¦‚ä½•å¢å¼º
            prev_method = info.get('prev_method', 'åŸºç¡€æ–¹æ³•')
            curr_method = info.get('curr_method', 'æ”¹è¿›æ–¹æ³•')
            return (
                f"**å¢é‡æ‰©å±•** ({curr_year}å¹´): è®ºæ–‡ã€Š{curr_title}ã€‹åœ¨ã€Œ{prev_method}ã€çš„åŸºç¡€ä¸Šï¼Œ"
                f"é€šè¿‡ {curr_method} å®ç°äº†æ¸è¿›å¼æ”¹è¿›ã€‚"
            )

        elif relation_type == 'Alternative':
            # å…³æ³¨ï¼šAçš„èŒƒå¼ â†’ Bçš„ä¸åŒèŒƒå¼
            prev_method = info.get('prev_method', 'åŸæœ‰æ–¹æ³•')
            curr_method = info.get('curr_method', 'æ›¿ä»£æ–¹æ³•')
            return (
                f"**å¦è¾Ÿè¹Šå¾„** ({curr_year}å¹´): ä¸åŒäºå‰äººã€Œ{prev_method}ã€çš„æ€è·¯ï¼Œ"
                f"è®ºæ–‡ã€Š{curr_title}ã€‹æå‡ºäº† {curr_method}ï¼Œæ¢ç´¢äº†è§£å†³é—®é¢˜çš„æ–°èŒƒå¼ã€‚"
            )

        else:  # Baselines
            # è½»é‡çº§æè¿°
            curr_method = info.get('curr_method', 'æ–°æ–¹æ³•')
            return (
                f"**æ¼”è¿›** ({curr_year}å¹´): è®ºæ–‡ã€Š{curr_title}ã€‹åœ¨å‰äººå·¥ä½œçš„åŸºç¡€ä¸Šï¼Œ"
                f"é€šè¿‡ {curr_method} æ¨è¿›äº†è¯¥æ–¹å‘çš„å‘å±•ã€‚"
            )
